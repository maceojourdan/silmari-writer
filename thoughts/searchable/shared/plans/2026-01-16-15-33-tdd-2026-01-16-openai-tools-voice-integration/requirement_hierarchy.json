{
  "requirements": [
    {
      "id": "REQ_000",
      "description": "The system must integrate OpenAI Deep Research API using o3-deep-research-2025-06-26 and o4-mini-deep-research-2025-06-26 models via the Responses API endpoint",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_000.1",
          "description": "Implement POST requests to https://api.openai.com/v1/responses endpoint",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "API.implement",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_000.2",
          "description": "Support developer and user role message inputs with input_text content type",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "User.support",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_000.3",
          "description": "Configure reasoning summary options (auto or detailed)",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.configure",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_000.4",
          "description": "Enable background mode for long-running research tasks",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.enable",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_000.5",
          "description": "Implement polling or webhook mechanisms for background task completion",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.implement",
          "related_concepts": [],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_001",
      "description": "The system must support Deep Research tool configuration options including web_search_preview, code_interpreter, file_search, and mcp tools",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_001.1",
          "description": "Implement web_search_preview tool with configurable domains, search_context_size, and user_location parameters for Deep Research API",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "web_search_preview tool can be enabled/disabled in Deep Research requests",
            "domains parameter accepts an array of allowed domains (whitelist) or blocked domains (blacklist)",
            "search_context_size parameter supports 'low', 'medium', 'high' values to control context token usage",
            "user_location parameter accepts country code and optional city/region for geo-relevant results",
            "Invalid domain patterns return validation errors before API call",
            "Empty domains array defaults to unrestricted web search",
            "Configuration persists across conversation sessions when user sets preferences",
            "UI displays active search restrictions to user before research begins"
          ],
          "implementation": {
            "frontend": [
              "WebSearchConfigPanel component with domain whitelist/blacklist input fields",
              "SearchContextSizeSelector dropdown with low/medium/high options and token cost estimates",
              "UserLocationPicker with country dropdown and optional city input with autocomplete",
              "ToolConfigurationModal that displays all web_search_preview settings before research execution",
              "Visual indicators showing active search filters in the Deep Research request UI"
            ],
            "backend": [
              "POST /api/tools/deep-research/configure endpoint to save web_search_preview settings",
              "WebSearchPreviewConfig interface with domains, search_context_size, user_location fields",
              "Domain validation service to verify domain format and check for invalid patterns",
              "buildWebSearchTool() function that constructs the tool object for OpenAI Responses API",
              "Merge user preferences with request-specific overrides before API call"
            ],
            "middleware": [
              "Validate domains array contains valid domain formats (no protocols, paths)",
              "Sanitize user_location to prevent injection attacks",
              "Rate limit configuration changes to prevent abuse"
            ],
            "shared": [
              "WebSearchPreviewConfig type definition with all configurable fields",
              "SearchContextSize union type: 'low' | 'medium' | 'high'",
              "UserLocation interface with country (required), city (optional), region (optional)",
              "DOMAIN_REGEX constant for domain validation",
              "DEFAULT_SEARCH_CONFIG constant with sensible defaults"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.webSearchPreview",
          "related_concepts": [
            "OpenAI Responses API",
            "web search filtering",
            "geolocation context",
            "search result ranking"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.2",
          "description": "Support code_interpreter tool execution for Deep Research without additional configuration requirements",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "code_interpreter tool can be enabled/disabled in Deep Research requests via simple toggle",
            "No additional configuration parameters required for basic functionality",
            "Code execution results are properly extracted from response output array",
            "Execution logs and intermediate outputs are captured and displayed to user",
            "Code interpreter session costs ($0.03/session) are tracked and displayed",
            "Errors from code execution are properly formatted and shown to user",
            "Generated files/outputs from code interpreter are downloadable",
            "Tool is disabled by default to avoid unexpected costs"
          ],
          "implementation": {
            "frontend": [
              "CodeInterpreterToggle component with on/off switch and cost warning",
              "CodeExecutionResultViewer to display code outputs, charts, and generated files",
              "CodeInterpreterSessionIndicator showing active session status during research",
              "CostEstimateTooltip showing $0.03/session cost when toggle is enabled",
              "DownloadButton for files generated by code interpreter"
            ],
            "backend": [
              "buildCodeInterpreterTool() function returning { type: 'code_interpreter' }",
              "extractCodeInterpreterResults() to parse code execution outputs from response",
              "Code interpreter file handler to process and store generated files",
              "Session cost tracking service to accumulate code_interpreter usage costs",
              "Error parser for code execution failures with user-friendly messages"
            ],
            "middleware": [
              "Validate user has accepted code_interpreter cost acknowledgment",
              "Log code_interpreter session usage for billing tracking"
            ],
            "shared": [
              "CodeInterpreterConfig interface (minimal, just enabled: boolean)",
              "CodeExecutionResult interface with stdout, stderr, files, status fields",
              "CodeInterpreterFile interface with filename, content_type, download_url",
              "CODE_INTERPRETER_SESSION_COST constant ($0.03)"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.codeInterpreter",
          "related_concepts": [
            "sandboxed code execution",
            "Python runtime",
            "data analysis",
            "computation results"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.3",
          "description": "Enable file_search tool with vector_store_ids support allowing maximum of 2 vector stores for Deep Research",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "file_search tool can reference up to 2 vector_store_ids",
            "Attempting to add more than 2 vector stores returns clear validation error",
            "Vector store IDs are validated against user's available stores before research begins",
            "UI allows user to select from their existing vector stores or create new ones",
            "File search results include source document references with page/chunk info",
            "Empty vector_store_ids array disables file_search functionality",
            "Invalid vector_store_id formats are rejected with specific error messages",
            "Vector store selection persists as user preference for future research tasks"
          ],
          "implementation": {
            "frontend": [
              "VectorStoreSelector component with multi-select (max 2) from user's stores",
              "VectorStoreCreator modal for uploading documents to create new vector store",
              "VectorStoreLimitWarning displayed when user attempts to select more than 2",
              "FileSearchResultsPanel showing retrieved document chunks with source attribution",
              "VectorStorePreview showing document count and last updated timestamp"
            ],
            "backend": [
              "GET /api/vector-stores endpoint to list user's available vector stores",
              "POST /api/vector-stores endpoint to create new vector store from uploaded files",
              "buildFileSearchTool(vectorStoreIds) function with 2-store limit validation",
              "validateVectorStoreIds() to verify stores exist and user has access",
              "extractFileSearchResults() to parse retrieved chunks from response with citations"
            ],
            "middleware": [
              "Validate vector_store_ids array length <= 2",
              "Verify user ownership/access to specified vector stores",
              "Check vector store status (ready/processing) before including in request"
            ],
            "shared": [
              "FileSearchConfig interface with vector_store_ids: string[] (max 2)",
              "VectorStore interface with id, name, file_count, status, created_at",
              "FileSearchResult interface with chunk_text, source_file, page_number, relevance_score",
              "MAX_VECTOR_STORES constant (2)",
              "VECTOR_STORE_ID_REGEX for format validation"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.fileSearch",
          "related_concepts": [
            "vector embeddings",
            "semantic search",
            "document retrieval",
            "RAG patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.4",
          "description": "Implement mcp (Model Context Protocol) tool with server_url and require_approval settings for custom data source integration",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "mcp tool accepts server_url parameter pointing to MCP-compliant server",
            "server_url is validated for HTTPS protocol requirement",
            "require_approval boolean controls whether MCP tool calls need user approval",
            "When require_approval is true, UI prompts user before each MCP tool invocation",
            "MCP server connection is verified before research begins",
            "Failed MCP server connections surface clear error with server URL",
            "Multiple MCP tools can be configured with different servers",
            "MCP tool invocation history is logged for audit purposes",
            "User can configure trusted MCP servers that bypass approval requirement"
          ],
          "implementation": {
            "frontend": [
              "McpServerConfigForm with server_url input and require_approval checkbox",
              "McpConnectionTestButton to verify server accessibility before saving",
              "McpApprovalDialog shown when require_approval is true and tool is invoked",
              "TrustedMcpServersList to manage pre-approved servers",
              "McpToolInvocationLog displaying history of MCP calls made during research",
              "McpServerStatusIndicator showing connection health"
            ],
            "backend": [
              "POST /api/tools/mcp/verify endpoint to test MCP server connectivity",
              "buildMcpTool(serverUrl, requireApproval) function for OpenAI API request",
              "McpServerRegistry to store user's configured MCP servers",
              "McpApprovalQueue to handle pending approval requests when require_approval is true",
              "McpInvocationLogger to record all MCP tool calls with timestamps and payloads",
              "Webhook handler for MCP approval responses from frontend"
            ],
            "middleware": [
              "Validate server_url uses HTTPS protocol",
              "Verify server_url is reachable and returns valid MCP schema",
              "Check server_url against blocklist of known malicious endpoints",
              "Enforce require_approval for untrusted/new MCP servers"
            ],
            "shared": [
              "McpToolConfig interface with server_url: string, require_approval: boolean",
              "McpServer interface with url, name, description, trusted, last_verified",
              "McpInvocation interface with server_url, tool_name, input, output, timestamp, approved_by",
              "McpApprovalRequest interface with invocation_id, tool_name, input_preview, status",
              "MCP_HTTPS_REQUIRED constant (true)",
              "DEFAULT_REQUIRE_APPROVAL constant (true for security)"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.mcpTool",
          "related_concepts": [
            "Model Context Protocol",
            "external tool servers",
            "approval workflows",
            "custom data sources"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_002",
      "description": "The system must process Deep Research responses including final report text, citations with URLs, and intermediate reasoning steps",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_002.1",
          "description": "Extract the final synthesized report text from the Deep Research API response by navigating to response.output[-1].content[0].text, handling potential null/undefined values and validating the content structure",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Successfully extracts text content from the last output item's first content block",
            "Returns null or empty string with appropriate error logging when output array is empty",
            "Handles cases where content array is missing or malformed",
            "Validates that extracted text is a non-empty string before returning",
            "Throws typed error when response structure doesn't match expected Deep Research format",
            "Supports both o3-deep-research and o4-mini-deep-research response formats",
            "Preserves markdown formatting in extracted report text",
            "Unit tests cover all edge cases: empty output, missing content, null response"
          ],
          "implementation": {
            "frontend": [
              "Create ReportDisplay component to render extracted markdown text",
              "Implement loading skeleton while report is being extracted",
              "Add copy-to-clipboard functionality for report text",
              "Display character/word count of extracted report"
            ],
            "backend": [
              "Create DeepResearchResponseParser service class",
              "Implement extractReportText(response: DeepResearchResponse): string | null method",
              "Add response structure validation before extraction",
              "Implement retry logic for malformed responses",
              "Log extraction metrics (text length, extraction time) for monitoring"
            ],
            "middleware": [
              "Add response validation middleware to verify Deep Research response schema",
              "Implement rate limiting awareness for Deep Research API calls"
            ],
            "shared": [
              "Define DeepResearchResponse TypeScript interface matching API response structure",
              "Define ReportContent interface with text, extractedAt timestamp, and source model",
              "Create DeepResearchError custom error class with specific error codes",
              "Define constants for maximum report text length and truncation behavior"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchResponseParser.extractReportText",
          "related_concepts": [
            "OpenAI Responses API",
            "Deep Research output structure",
            "error handling",
            "content validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.2",
          "description": "Parse citation annotations from the Deep Research response output, extracting title, URL, and position metadata from response.output[-1].content[0].annotations array for proper attribution and reference linking",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Extracts all citation annotations from the response content block",
            "Each citation includes: title (string), url (valid URL string), start_index, end_index",
            "Validates URLs are properly formatted and accessible (HTTP/HTTPS)",
            "Deduplicates citations with identical URLs while preserving first occurrence",
            "Handles missing or null annotation arrays gracefully",
            "Preserves citation order as they appear in the source text",
            "Maps citation positions to corresponding text segments for inline linking",
            "Filters out citations with malformed or empty URLs",
            "Returns empty array (not null) when no citations present",
            "Unit tests verify extraction with 0, 1, and 10+ citations"
          ],
          "implementation": {
            "frontend": [
              "Create CitationList component displaying citations as clickable links",
              "Implement inline citation markers within report text (superscript numbers)",
              "Add tooltip/popover showing citation preview on hover",
              "Create CitationCard component with title, URL, and link icon",
              "Implement 'Copy all citations' button for bibliography export",
              "Add visual indicator distinguishing visited vs unvisited citation links"
            ],
            "backend": [
              "Create CitationExtractor service class",
              "Implement parseAnnotations(content: ContentBlock): Citation[] method",
              "Add URL validation and sanitization logic",
              "Implement deduplication algorithm preserving insertion order",
              "Create citation-to-text-position mapping for inline references",
              "Add domain extraction for citation source grouping"
            ],
            "middleware": [
              "Add URL validation middleware to verify citation URLs are safe",
              "Implement citation caching to avoid re-parsing identical responses"
            ],
            "shared": [
              "Define Citation interface: { title: string, url: string, startIndex: number, endIndex: number, domain?: string }",
              "Define CitationAnnotation interface matching OpenAI's annotation schema",
              "Create URL validation utility function",
              "Define CitationGroup interface for grouping by domain/source"
            ]
          },
          "testable_properties": [],
          "function_id": "CitationExtractor.parseAnnotations",
          "related_concepts": [
            "citation management",
            "URL validation",
            "annotation parsing",
            "reference linking",
            "academic attribution"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.3",
          "description": "Process intermediate reasoning summaries from Deep Research output items by filtering for 'reasoning' type items and extracting their summary field to provide transparency into the research agent's thought process",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Iterates through all items in response.output array",
            "Filters items where item.type === 'reasoning'",
            "Extracts summary field from each reasoning item",
            "Preserves chronological order of reasoning steps",
            "Handles both 'auto' and 'detailed' reasoning summary modes",
            "Returns array of ReasoningStep objects with timestamp and summary",
            "Handles empty or missing reasoning items without errors",
            "Captures reasoning step index for progress tracking",
            "Supports streaming updates for real-time reasoning display",
            "Unit tests verify extraction with varying numbers of reasoning steps"
          ],
          "implementation": {
            "frontend": [
              "Create ReasoningTimeline component showing collapsible reasoning steps",
              "Implement 'Show AI's thinking' toggle button",
              "Add expandable accordion for each reasoning step",
              "Display step number and timestamp for each reasoning item",
              "Create progress indicator showing current reasoning step during execution",
              "Add animation for new reasoning steps appearing in real-time"
            ],
            "backend": [
              "Create ReasoningStepProcessor service class",
              "Implement extractIntermediateSteps(output: OutputItem[]): ReasoningStep[] method",
              "Add step classification (planning, searching, synthesizing, etc.)",
              "Implement streaming handler for real-time reasoning updates",
              "Store reasoning steps for audit trail and debugging",
              "Calculate reasoning duration between steps"
            ],
            "middleware": [
              "Add SSE (Server-Sent Events) support for streaming reasoning updates",
              "Implement reasoning step caching for resumed/background tasks"
            ],
            "shared": [
              "Define ReasoningStep interface: { index: number, summary: string, timestamp: Date, type: 'planning' | 'searching' | 'synthesizing' }",
              "Define OutputItem union type covering all possible output item types",
              "Create constants for reasoning step type identifiers",
              "Define ReasoningProgress interface for streaming updates"
            ]
          },
          "testable_properties": [],
          "function_id": "ReasoningStepProcessor.extractIntermediateSteps",
          "related_concepts": [
            "reasoning transparency",
            "chain-of-thought",
            "agentic workflow",
            "step logging",
            "research provenance"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.4",
          "description": "Capture and log web_search_call queries from Deep Research output for transparency, allowing users to see exactly what searches the research agent performed during its investigation",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Filters response.output items where item.type === 'web_search_call'",
            "Extracts query string from each web search call item",
            "Captures search timestamp or order for chronological display",
            "Records number of results returned per search (if available)",
            "Groups related searches by topic or research phase",
            "Handles concurrent/parallel search calls",
            "Supports both synchronous and background mode responses",
            "Returns SearchQuery[] array with query text and metadata",
            "Deduplicates identical consecutive queries",
            "Unit tests verify capture with 0, 5, and 20+ search calls"
          ],
          "implementation": {
            "frontend": [
              "Create SearchQueryList component displaying all queries performed",
              "Implement search query timeline visualization",
              "Add 'View searches' expandable section in response display",
              "Create SearchQueryChip component for compact query display",
              "Add ability to re-run individual searches manually",
              "Display query count badge on search transparency toggle"
            ],
            "backend": [
              "Create WebSearchCallLogger service class",
              "Implement captureSearchQueries(output: OutputItem[]): SearchQuery[] method",
              "Add query deduplication with occurrence count",
              "Store search queries for analytics and debugging",
              "Calculate search diversity metrics (unique terms, topic spread)",
              "Implement query categorization by research phase"
            ],
            "middleware": [
              "Add search query logging middleware for audit compliance",
              "Implement query sanitization to remove sensitive terms if needed"
            ],
            "shared": [
              "Define SearchQuery interface: { query: string, index: number, timestamp?: Date, resultCount?: number, phase?: string }",
              "Define WebSearchCallItem interface matching OpenAI's output schema",
              "Create query normalization utility for deduplication",
              "Define SearchMetrics interface for analytics aggregation"
            ]
          },
          "testable_properties": [],
          "function_id": "WebSearchCallLogger.captureSearchQueries",
          "related_concepts": [
            "search transparency",
            "query logging",
            "audit trail",
            "research methodology",
            "web search preview tool"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_003",
      "description": "The system must integrate OpenAI Image Creation API using gpt-image-1.5 model (with fallback awareness for DALL-E 3 deprecation on May 12, 2026)",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_003.1",
          "description": "Implement POST requests to https://api.openai.com/v1/images/generations endpoint with proper request formatting, authentication, and error handling for image generation requests",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "POST requests are sent to https://api.openai.com/v1/images/generations with correct Content-Type: application/json header",
            "Authorization header includes Bearer token with OPENAI_API_KEY from environment variables",
            "Request body includes required fields: model, prompt, and optional fields: n, size, quality, output_format, background",
            "Prompt length validation enforces 32K character limit for GPT Image models",
            "Request timeout is configurable with default of 120 seconds for image generation",
            "Rate limit errors (429) trigger exponential backoff retry logic with maximum 3 attempts",
            "API errors return structured error responses with code, message, and retryable status",
            "Request logging captures prompt (truncated), model, size, quality for debugging without exposing sensitive data",
            "Network errors are caught and wrapped in consistent ToolError format with suggestedAction",
            "Successful requests return parsed JSON response with b64_json field"
          ],
          "implementation": {
            "frontend": [
              "Loading state indicator during image generation (can take 10-30 seconds)",
              "Error message display component for failed image generation attempts",
              "Retry button for retryable errors with visual feedback",
              "Progress indicator showing request status (pending, generating, complete, failed)"
            ],
            "backend": [
              "API route handler at /api/tools/generate-image/route.ts",
              "OpenAI client instantiation with API key from environment",
              "Request body validation using Zod schema for type safety",
              "Exponential backoff utility for rate limit handling (10s base, max 3 retries)",
              "Structured error response builder for consistent error format",
              "Request timeout wrapper with configurable duration"
            ],
            "middleware": [
              "API key validation ensuring OPENAI_API_KEY is present before request",
              "Request body size limit validation (prompt length check)",
              "Rate limiting at application level to prevent API quota exhaustion",
              "Request logging middleware for debugging and cost tracking"
            ],
            "shared": [
              "ImageGenerationRequest interface with model, prompt, size, quality, output_format, background, n fields",
              "ImageGenerationResponse interface with b64_json, revised_prompt fields",
              "ToolError type with code, message, retryable, suggestedAction fields",
              "API_ENDPOINTS constant for image generation URL",
              "DEFAULT_IMAGE_TIMEOUT constant (120000ms)"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationService.createImageRequest",
          "related_concepts": [
            "OpenAI API authentication",
            "HTTP POST requests",
            "API rate limiting",
            "Request validation",
            "Error handling patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.2",
          "description": "Support gpt-image-1.5, gpt-image-1, and gpt-image-1-mini models with intelligent model selection based on use case, quality requirements, and cost optimization",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Default model is gpt-image-1.5 for production quality output",
            "gpt-image-1-mini is used when cost optimization is explicitly requested (80% cheaper)",
            "gpt-image-1 is available as fallback if gpt-image-1.5 encounters issues",
            "Model parameter accepts only valid values: 'gpt-image-1.5', 'gpt-image-1', 'gpt-image-1-mini'",
            "Invalid model values are rejected with descriptive error message",
            "DALL-E 3 model requests are rejected with deprecation warning and migration guidance",
            "DALL-E 2 model requests are rejected with deprecation warning and migration guidance",
            "Model selection respects quality parameter constraints (gpt-image-1-mini supports low/medium/high)",
            "Cost estimation is provided based on model and quality combination before execution",
            "Model capabilities are validated against requested parameters (size, n, background support)"
          ],
          "implementation": {
            "frontend": [
              "Model selector dropdown with descriptions (Best Quality, Standard, Cost-Optimized)",
              "Cost estimate display based on selected model and quality",
              "Deprecation warning banner if user attempts DALL-E 3/2 selection",
              "Tooltip explaining model differences and use cases",
              "Quality selector that updates available options based on model choice"
            ],
            "backend": [
              "Model validation function checking against allowed models array",
              "Cost estimation utility returning price per image based on model/quality matrix",
              "Deprecation checker for DALL-E models with migration message",
              "Model fallback logic attempting gpt-image-1 if gpt-image-1.5 fails with specific errors",
              "Model capability validator ensuring requested features are supported"
            ],
            "middleware": [
              "Request validation rejecting deprecated model names with 400 status",
              "Model usage logging for cost tracking and analytics",
              "Feature flag for gradual migration from legacy models if any exist"
            ],
            "shared": [
              "SUPPORTED_IMAGE_MODELS constant array: ['gpt-image-1.5', 'gpt-image-1', 'gpt-image-1-mini']",
              "DEPRECATED_MODELS constant with deprecation date and migration guidance",
              "IMAGE_MODEL_PRICING object with cost per image by model/quality",
              "ModelCapabilities type defining supported features per model",
              "ImageModel union type for type-safe model selection"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationService.selectModel",
          "related_concepts": [
            "Model selection strategy",
            "Cost optimization",
            "Quality tiers",
            "DALL-E 3 deprecation",
            "Fallback patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.3",
          "description": "Handle base64 response format from GPT Image models, including proper parsing, validation, and data extraction since GPT Image models always return base64 (never URLs)",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Response JSON is parsed and b64_json field is extracted from data[0].b64_json",
            "Base64 string is validated for correct format before processing",
            "Invalid base64 responses trigger descriptive error with response details for debugging",
            "Multiple images (n > 1) are handled by iterating through data array",
            "Revised prompt is extracted from response if present and returned to user",
            "Empty or null b64_json field triggers error with appropriate message",
            "Response parsing handles potential API format changes gracefully with version checking",
            "Large base64 strings (up to 10MB for high quality images) are handled without memory issues",
            "Response metadata (model, created timestamp) is preserved for logging",
            "Parsing errors include original response structure in error details for debugging"
          ],
          "implementation": {
            "frontend": [
              "Response status indicator showing parsing progress for multiple images",
              "Error display for malformed API responses with retry option",
              "Revised prompt display showing AI's interpretation of user prompt",
              "Image count indicator when multiple images are returned"
            ],
            "backend": [
              "Response parser function extracting b64_json from OpenAI response structure",
              "Base64 format validator using regex pattern for valid base64 characters",
              "Multi-image iterator processing data array for n > 1 requests",
              "Metadata extractor for logging (model, created, usage if present)",
              "Response schema validator checking expected fields exist"
            ],
            "middleware": [
              "Response size monitoring for memory usage alerts",
              "Response logging with base64 content omitted (size only)",
              "API version compatibility checker for response format changes"
            ],
            "shared": [
              "OpenAIImageResponse interface matching API response structure",
              "ImageData interface with b64_json, revised_prompt optional fields",
              "Base64ValidationResult type for validation function return",
              "VALID_BASE64_REGEX constant for format validation",
              "MAX_IMAGE_SIZE_BYTES constant for memory management"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationService.parseBase64Response",
          "related_concepts": [
            "Base64 encoding",
            "JSON parsing",
            "Response validation",
            "Data integrity",
            "Image format detection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.4",
          "description": "Convert base64 responses to image buffers for storage, including proper buffer creation, format handling, and persistence to Vercel Blob storage following existing upload patterns",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Base64 string is decoded to Buffer using Buffer.from(base64, 'base64')",
            "Buffer is validated for correct image magic bytes (PNG, JPEG, WebP signatures)",
            "Image buffer is uploaded to Vercel Blob storage using existing upload pattern",
            "Generated blob URL is returned for display and future access",
            "Filename is generated with timestamp and format extension (e.g., 'generated-image-1705432800000.png')",
            "Content-Type header is set correctly based on output_format parameter",
            "Upload errors are caught and return actionable error with retry option",
            "Blob URL expiration is configured appropriately (or permanent storage if supported)",
            "Multiple images are stored individually with indexed filenames",
            "Storage cleanup mechanism removes old generated images after configurable retention period",
            "Image metadata (prompt, model, quality, generation timestamp) is stored alongside image"
          ],
          "implementation": {
            "frontend": [
              "Image display component rendering blob URL in <img> tag",
              "Download button allowing user to save generated image locally",
              "Copy URL button for sharing generated image link",
              "Image gallery view for multiple generated images",
              "Loading skeleton while image uploads to blob storage",
              "Storage usage indicator if quota limits apply"
            ],
            "backend": [
              "Buffer conversion function: Buffer.from(b64_json, 'base64')",
              "Image signature validator checking first bytes for format verification",
              "Vercel Blob upload integration using @vercel/blob put() function",
              "Filename generator with timestamp and format extension",
              "Metadata storage alongside image (prompt hash, model, quality, timestamp)",
              "Batch upload handler for multiple images with Promise.all",
              "Storage cleanup cron job or on-demand cleanup function"
            ],
            "middleware": [
              "Upload size validation before blob storage attempt",
              "Content-Type header setter based on output_format",
              "Storage quota checker before upload attempt",
              "Upload retry logic for transient storage errors"
            ],
            "shared": [
              "IMAGE_MAGIC_BYTES constant with PNG/JPEG/WebP signatures",
              "ContentType enum: 'image/png', 'image/jpeg', 'image/webp'",
              "StoredImage interface with url, filename, contentType, metadata fields",
              "ImageMetadata interface with prompt, model, quality, createdAt, size fields",
              "BLOB_STORAGE_PREFIX constant for organizing generated images",
              "IMAGE_RETENTION_DAYS constant for cleanup policy"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageStorageService.convertAndStoreImage",
          "related_concepts": [
            "Buffer operations",
            "Vercel Blob storage",
            "Image format conversion",
            "File persistence",
            "URL generation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_004",
      "description": "The system must support all gpt-image-1.5 API parameters including size, quality, output_format, background, n, stream, and partial_images",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_004.1",
          "description": "Support size options: 1024x1024, 1536x1024, 1024x1536, auto for controlling generated image dimensions",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "TypeScript enum or union type defines all valid size options: '1024x1024' | '1536x1024' | '1024x1536' | 'auto'",
            "Default size is '1024x1024' when no size is specified",
            "UI provides dropdown or radio button selection for image size with human-readable labels (Square, Landscape, Portrait, Auto)",
            "API endpoint validates size parameter and rejects invalid values with 400 status",
            "Size parameter is correctly passed to OpenAI Images API in the request body",
            "Auto size allows OpenAI to determine optimal dimensions based on prompt content",
            "Error message is shown if user selects incompatible size for certain use cases",
            "Unit tests verify all four size options are accepted by the API handler",
            "Integration test confirms generated images match the requested dimensions"
          ],
          "implementation": {
            "frontend": [
              "Create ImageSizeSelector component with radio buttons or dropdown for size selection",
              "Display preview indicators showing aspect ratio visually (square, landscape, portrait icons)",
              "Add size selection to ImageGenerationForm component",
              "Implement responsive preview box that reflects selected aspect ratio",
              "Store selected size in component state and include in API request payload"
            ],
            "backend": [
              "Create /api/tools/generate-image/route.ts endpoint accepting size parameter",
              "Validate size against allowed values enum before calling OpenAI API",
              "Pass validated size to openai.images.generate() call",
              "Return image dimensions in response metadata for frontend display"
            ],
            "middleware": [
              "Add Zod schema validation for size parameter with enum constraint",
              "Sanitize and validate size parameter before processing"
            ],
            "shared": [
              "Create ImageSize type: '1024x1024' | '1536x1024' | '1024x1536' | 'auto'",
              "Create IMAGE_SIZE_OPTIONS constant array with label/value/description objects",
              "Add ImageGenerationRequest interface with optional size field",
              "Create utility function getAspectRatioFromSize(size: ImageSize): { width: number, height: number }"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationTypes.ImageSizeOptions",
          "related_concepts": [
            "image dimensions",
            "aspect ratio",
            "landscape vs portrait",
            "auto size detection",
            "GPT-image-1.5 API"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.2",
          "description": "Support quality levels: low, medium, high for controlling image generation fidelity and cost",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "TypeScript type defines quality options: 'low' | 'medium' | 'high'",
            "Default quality is 'high' for best output unless user specifies otherwise",
            "UI displays quality options with associated estimated costs ($0.01 low, $0.04 medium, $0.17 high)",
            "Quality selection affects both visual fidelity and API cost",
            "API endpoint validates quality parameter and rejects invalid values",
            "Quality parameter is correctly passed to OpenAI Images API",
            "Tooltip or help text explains quality differences to users",
            "Cost estimation is shown before user confirms generation",
            "Unit tests verify all three quality levels are processed correctly",
            "Visual comparison documentation shows quality differences"
          ],
          "implementation": {
            "frontend": [
              "Create QualitySelector component with three-option toggle or slider",
              "Display estimated cost next to each quality option",
              "Show quality recommendation based on use case (draft vs final)",
              "Add quality selection to ImageGenerationForm component",
              "Implement cost estimation display that updates with quality selection",
              "Add confirmation dialog showing estimated cost before generation"
            ],
            "backend": [
              "Accept quality parameter in /api/tools/generate-image endpoint",
              "Validate quality against allowed enum values",
              "Pass quality to openai.images.generate() call",
              "Log quality selection for cost tracking and analytics",
              "Return actual cost in response metadata if available from API"
            ],
            "middleware": [
              "Add Zod validation for quality parameter",
              "Implement cost estimation middleware for pre-request cost display"
            ],
            "shared": [
              "Create ImageQuality type: 'low' | 'medium' | 'high'",
              "Create QUALITY_OPTIONS constant with label/value/estimatedCost objects",
              "Add quality field to ImageGenerationRequest interface",
              "Create estimateImageCost(quality: ImageQuality): number utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationTypes.QualityLevels",
          "related_concepts": [
            "image quality",
            "generation cost",
            "rendering fidelity",
            "cost optimization",
            "quality-price tradeoff"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.3",
          "description": "Support output formats: png, jpeg, webp for controlling the generated image file type",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "TypeScript type defines format options: 'png' | 'jpeg' | 'webp'",
            "Default format is 'png' for maximum compatibility and quality",
            "UI provides format selection with explanation of each format's benefits",
            "PNG is recommended when transparency is needed",
            "JPEG is recommended for photographs and smaller file sizes",
            "WebP is recommended for modern browsers with best compression",
            "Format selection affects file extension in download filename",
            "API endpoint validates format parameter",
            "Format is correctly passed to OpenAI API output_format parameter",
            "Generated image is served with correct Content-Type header",
            "Unit tests verify all three formats are generated correctly"
          ],
          "implementation": {
            "frontend": [
              "Create FormatSelector component with three format options",
              "Display format characteristics (transparency support, file size, compatibility)",
              "Auto-recommend PNG when transparent background is selected",
              "Add format selection to ImageGenerationForm component",
              "Update download filename to reflect selected format extension",
              "Show file size estimate based on format selection"
            ],
            "backend": [
              "Accept output_format parameter in API endpoint",
              "Validate format against allowed enum values",
              "Pass output_format to openai.images.generate() call",
              "Set correct Content-Type header based on format (image/png, image/jpeg, image/webp)",
              "Handle base64 decoding for all three formats correctly"
            ],
            "middleware": [
              "Add Zod validation for output_format parameter",
              "Set appropriate caching headers based on format"
            ],
            "shared": [
              "Create ImageFormat type: 'png' | 'jpeg' | 'webp'",
              "Create FORMAT_OPTIONS constant with label/value/mimeType/description objects",
              "Add outputFormat field to ImageGenerationRequest interface",
              "Create getMimeType(format: ImageFormat): string utility",
              "Create getFileExtension(format: ImageFormat): string utility"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationTypes.OutputFormats",
          "related_concepts": [
            "image format",
            "file compression",
            "transparency support",
            "file size optimization",
            "browser compatibility"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.4",
          "description": "Support background options: auto, transparent, opaque for controlling image background transparency",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "TypeScript type defines background options: 'auto' | 'transparent' | 'opaque'",
            "Default background is 'auto' letting the model decide based on prompt",
            "Transparent background option only available when PNG format is selected",
            "UI shows visual preview of background option effect",
            "Warning displayed if transparent selected with JPEG format (not supported)",
            "Auto-switch format to PNG when transparent background is selected",
            "API endpoint validates background parameter",
            "Background parameter is correctly passed to OpenAI API",
            "Generated image contains proper alpha channel when transparent is selected",
            "Unit tests verify transparent PNG has alpha channel",
            "Integration test confirms background removal works as expected"
          ],
          "implementation": {
            "frontend": [
              "Create BackgroundSelector component with three options and visual previews",
              "Show checkerboard pattern preview for transparent option",
              "Implement format-background dependency (auto-select PNG for transparent)",
              "Display warning toast if incompatible format/background combination",
              "Add background selection to ImageGenerationForm component",
              "Show transparency indicator on generated image preview"
            ],
            "backend": [
              "Accept background parameter in API endpoint",
              "Validate background against allowed enum values",
              "Validate format compatibility (transparent requires png or webp)",
              "Return 400 error with clear message for invalid combinations",
              "Pass background to openai.images.generate() call"
            ],
            "middleware": [
              "Add Zod validation for background parameter",
              "Add cross-field validation for format/background compatibility",
              "Transform request to enforce PNG when transparent is requested"
            ],
            "shared": [
              "Create ImageBackground type: 'auto' | 'transparent' | 'opaque'",
              "Create BACKGROUND_OPTIONS constant with label/value/description/compatibleFormats",
              "Add background field to ImageGenerationRequest interface",
              "Create isBackgroundCompatible(background: ImageBackground, format: ImageFormat): boolean utility",
              "Create getRecommendedFormat(background: ImageBackground): ImageFormat utility"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationTypes.BackgroundOptions",
          "related_concepts": [
            "transparency",
            "alpha channel",
            "PNG transparency",
            "background removal",
            "compositing"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.5",
          "description": "Support generating 1-10 images per request using the n parameter for batch image creation",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "User can specify number of images to generate from 1 to 10",
            "Default is 1 image when no count is specified",
            "UI provides number input or slider for selecting image count",
            "Cost estimate updates to reflect total cost (per-image cost \u00d7 count)",
            "API validates n parameter is integer between 1 and 10 inclusive",
            "All generated images are returned in a single response as array",
            "UI displays all generated images in a gallery/grid view",
            "User can select, download, or dismiss individual images from the batch",
            "Progress indicator shows batch generation status",
            "Error handling addresses partial batch failures gracefully",
            "Unit tests verify counts 1, 5, and 10 work correctly",
            "Integration test confirms all requested images are returned"
          ],
          "implementation": {
            "frontend": [
              "Create ImageCountSelector component with number input (1-10) and +/- buttons",
              "Display total estimated cost based on count \u00d7 per-image cost",
              "Create ImageGallery component to display multiple generated images",
              "Implement image selection UI (checkboxes, highlight) for batch results",
              "Add bulk download option for all or selected images",
              "Show generation progress for batch requests",
              "Implement lazy loading for large image batches",
              "Add 'Generate More' button to append to existing batch"
            ],
            "backend": [
              "Accept n parameter in API endpoint with integer validation",
              "Validate n is between 1 and 10 inclusive",
              "Pass n to openai.images.generate() call",
              "Process array of base64 images from response",
              "Upload each image to Vercel Blob and return array of URLs",
              "Handle partial failures (some images fail, others succeed)",
              "Return metadata including count requested vs count generated"
            ],
            "middleware": [
              "Add Zod validation for n parameter (z.number().int().min(1).max(10))",
              "Calculate and log total request cost for analytics",
              "Implement rate limiting based on total images (not just requests)"
            ],
            "shared": [
              "Add n (or count) field to ImageGenerationRequest interface",
              "Create ImageGenerationResponse interface with images array",
              "Create IMAGE_COUNT_LIMITS constant { min: 1, max: 10, default: 1 }",
              "Create calculateBatchCost(count: number, quality: ImageQuality): number utility",
              "Create GeneratedImage interface { url: string, index: number, revisedPrompt?: string }"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationTypes.BatchGeneration",
          "related_concepts": [
            "batch generation",
            "multiple variations",
            "image count",
            "cost multiplication",
            "parallel generation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_005",
      "description": "The system must implement document generation using AI-generated structured content combined with document creation libraries (no native OpenAI API exists)",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_005.1",
          "description": "Generate structured JSON content using OpenAI Structured Outputs with json_schema response format",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.generate",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_005.2",
          "description": "Define JSON schemas for document content with title and sections structure",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.define",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_005.3",
          "description": "Integrate PDFKit or similar library for PDF generation",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.integrate",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_005.4",
          "description": "Integrate docx library for Word document creation",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.integrate",
          "related_concepts": [],
          "category": "functional"
        },
        {
          "id": "REQ_005.5",
          "description": "Integrate ExcelJS for Excel spreadsheet generation",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [],
          "implementation": null,
          "testable_properties": [],
          "function_id": "Service.integrate",
          "related_concepts": [],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_006",
      "description": "The system must maintain existing voice input implementation using MediaRecorder, Vercel Blob storage, and OpenAI Whisper API",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_006.1",
          "description": "Capture audio using browser MediaRecorder API with automatic MIME type detection for cross-browser compatibility (webm for Chrome/Firefox, mp4 for Safari)",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Request microphone permission using navigator.mediaDevices.getUserMedia({ audio: true })",
            "Detect supported MIME type (audio/webm for Chrome/Firefox, audio/mp4 for Safari)",
            "Create MediaRecorder instance with detected MIME type",
            "Collect audio data chunks via ondataavailable handler",
            "Enforce maximum recording duration of 5 minutes (300 seconds)",
            "Create Blob from collected chunks when recording stops",
            "Handle microphone permission denial with user-friendly error message",
            "Provide visual feedback during recording (recording indicator, duration timer)",
            "Support start, stop, and cancel recording actions",
            "Clean up MediaRecorder and MediaStream resources on component unmount"
          ],
          "implementation": {
            "frontend": [
              "AudioRecorder.tsx component with recording state management",
              "Recording button with start/stop toggle functionality",
              "Visual recording indicator (pulsing red dot or waveform)",
              "Duration timer display showing elapsed recording time",
              "Error state display for permission denial or unsupported browsers",
              "Cancel button to abort recording without processing"
            ],
            "backend": [],
            "middleware": [],
            "shared": [
              "MIME type detection utility function",
              "MAX_RECORDING_TIME constant (5 minutes = 300000ms)",
              "RecordingState type enum (idle, recording, processing, error)",
              "AudioRecorderConfig interface for configuration options"
            ]
          },
          "testable_properties": [],
          "function_id": "AudioRecorder.captureAudio",
          "related_concepts": [
            "MediaRecorder API",
            "MIME type detection",
            "audio chunks",
            "browser compatibility",
            "getUserMedia"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.2",
          "description": "Upload recorded audio blob to Vercel Blob storage to bypass the 4.5MB serverless function payload limit, enabling larger audio files up to 25MB",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "POST audio blob to /api/upload endpoint",
            "Validate file size does not exceed 25MB limit before upload",
            "Generate unique filename with timestamp for blob storage",
            "Return blob URL upon successful upload",
            "Handle upload failures with appropriate error messages",
            "Support progress tracking for upload status",
            "Implement retry logic for transient upload failures (max 3 attempts)",
            "Set appropriate content-type header based on audio MIME type",
            "Return upload error if Vercel Blob storage is unavailable"
          ],
          "implementation": {
            "frontend": [
              "Upload progress indicator during blob upload",
              "Error handling UI for upload failures",
              "Retry button for failed uploads"
            ],
            "backend": [
              "/api/upload/route.ts endpoint for Vercel Blob upload",
              "Vercel Blob SDK integration (@vercel/blob)",
              "File validation (size, type checking)",
              "Unique filename generation with UUID or timestamp"
            ],
            "middleware": [
              "Request size validation middleware",
              "Content-type validation for audio files"
            ],
            "shared": [
              "MAX_FILE_SIZE constant (25MB = 26214400 bytes)",
              "UploadResponse interface { url: string, pathname: string }",
              "UploadError interface { code: string, message: string }",
              "Supported audio MIME types array"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.uploadToBlob",
          "related_concepts": [
            "Vercel Blob storage",
            "file upload",
            "serverless limits",
            "blob URL",
            "multipart upload"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.3",
          "description": "Call /api/transcribe endpoint with the Vercel Blob URL to initiate Whisper transcription processing",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "POST request to /api/transcribe with blob URL in request body",
            "Include isVoiceTranscription flag for message context",
            "Handle HTTP error responses (400, 401, 429, 500)",
            "Implement exponential backoff for rate limit errors (429)",
            "Set appropriate request timeout (30 seconds default)",
            "Return transcribed text on success",
            "Display loading state during transcription processing",
            "Handle network connectivity errors gracefully",
            "Validate blob URL format before making request"
          ],
          "implementation": {
            "frontend": [
              "Transcription loading state with spinner or skeleton",
              "Error display component for transcription failures",
              "Retry mechanism for rate-limited requests"
            ],
            "backend": [
              "/api/transcribe/route.ts endpoint",
              "Request body validation (blob URL required)",
              "Error response formatting with appropriate status codes"
            ],
            "middleware": [
              "Rate limiting middleware for transcription endpoint",
              "Request validation middleware for blob URL format"
            ],
            "shared": [
              "TranscribeRequest interface { blobUrl: string }",
              "TranscribeResponse interface { text: string, duration?: number }",
              "RETRY_ATTEMPTS constant (3)",
              "RATE_LIMIT_DELAY constant (10000ms base)",
              "NETWORK_ERROR_DELAY constant (2000ms base)"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.requestTranscription",
          "related_concepts": [
            "REST API",
            "transcription request",
            "blob URL",
            "async processing",
            "error handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.4",
          "description": "Process audio through OpenAI Whisper API using openai.audio.transcriptions.create with whisper-1 model for speech-to-text conversion",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Fetch audio file from Vercel Blob URL",
            "Convert fetched response to File object with appropriate filename",
            "Call openai.audio.transcriptions.create with whisper-1 model",
            "Handle OpenAI API rate limits with exponential backoff (10s base, 3 retries)",
            "Handle network errors with retry logic (2s base delay)",
            "Return transcribed text from API response",
            "Log transcription duration and audio length for analytics",
            "Handle empty audio or silence with appropriate response",
            "Support multiple audio formats (webm, mp4, wav, mp3)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "OpenAI SDK initialization with API key from environment",
              "Audio file fetch from blob URL",
              "openai.audio.transcriptions.create() call with whisper-1 model",
              "Exponential backoff retry implementation",
              "Error classification (rate limit vs network vs API error)"
            ],
            "middleware": [
              "OpenAI API key validation",
              "Request logging for transcription analytics"
            ],
            "shared": [
              "WhisperConfig interface { model: string, temperature?: number }",
              "TranscriptionResult interface { text: string, language?: string }",
              "WHISPER_MODEL constant ('whisper-1')",
              "Retry utility function with exponential backoff"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.processWithWhisper",
          "related_concepts": [
            "OpenAI Whisper API",
            "speech-to-text",
            "audio processing",
            "transcription",
            "language detection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.5",
          "description": "Delete the temporary blob from Vercel Blob storage after successful transcription and return the transcribed text to the client",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Delete blob from Vercel Blob storage using blob URL after transcription",
            "Handle blob deletion failures gracefully (log but don't fail request)",
            "Return transcribed text in response body",
            "Include transcription metadata (duration, confidence if available)",
            "Set isVoiceTranscription flag to true in response for message handling",
            "Auto-send transcribed text as message if configured",
            "Clean up blob even if transcription fails (in finally block)",
            "Log cleanup success/failure for monitoring",
            "Implement idempotent cleanup (handle already-deleted blobs)"
          ],
          "implementation": {
            "frontend": [
              "Handle successful transcription response",
              "Auto-populate message input with transcribed text",
              "Option to auto-send transcribed message",
              "Display transcription result before sending"
            ],
            "backend": [
              "Vercel Blob del() function call for cleanup",
              "Try-finally pattern for guaranteed cleanup",
              "Response formatting with text and metadata",
              "Error logging for failed deletions"
            ],
            "middleware": [],
            "shared": [
              "CleanupResult interface { deleted: boolean, error?: string }",
              "TranscriptionResponse interface { text: string, isVoiceTranscription: boolean, metadata?: object }",
              "Message type extension with isVoiceTranscription flag (types.ts:49-56)"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.cleanupAndRespond",
          "related_concepts": [
            "blob cleanup",
            "resource management",
            "response handling",
            "temporary storage",
            "garbage collection"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_007",
      "description": "The system must enforce voice recording constraints: 5 minute max recording time, 25 MB max file size, 3 retry attempts, exponential backoff for rate limits and network errors",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_007.1",
          "description": "Enforce 5 minute maximum recording duration in AudioRecorder component with visual countdown, automatic stop, and user feedback",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "Recording automatically stops after exactly 300 seconds (5 minutes)",
            "Visual countdown timer displays remaining time in MM:SS format",
            "Timer updates every second during active recording",
            "Warning indicator appears when 30 seconds or less remain (visual color change to yellow/orange)",
            "Critical warning at 10 seconds remaining (visual color change to red, optional pulse animation)",
            "Toast/notification informs user when recording auto-stopped due to time limit",
            "Recording state transitions cleanly from 'recording' to 'stopped' without data loss",
            "All audio chunks collected up to the stop point are preserved",
            "Timer resets to 5:00 when new recording starts",
            "Timer cleanup occurs on component unmount to prevent memory leaks"
          ],
          "implementation": {
            "frontend": [
              "Add MAX_RECORDING_DURATION constant (300000ms) to AudioRecorder.tsx",
              "Create RecordingTimer sub-component with countdown display",
              "Implement useEffect with setInterval for 1-second countdown updates",
              "Add timerRef to track interval for cleanup",
              "Add remainingTime state variable initialized to MAX_RECORDING_DURATION",
              "Implement formatTime utility function for MM:SS display",
              "Add conditional CSS classes for warning states (yellow at \u226430s, red at \u226410s)",
              "Integrate with existing stopRecording() function for auto-stop",
              "Add 'Time limit reached' toast notification using existing toast system",
              "Ensure timer visibility is tied to isRecording state"
            ],
            "backend": [],
            "middleware": [],
            "shared": [
              "Define VOICE_RECORDING_LIMITS constant object in constants.ts with MAX_DURATION_MS: 300000",
              "Create formatDuration(ms: number): string utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "AudioRecorder.enforceMaxDuration",
          "related_concepts": [
            "MediaRecorder API",
            "Timer management",
            "User feedback",
            "Recording state",
            "Auto-stop behavior"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.2",
          "description": "Validate 25 MB maximum file size before upload to prevent oversized files from consuming bandwidth and failing at the server",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "File size validation occurs BEFORE upload attempt begins",
            "Files exceeding 25 MB (26,214,400 bytes) are rejected with clear error message",
            "Error message specifies actual file size and maximum allowed size",
            "Validation applies to both direct file uploads and recorded audio blobs",
            "No network request is made for oversized files (bandwidth savings)",
            "User receives actionable feedback: 'Recording is X MB, maximum allowed is 25 MB'",
            "Size check uses Blob.size property for accurate byte count",
            "Validation function is reusable across upload scenarios",
            "Edge case: exactly 25 MB file is allowed (\u2264 not <)",
            "Compressed audio estimation shown during recording (optional enhancement)"
          ],
          "implementation": {
            "frontend": [
              "Add validateFileSize(blob: Blob): ValidationResult function in transcription.ts",
              "Update uploadAudio() to call validateFileSize before fetch",
              "Display error toast with file size details on validation failure",
              "Add file size display in AudioRecorder UI during/after recording",
              "Implement formatFileSize utility for human-readable sizes (KB, MB)"
            ],
            "backend": [
              "Add server-side validation in /api/upload/route.ts as backup",
              "Return 413 Payload Too Large status for oversized uploads",
              "Include max allowed size in error response body"
            ],
            "middleware": [],
            "shared": [
              "Define MAX_FILE_SIZE_BYTES: 26214400 in VOICE_RECORDING_LIMITS",
              "Create validateFileSize interface: { valid: boolean; actualSize: number; maxSize: number; error?: string }",
              "Create formatFileSize(bytes: number): string utility"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.validateFileSize",
          "related_concepts": [
            "File validation",
            "Blob size checking",
            "Upload gating",
            "User feedback",
            "Error prevention"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.3",
          "description": "Implement 3 retry attempts for transcription failures with attempt tracking, logging, and graceful degradation",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "Maximum of 3 total attempts (1 initial + 2 retries) before final failure",
            "Each retry attempt is logged with attempt number and error details",
            "User sees progress indicator showing retry status (e.g., 'Retrying... (attempt 2 of 3)')",
            "Different error types trigger appropriate retry behavior (rate limit vs network vs API error)",
            "Final failure after 3 attempts shows aggregated error information",
            "Successful retry clears error state and proceeds normally",
            "Retry counter resets for each new transcription request",
            "Non-retryable errors (4xx client errors except 429) fail immediately without retry",
            "Retryable errors: 429 (rate limit), 5xx (server errors), network failures",
            "Timeout errors are treated as retryable"
          ],
          "implementation": {
            "frontend": [
              "Update TranscriptionStatus component to show attempt count",
              "Add retry progress indicator in AudioRecorder UI",
              "Display final error with 'Failed after 3 attempts' message",
              "Show intermediate retry notifications (non-blocking)"
            ],
            "backend": [
              "Wrap transcription call in retry loop in /api/transcribe/route.ts",
              "Add attempt counter variable initialized to 0",
              "Implement isRetryableError(error): boolean helper function",
              "Log each attempt with structured logging: { attempt, maxAttempts, error, timestamp }",
              "Aggregate errors from all attempts for final error response",
              "Return 503 Service Unavailable after all retries exhausted"
            ],
            "middleware": [],
            "shared": [
              "Define MAX_RETRY_ATTEMPTS: 3 in VOICE_RECORDING_LIMITS",
              "Create RetryableErrorCodes: [429, 500, 502, 503, 504] constant",
              "Define TranscriptionAttempt interface: { attempt: number; timestamp: Date; error?: Error; success: boolean }"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.retryWithAttempts",
          "related_concepts": [
            "Retry logic",
            "Failure recovery",
            "Attempt counting",
            "Error aggregation",
            "Circuit breaker pattern"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.4",
          "description": "Apply 10 second base delay with exponential backoff for rate limit errors (HTTP 429) to respect API limits and maximize retry success",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "First retry after rate limit waits 10 seconds base delay",
            "Second retry waits 20 seconds (10 * 2^1)",
            "Third retry waits 40 seconds (10 * 2^2)",
            "Retry-After header from API response is respected if present and greater than calculated delay",
            "Maximum backoff cap of 60 seconds to prevent excessive waits",
            "Random jitter of \u00b110% added to prevent thundering herd",
            "User sees countdown during wait: 'Rate limited. Retrying in X seconds...'",
            "Countdown updates every second during wait period",
            "User can cancel retry during wait period",
            "Delay calculation: min(BASE_DELAY * 2^attempt + jitter, MAX_DELAY)"
          ],
          "implementation": {
            "frontend": [
              "Create RateLimitCountdown component showing seconds remaining",
              "Add cancel button during rate limit wait period",
              "Update AudioRecorder state to include 'rate_limited' status",
              "Show informative message explaining rate limiting to user"
            ],
            "backend": [
              "Implement calculateRateLimitDelay(attempt: number, retryAfterHeader?: string): number",
              "Parse Retry-After header (seconds or HTTP date format)",
              "Add jitter calculation: delay * (0.9 + Math.random() * 0.2)",
              "Implement sleep/delay function with AbortController support",
              "Log rate limit events with delay duration for monitoring",
              "Track rate limit frequency for potential alerting"
            ],
            "middleware": [],
            "shared": [
              "Define RATE_LIMIT_BASE_DELAY_MS: 10000 in VOICE_RECORDING_LIMITS",
              "Define RATE_LIMIT_MAX_DELAY_MS: 60000 in VOICE_RECORDING_LIMITS",
              "Create calculateExponentialBackoff(baseDelay: number, attempt: number, maxDelay: number): number utility",
              "Create addJitter(delay: number, jitterPercent: number): number utility"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.rateLimitBackoff",
          "related_concepts": [
            "Rate limiting",
            "Exponential backoff",
            "429 handling",
            "API throttling",
            "Jitter",
            "Retry-After header"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.5",
          "description": "Apply 2 second base delay with exponential backoff for network errors to handle transient connectivity issues efficiently",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "First retry after network error waits 2 seconds base delay",
            "Second retry waits 4 seconds (2 * 2^1)",
            "Third retry waits 8 seconds (2 * 2^2)",
            "Network errors detected: TypeError from fetch, AbortError, connection refused",
            "Online/offline status checked before retry attempt",
            "If offline, wait for online event before starting delay timer",
            "Maximum backoff cap of 30 seconds for network errors",
            "Random jitter of \u00b110% added to delays",
            "User sees 'Connection lost. Retrying when back online...' if offline",
            "User sees 'Network error. Retrying in X seconds...' if online but failed",
            "Distinguish between timeout errors and connection errors in UI messaging"
          ],
          "implementation": {
            "frontend": [
              "Create NetworkErrorRetry component with status messaging",
              "Add navigator.onLine check before retry",
              "Listen to 'online' event for automatic retry trigger",
              "Show network status indicator during retry wait",
              "Allow manual retry button during network error state",
              "Update AudioRecorder to handle 'network_error' status"
            ],
            "backend": [
              "Implement calculateNetworkErrorDelay(attempt: number): number",
              "Detect network-specific errors in catch block: TypeError, 'Failed to fetch', ECONNREFUSED",
              "Implement request timeout with AbortController (30 second default)",
              "Log network errors with attempt details and error classification",
              "Add fetch wrapper with timeout: fetchWithTimeout(url, options, timeoutMs)"
            ],
            "middleware": [],
            "shared": [
              "Define NETWORK_ERROR_BASE_DELAY_MS: 2000 in VOICE_RECORDING_LIMITS",
              "Define NETWORK_ERROR_MAX_DELAY_MS: 30000 in VOICE_RECORDING_LIMITS",
              "Define REQUEST_TIMEOUT_MS: 30000 in VOICE_RECORDING_LIMITS",
              "Create isNetworkError(error: unknown): boolean utility function",
              "Create NetworkErrorType enum: TIMEOUT | CONNECTION_REFUSED | OFFLINE | UNKNOWN"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.networkErrorBackoff",
          "related_concepts": [
            "Network resilience",
            "Connectivity detection",
            "Fetch errors",
            "Timeout handling",
            "Connection recovery"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_008",
      "description": "The system must implement an Intent Classification layer to route plain language requests to appropriate tools",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_008.1",
          "description": "Create classifyIntent function using gpt-4o-mini with json_object response format to analyze user messages and determine the appropriate tool to route the request to",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "Function accepts a userMessage string parameter and optional conversationHistory array",
            "Function makes a POST request to OpenAI API using gpt-4o-mini model",
            "Request includes response_format: { type: 'json_object' } configuration",
            "Function returns a Promise<ClassifiedIntent> with parsed JSON response",
            "Function handles empty or whitespace-only messages with appropriate error",
            "Function validates the returned JSON matches ClassifiedIntent structure",
            "Function logs classification attempts for debugging and analytics",
            "Function completes classification within 2 seconds for typical messages",
            "Function reuses existing retry logic pattern from generate/route.ts",
            "Unit tests verify JSON parsing of all possible tool type responses"
          ],
          "implementation": {
            "frontend": [
              "No direct UI components - this is a utility function called from message processing",
              "Loading indicator shown while classification is in progress"
            ],
            "backend": [
              "Create /api/tools/classify/route.ts API endpoint",
              "Import OpenAI SDK and initialize client with OPENAI_API_KEY",
              "Implement request validation for message parameter",
              "Call openai.chat.completions.create with model 'gpt-4o-mini' and response_format: { type: 'json_object' }",
              "Parse and validate JSON response against ClassifiedIntent interface",
              "Return 400 for invalid message format, 500 for API errors",
              "Apply existing retry logic pattern (MAX_RETRIES=3, exponential backoff)"
            ],
            "middleware": [
              "No additional middleware required - uses existing Next.js API route handling"
            ],
            "shared": [
              "Create /lib/intentClassifier.ts with classifyIntent function",
              "Define ClassifiedIntent interface with tool, confidence, and extractedParams fields",
              "Define ToolIntent union type for allowed tool values",
              "Export classifyIntent function for use by message processing pipeline"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.classifyIntent",
          "related_concepts": [
            "OpenAI Chat Completions API",
            "JSON Schema Validation",
            "Response Format",
            "Structured Outputs",
            "gpt-4o-mini model"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.2",
          "description": "Define and implement the four core intent types (deep_research, image_generation, document_generation, chat_completion) with their associated type definitions and validation",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "ToolIntent type is defined as union: 'deep_research' | 'image_generation' | 'document_generation' | 'chat_completion'",
            "deep_research intent includes extractedParams for query, depth preference, and topic keywords",
            "image_generation intent includes extractedParams for prompt, size hint, style preference",
            "document_generation intent includes extractedParams for document type (pdf/docx/xlsx), content description, template hint",
            "chat_completion is the default fallback intent for general conversation",
            "Each intent type has a TypeScript interface for its extracted parameters",
            "Type guard functions exist to validate and narrow intent types",
            "Enum or constant object provides display names for each intent type",
            "Unit tests verify each intent type can be correctly serialized/deserialized",
            "Documentation comments explain when each intent type should be used"
          ],
          "implementation": {
            "frontend": [
              "Intent type badges/icons for displaying detected intent to user (optional feedback)",
              "Toast notification when non-chat intent is detected to inform user of tool selection"
            ],
            "backend": [
              "Validation function to ensure returned tool is one of the four valid types",
              "Fallback to 'chat_completion' if classifier returns unknown tool type",
              "Log warning when fallback is triggered for monitoring classifier accuracy"
            ],
            "middleware": [
              "No additional middleware required"
            ],
            "shared": [
              "Create types in /lib/intentClassifier.ts or /lib/types.ts",
              "Define ToolIntent union type",
              "Define DeepResearchParams interface: { query: string; depth?: 'quick' | 'thorough'; topics?: string[] }",
              "Define ImageGenerationParams interface: { prompt: string; size?: string; style?: string }",
              "Define DocumentGenerationParams interface: { type: 'pdf' | 'docx' | 'xlsx'; contentDescription: string; template?: string }",
              "Define ChatCompletionParams interface: { message: string }",
              "Define ExtractedParams as discriminated union of all param types",
              "Create isValidToolIntent type guard function",
              "Create TOOL_INTENT_DISPLAY_NAMES constant object"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.ToolIntentTypes",
          "related_concepts": [
            "TypeScript Union Types",
            "Intent Detection",
            "Tool Routing",
            "Discriminated Unions",
            "Exhaustive Pattern Matching"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.3",
          "description": "Define and implement the ClassifiedIntent response structure with tool name, confidence score (0.0-1.0), and extracted parameters object",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "ClassifiedIntent interface contains 'tool' field of type ToolIntent",
            "ClassifiedIntent interface contains 'confidence' field as number between 0.0 and 1.0",
            "ClassifiedIntent interface contains 'extractedParams' field as Record<string, unknown>",
            "Confidence score reflects classifier certainty (>0.8 high, 0.5-0.8 medium, <0.5 low)",
            "extractedParams contains tool-specific parameters parsed from user message",
            "Response includes optional 'alternativeIntents' array for ambiguous cases",
            "Validation function ensures confidence is within valid range",
            "Validation function ensures extractedParams matches expected structure for tool type",
            "Low confidence (<0.5) triggers clarification prompt to user",
            "Unit tests verify confidence normalization and param extraction for sample messages"
          ],
          "implementation": {
            "frontend": [
              "Display confidence indicator when showing detected intent",
              "Prompt user for clarification when confidence is below threshold (configurable, default 0.5)",
              "Show alternative intents as suggestions when multiple intents have similar confidence"
            ],
            "backend": [
              "Parse confidence from classifier response and clamp to 0.0-1.0 range",
              "Extract and validate tool-specific parameters from response",
              "Include alternativeIntents in response when second-best confidence > 0.4",
              "Log confidence distribution for classifier accuracy monitoring"
            ],
            "middleware": [
              "No additional middleware required"
            ],
            "shared": [
              "Define ClassifiedIntent interface in /lib/intentClassifier.ts",
              "Define AlternativeIntent interface: { tool: ToolIntent; confidence: number }",
              "Create validateClassifiedIntent function to ensure response integrity",
              "Create clampConfidence utility function: (n: number) => Math.max(0, Math.min(1, n))",
              "Define CONFIDENCE_THRESHOLDS constant: { HIGH: 0.8, MEDIUM: 0.5, LOW: 0.3 }",
              "Create shouldRequestClarification function based on confidence threshold"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.ClassifiedIntentResponse",
          "related_concepts": [
            "Confidence Scoring",
            "Parameter Extraction",
            "NLP Classification",
            "Structured Response",
            "JSON Schema"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.4",
          "description": "Create and maintain the system prompt that defines classification criteria for each tool type, providing clear guidance to gpt-4o-mini on how to categorize user requests",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "System prompt clearly defines all four tool types with descriptions",
            "deep_research criteria: research queries, fact-finding, analysis requests, 'research', 'investigate', 'find out about', 'what is the latest on'",
            "image_generation criteria: create images, draw, visualize, artwork, 'create an image', 'draw me', 'generate a picture', 'visualize'",
            "document_generation criteria: create PDF, Word doc, Excel spreadsheet, report, 'create a document', 'generate a report', 'make a spreadsheet', 'export as PDF'",
            "chat_completion criteria: general conversation, writing assistance, questions, creative writing - default when no other tool matches",
            "System prompt includes 2-3 few-shot examples for each tool type",
            "System prompt specifies JSON output format with required fields",
            "System prompt instructs extraction of relevant parameters from user message",
            "System prompt includes guidance on confidence scoring criteria",
            "System prompt is stored as constant for easy maintenance and versioning",
            "Unit tests verify classifier produces expected tool for sample trigger phrases"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend implementation required"
            ],
            "backend": [
              "Create INTENT_CLASSIFIER_SYSTEM_PROMPT constant string in /lib/intentClassifier.ts",
              "Include clear JSON schema definition in prompt for response format",
              "Include few-shot examples demonstrating each tool classification",
              "Include parameter extraction guidance for each tool type",
              "Include confidence scoring rubric in prompt",
              "Version the prompt with comments for tracking changes"
            ],
            "middleware": [
              "No additional middleware required"
            ],
            "shared": [
              "Store system prompt as exported constant INTENT_CLASSIFIER_SYSTEM_PROMPT",
              "Create separate constants for each tool's trigger phrases: DEEP_RESEARCH_TRIGGERS, IMAGE_GENERATION_TRIGGERS, DOCUMENT_GENERATION_TRIGGERS",
              "Define JSON schema for expected response format as reference documentation",
              "Create prompt template that can incorporate conversation history context",
              "Document prompt versioning strategy in code comments"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.SystemPrompt",
          "related_concepts": [
            "Prompt Engineering",
            "Few-Shot Examples",
            "System Instructions",
            "Classification Criteria",
            "Tool Descriptions"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_009",
      "description": "The system must implement a Tool Registry with tool definitions including name, description, trigger phrases, handler functions, and response types",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_009.1",
          "description": "Create the central toolRegistry Map data structure with ToolDefinition entries for all supported tools (deep_research, image_generation, document_generation, chat_completion)",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "ToolDefinition interface is defined with required fields: name (string), description (string), triggerPhrases (string[]), handler (function reference), responseType (enum)",
            "ResponseType enum includes 'text', 'image', and 'file' values",
            "toolRegistry is implemented as Map<string, ToolDefinition> for O(1) lookup performance",
            "Registry contains entries for: 'deep_research', 'image_generation', 'document_generation', 'chat_completion'",
            "Each tool entry includes unique identifier key matching its handler purpose",
            "Registry is exported as a singleton module for consistent access across application",
            "TypeScript strict mode compilation passes with no type errors",
            "Unit tests verify all four tool entries exist and have valid ToolDefinition shape",
            "Registry is immutable after initialization (use Object.freeze or readonly Map pattern)"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Create frontend/src/lib/tools/types.ts with ToolDefinition interface and ResponseType enum",
              "Create frontend/src/lib/tools/toolRegistry.ts exporting toolRegistry Map<string, ToolDefinition>",
              "Define ToolParams interface for handler function parameter typing",
              "Define ToolResult interface with success/error union type for handler return values",
              "Add ToolError interface extending existing error patterns from types.ts (code, message, retryable)",
              "Create index.ts barrel export for tools module"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.createToolRegistryMap",
          "related_concepts": [
            "Map data structure",
            "ToolDefinition interface",
            "type-safe registry pattern",
            "handler function references",
            "tool configuration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.2",
          "description": "Define comprehensive trigger phrase arrays for each tool to enable accurate intent classification from natural language input",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "deep_research trigger phrases include: 'research', 'investigate', 'find out about', 'analyze', 'look into', 'study', 'explore', 'what is the latest on', 'tell me about'",
            "image_generation trigger phrases include: 'create image', 'draw', 'generate picture', 'visualize', 'make an image', 'create artwork', 'design', 'illustrate', 'paint'",
            "document_generation trigger phrases include: 'create document', 'generate report', 'make spreadsheet', 'create PDF', 'generate Word doc', 'make Excel', 'export as', 'create file'",
            "chat_completion trigger phrases include: 'write', 'help me with', 'explain', 'tell me', 'answer', 'chat' (fallback default)",
            "Each phrase array contains minimum 5 unique phrases covering common voice/text variations",
            "Phrases are lowercase for case-insensitive matching",
            "Phrases avoid overlap between tools to prevent classification ambiguity",
            "Unit tests validate phrase uniqueness across tools and minimum phrase count per tool",
            "Integration test verifies intent classifier uses trigger phrases as classification hints"
          ],
          "implementation": {
            "frontend": [],
            "backend": [],
            "middleware": [],
            "shared": [
              "Add triggerPhrases: readonly string[] field to ToolDefinition interface",
              "Populate deep_research entry with research-related phrases from common voice patterns",
              "Populate image_generation entry with creative/visual phrases",
              "Populate document_generation entry with file/document creation phrases",
              "Populate chat_completion entry with general writing/conversation phrases",
              "Create helper function isPhraseMatch(input: string, phrases: string[]): boolean for substring matching",
              "Add validation function validateTriggerPhrases() checking uniqueness and minimum counts"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.defineTriggerPhrases",
          "related_concepts": [
            "natural language processing",
            "intent detection",
            "phrase matching",
            "voice command patterns",
            "synonym handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.3",
          "description": "Specify the responseType enum value for each tool to determine how the UI should render tool execution results",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "ResponseType enum is defined with exactly three values: 'text' | 'image' | 'file'",
            "deep_research responseType is 'text' (returns markdown report with citations)",
            "image_generation responseType is 'image' (returns base64 or blob URL for display)",
            "document_generation responseType is 'file' (returns downloadable file URL with filename)",
            "chat_completion responseType is 'text' (returns standard chat response)",
            "Message interface in types.ts is extended with optional responseType field for rendering hints",
            "MessageBubble component uses responseType to determine rendering strategy",
            "Unit tests verify each tool's responseType matches expected value",
            "Type guard functions isTextResponse(), isImageResponse(), isFileResponse() are implemented"
          ],
          "implementation": {
            "frontend": [
              "Extend Message interface in types.ts with optional toolResponseType?: ResponseType field",
              "Create frontend/src/components/chat/ToolResponseRenderer.tsx component",
              "ToolResponseRenderer switches on responseType to render: TextResponse, ImageResponse, FileResponse",
              "ImageResponse component displays image with loading state and error fallback",
              "FileResponse component displays download button with filename and file size",
              "Update MessageBubble.tsx to use ToolResponseRenderer when toolResponseType is present"
            ],
            "backend": [],
            "middleware": [],
            "shared": [
              "Define ResponseType = 'text' | 'image' | 'file' in tools/types.ts",
              "Add responseType field to ToolDefinition interface",
              "Create type guard functions for each response type in tools/typeGuards.ts",
              "Define ToolTextResult, ToolImageResult, ToolFileResult interfaces for type-safe handling"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.specifyResponseTypes",
          "related_concepts": [
            "response rendering",
            "message type handling",
            "file download",
            "image display",
            "text formatting",
            "UI components"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.4",
          "description": "Register handler function references (handleDeepResearch, handleImageGeneration, handleDocumentGeneration) in the toolRegistry with proper async function signatures and error handling",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "handleDeepResearch handler is registered for 'deep_research' tool",
            "handleImageGeneration handler is registered for 'image_generation' tool",
            "handleDocumentGeneration handler is registered for 'document_generation' tool",
            "handleChatCompletion handler is registered for 'chat_completion' tool (wraps existing /api/generate)",
            "Each handler follows signature: (params: ToolParams) => Promise<ToolResult>",
            "Handlers throw ToolError with retryable flag for recoverable failures",
            "Handler implementations are in separate files: tools/deepResearch.ts, tools/imageGeneration.ts, tools/documentGeneration.ts",
            "Registry handler field references imported async functions (not inline implementations)",
            "Integration tests verify handler invocation through registry lookup and execution",
            "Each handler includes timeout handling (configurable per tool, default 30s for sync, 300s for async)"
          ],
          "implementation": {
            "frontend": [
              "Create frontend/src/lib/tools/handlers/index.ts barrel export",
              "Implement invokeToolHandler(toolName: string, params: ToolParams): Promise<ToolResult> utility"
            ],
            "backend": [
              "Create frontend/src/app/api/tools/deep-research/route.ts calling OpenAI Responses API",
              "Create frontend/src/app/api/tools/generate-image/route.ts calling OpenAI Images API (gpt-image-1.5)",
              "Create frontend/src/app/api/tools/generate-document/route.ts with structured outputs and library integration",
              "Each route follows existing error handling pattern from generate/route.ts (ChatGenerationError pattern)",
              "Implement retry logic with exponential backoff matching existing MAX_RETRIES pattern",
              "Add request validation for required parameters per tool"
            ],
            "middleware": [
              "Add rate limiting middleware for expensive tools (deep-research, generate-image)",
              "Add request size validation for document generation content"
            ],
            "shared": [
              "Create frontend/src/lib/tools/handlers/deepResearch.ts with handleDeepResearch function",
              "Create frontend/src/lib/tools/handlers/imageGeneration.ts with handleImageGeneration function",
              "Create frontend/src/lib/tools/handlers/documentGeneration.ts with handleDocumentGeneration function",
              "Create frontend/src/lib/tools/handlers/chatCompletion.ts wrapping existing generateResponse from api.ts",
              "Define ToolParams type union: DeepResearchParams | ImageGenerationParams | DocumentGenerationParams | ChatCompletionParams",
              "Define handler type: ToolHandler = (params: ToolParams) => Promise<ToolResult>"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.registerHandlers",
          "related_concepts": [
            "async function handlers",
            "dependency injection",
            "error propagation",
            "API route integration",
            "handler pattern"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_010",
      "description": "The system must implement a Deep Research Handler that executes research queries and returns text with citations",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_010.1",
          "description": "Implement depth parameter selection logic that maps user-specified research depth (quick vs thorough) to appropriate OpenAI model selection, with quick using o4-mini-deep-research for faster/cheaper queries (~$3) and thorough using o3-deep-research for comprehensive analysis (~$30)",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "When depth='quick', system selects 'o4-mini-deep-research-2025-06-26' model",
            "When depth='thorough', system selects 'o3-deep-research-2025-06-26' model",
            "When depth parameter is undefined/null, system defaults to 'quick' for cost optimization",
            "Depth parameter is validated against allowed values ['quick', 'thorough'] with appropriate error for invalid values",
            "Model selection is logged for cost tracking and analytics purposes",
            "Estimated cost is calculated and optionally returned to caller before execution"
          ],
          "implementation": {
            "frontend": [
              "Add depth selector dropdown/toggle in research request UI with options 'Quick (~$3)' and 'Thorough (~$30)'",
              "Display estimated cost warning before submission when 'thorough' is selected",
              "Persist user's last depth preference in localStorage for convenience",
              "Show visual indicator (icon/badge) distinguishing quick vs thorough research requests in chat"
            ],
            "backend": [
              "Create DepthConfig interface with model mapping: { quick: 'o4-mini-deep-research-2025-06-26', thorough: 'o3-deep-research-2025-06-26' }",
              "Implement getModelForDepth(depth: 'quick' | 'thorough'): string utility function",
              "Add cost estimation function: estimateResearchCost(depth: ResearchDepth): { min: number, max: number, average: number }",
              "Validate depth parameter in request handler with Zod schema"
            ],
            "middleware": [
              "Add request validation middleware to ensure depth parameter is valid enum value",
              "Log model selection with request ID for cost attribution and debugging"
            ],
            "shared": [
              "Define ResearchDepth type: 'quick' | 'thorough'",
              "Create RESEARCH_MODEL_CONFIG constant mapping depths to model names",
              "Define ResearchCostEstimate interface: { depth: ResearchDepth, model: string, estimatedCost: number }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchHandler.configureDepthParameter",
          "related_concepts": [
            "model selection",
            "cost optimization",
            "user preference handling",
            "research quality tiers"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.2",
          "description": "Implement the /api/tools/deep-research POST endpoint that accepts research queries, validates input, initializes OpenAI Responses API client, and manages the research request lifecycle including error handling and rate limiting",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "Endpoint accepts POST requests at /api/tools/deep-research with JSON body containing query, depth, and optional configuration",
            "Request body is validated with Zod schema requiring: query (string, min 10 chars, max 10000 chars), depth (optional enum)",
            "Returns 400 Bad Request with detailed validation errors for invalid input",
            "Returns 401 Unauthorized when OPENAI_API_KEY is missing or invalid",
            "Returns 429 Too Many Requests when rate limited, with Retry-After header",
            "Returns 202 Accepted for background jobs with job ID and polling URL",
            "Returns 500 Internal Server Error with safe error message (no API key exposure) for OpenAI API failures",
            "Successful response includes: jobId, status, estimatedCompletionTime, pollingUrl",
            "Request timeout is set appropriately (at least 5 minutes for initial request)"
          ],
          "implementation": {
            "frontend": [
              "Create useDeepResearch hook that handles POST request, loading states, and polling",
              "Implement research request form component with query input and depth selector",
              "Display loading state with estimated wait time when research is in progress",
              "Handle all error states with user-friendly messages and retry options"
            ],
            "backend": [
              "Create /api/tools/deep-research/route.ts with POST handler",
              "Initialize OpenAI client: new OpenAI({ apiKey: process.env.OPENAI_API_KEY })",
              "Implement request body parsing with Zod schema validation",
              "Call openai.responses.create() with properly formatted request",
              "Generate unique jobId (UUID) for tracking background research tasks",
              "Store job metadata in temporary storage (Vercel KV or in-memory cache) for polling",
              "Implement exponential backoff retry logic for transient OpenAI failures",
              "Add request logging with sanitized data (no PII, no API keys)"
            ],
            "middleware": [
              "Add API key validation middleware that checks OPENAI_API_KEY exists",
              "Implement rate limiting: max 10 research requests per user per hour",
              "Add request size limit middleware (max 100KB body)",
              "Log request metadata for analytics: timestamp, depth, query length, user identifier"
            ],
            "shared": [
              "Define DeepResearchRequest interface: { query: string, depth?: ResearchDepth, tools?: ToolConfig[] }",
              "Define DeepResearchJobResponse interface: { jobId: string, status: 'pending' | 'processing' | 'completed' | 'failed', pollingUrl: string }",
              "Create deepResearchRequestSchema Zod validator",
              "Define API error response types: ValidationError, AuthError, RateLimitError, InternalError"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchAPI.postToEndpoint",
          "related_concepts": [
            "REST API design",
            "OpenAI Responses API",
            "request validation",
            "error handling",
            "rate limiting"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.3",
          "description": "Configure the OpenAI Responses API request with web_search_preview tool enabled for internet research capability and background mode for handling long-running research tasks that can take tens of minutes",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "web_search_preview tool is always included in tools array for research requests",
            "web_search_preview can optionally accept domain restrictions via 'domains' parameter",
            "background: true is set by default for all deep research requests to prevent timeouts",
            "code_interpreter tool is optionally included when depth='thorough' for data analysis",
            "Tool configuration supports search_context_size parameter: 'low', 'medium', 'high'",
            "Developer system prompt is included to guide research format and citation requirements",
            "Reasoning summary mode is set to 'auto' for standard queries, 'detailed' for thorough depth",
            "Request properly formats input array with developer and user message roles"
          ],
          "implementation": {
            "frontend": [
              "Add optional 'Domain restrictions' input field for advanced users to limit search scope",
              "Add checkbox for 'Include code analysis' option (enables code_interpreter)",
              "Display tool configuration in research request summary before submission",
              "Show which tools are being used in the research progress indicator"
            ],
            "backend": [
              "Create buildToolsConfig function that assembles tools array based on request options",
              "Implement default tool config: [{ type: 'web_search_preview' }]",
              "Add optional code_interpreter when includeCodeAnalysis=true or depth='thorough'",
              "Create buildRequestInput function that formats messages array: [{ role: 'developer', content: [...] }, { role: 'user', content: [...] }]",
              "Define system prompt template for consistent research output format",
              "Implement domain filtering: { type: 'web_search_preview', domains: string[] }",
              "Set background: true in all responses.create() calls",
              "Configure reasoning: { summary: depth === 'thorough' ? 'detailed' : 'auto' }"
            ],
            "middleware": [
              "Validate domain restrictions are valid URLs/domains",
              "Sanitize user query for injection attempts before including in request"
            ],
            "shared": [
              "Define ToolConfig type matching OpenAI schema: { type: 'web_search_preview' | 'code_interpreter' | 'file_search', ... }",
              "Define WebSearchConfig interface: { domains?: string[], search_context_size?: 'low' | 'medium' | 'high', user_location?: { country: string } }",
              "Create DEFAULT_DEVELOPER_PROMPT constant with research formatting instructions",
              "Define ResearchRequestOptions interface: { includeCodeAnalysis?: boolean, domainRestrictions?: string[], searchContextSize?: 'low' | 'medium' | 'high' }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchHandler.configureToolsAndBackground",
          "related_concepts": [
            "OpenAI tools configuration",
            "web search",
            "background processing",
            "async job handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.4",
          "description": "Implement polling mechanism to check background research job completion status, and upon completion, extract the final report text and citation annotations from the nested response structure",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "Polling endpoint /api/tools/deep-research/[jobId]/status returns current job status",
            "Polling uses exponential backoff: start at 5s, max 60s, with jitter",
            "Maximum polling duration is 30 minutes before timeout with appropriate error",
            "When status='completed', response includes full report text and citations array",
            "Report text is extracted from response.output[-1].content[0].text",
            "Citations are extracted from response.output[-1].content[0].annotations",
            "Each citation includes: title, url, and optional snippet/quote",
            "Intermediate reasoning steps are optionally captured for transparency",
            "Failed jobs return error details with failure reason",
            "Polling handles 'in_progress' status gracefully with ETA updates"
          ],
          "implementation": {
            "frontend": [
              "Create useResearchPolling hook with configurable polling interval and max duration",
              "Display real-time progress updates: 'Searching...', 'Analyzing sources...', 'Synthesizing report...'",
              "Show intermediate reasoning steps in collapsible section during polling",
              "Render final report with formatted citations as clickable links",
              "Display citation list at bottom of report with numbered references",
              "Implement 'Cancel research' button that stops polling (job continues in background)",
              "Show toast notification when long-running research completes in background"
            ],
            "backend": [
              "Create /api/tools/deep-research/[jobId]/status/route.ts GET endpoint",
              "Implement checkJobStatus function that retrieves job from OpenAI",
              "Parse response.output array to extract latest content: response.output.at(-1)",
              "Extract report text: output.content.find(c => c.type === 'text')?.text",
              "Extract citations: output.content.find(c => c.type === 'text')?.annotations",
              "Transform annotations to Citation[]: { title: a.title, url: a.url, quote: a.quote }",
              "Capture intermediate steps by filtering output for type='reasoning' and type='web_search_call'",
              "Implement job status caching to reduce OpenAI API calls during frequent polling",
              "Store completed results for 24 hours to allow retrieval without re-polling"
            ],
            "middleware": [
              "Validate jobId format (UUID) in URL parameter",
              "Rate limit polling: max 1 request per second per job",
              "Add ETag/If-None-Match support for efficient polling without data transfer"
            ],
            "shared": [
              "Define Citation interface: { title: string, url: string, quote?: string, startIndex?: number, endIndex?: number }",
              "Define DeepResearchResult interface: { text: string, citations: Citation[], reasoningSteps?: ReasoningStep[], searchQueries?: string[] }",
              "Define JobStatus type: 'pending' | 'in_progress' | 'completed' | 'failed' | 'cancelled'",
              "Define PollResponse interface: { status: JobStatus, result?: DeepResearchResult, error?: string, progress?: { step: string, percentage?: number } }",
              "Create parseDeepResearchResponse utility function that handles all response extraction",
              "Define ReasoningStep interface: { summary: string, timestamp: string }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchHandler.pollAndExtractResults",
          "related_concepts": [
            "polling",
            "async job handling",
            "response parsing",
            "citation extraction",
            "webhook alternative"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    }
  ],
  "metadata": {
    "source": "agent_sdk_decomposition",
    "research_length": 29166
  }
}
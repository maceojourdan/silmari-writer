{
  "requirements": [
    {
      "id": "REQ_000",
      "description": "The system must integrate OpenAI Deep Research API capabilities for autonomous research with citation-rich reports",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_000.1",
          "description": "Implement POST requests to https://api.openai.com/v1/responses endpoint with o3-deep-research and o4-mini-deep-research models for autonomous research execution",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "POST requests are sent to https://api.openai.com/v1/responses with correct Authorization header using Bearer token",
            "Request body includes model field set to either 'o3-deep-research-2025-06-26' or 'o4-mini-deep-research-2025-06-26'",
            "OpenAI API key is securely retrieved from environment variable OPENAI_API_KEY",
            "Content-Type header is set to 'application/json' for all requests",
            "Request timeout is configured to handle long-running operations (minimum 120 seconds for non-background mode)",
            "HTTP errors (4xx, 5xx) are caught and transformed into typed DeepResearchApiError with status code and message",
            "Rate limit errors (429) trigger exponential backoff retry with maximum 3 attempts",
            "Network failures trigger retry with 2-second base delay and exponential backoff",
            "Response body is validated against expected DeepResearchResponse schema before processing",
            "API version header 'OpenAI-Beta: responses=v1' is included if required by API documentation",
            "Request/response logging captures timing, model used, and response size for monitoring",
            "Unit tests mock OpenAI API and verify correct request formation for both models"
          ],
          "implementation": {
            "frontend": [
              "Create DeepResearchPanel component with model selector dropdown (Quick/Thorough options)",
              "Implement ResearchQueryInput component with textarea for multi-line research queries",
              "Add ModelCostIndicator showing estimated cost ($3 for o4-mini, $30 for o3) before submission",
              "Create ResearchSubmitButton with loading state and cancel functionality",
              "Display API error messages in user-friendly format with suggested actions"
            ],
            "backend": [
              "Create /api/tools/deep-research/route.ts with POST handler accepting query and model parameters",
              "Implement DeepResearchClient class wrapping OpenAI SDK for Responses API calls",
              "Create buildDeepResearchRequest(query: string, model: DeepResearchModel) function",
              "Implement exponential backoff retry logic with configurable max attempts and delays",
              "Add request logging middleware capturing API latency and response metadata",
              "Create error handler mapping OpenAI API errors to user-friendly DeepResearchApiError"
            ],
            "middleware": [
              "Validate OPENAI_API_KEY environment variable is set before processing requests",
              "Implement request body validation using Zod schema for query and model fields",
              "Add rate limiting (10 requests per minute per user) to prevent API abuse",
              "Log all API calls with anonymized query summaries for audit trail"
            ],
            "shared": [
              "Define DeepResearchModel type: 'o3-deep-research-2025-06-26' | 'o4-mini-deep-research-2025-06-26'",
              "Create DeepResearchRequest interface with query: string, model: DeepResearchModel, tools?: ToolConfig[]",
              "Define DeepResearchApiError class with code, message, statusCode, retryable fields",
              "Create DEEP_RESEARCH_MODELS constant mapping user-friendly names to API model strings",
              "Define DEEP_RESEARCH_ENDPOINT constant: 'https://api.openai.com/v1/responses'",
              "Create deepResearchRequestSchema Zod validator for request body validation"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchAPI.implementResponsesEndpoint",
          "related_concepts": [
            "OpenAI Responses API",
            "HTTP client configuration",
            "API authentication",
            "request/response lifecycle",
            "model selection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.2",
          "description": "Support developer and user role message inputs with input_text content type for proper prompt structuring in Deep Research requests",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "Input array supports objects with role: 'developer' for system-level instructions",
            "Input array supports objects with role: 'user' for research queries",
            "Each message content is formatted as array with {type: 'input_text', text: string} objects",
            "Developer messages can set research guidelines, output format preferences, and domain constraints",
            "User messages contain the actual research query to be investigated",
            "Multiple user messages in sequence are supported for multi-turn context",
            "Empty or whitespace-only text content returns validation error before API call",
            "Maximum text length per message is enforced (32,000 characters based on API limits)",
            "Developer role messages are optional - user role messages are required",
            "Message order is preserved: developer messages should precede user messages",
            "Special characters and unicode in text content are properly encoded",
            "Unit tests verify correct message structure formation for various input combinations"
          ],
          "implementation": {
            "frontend": [
              "Create ResearchContextInput component for optional developer/system instructions",
              "Implement ResearchQueryInput for primary user research question",
              "Add character counter showing remaining characters (32K limit) for each input field",
              "Create ResearchPresetSelector with common developer instruction templates",
              "Implement input validation with inline error messages for empty/too-long content",
              "Add 'Preview API Request' button showing formatted message structure"
            ],
            "backend": [
              "Create buildInputMessages(developerContext?: string, userQuery: string) function",
              "Implement InputMessage interface with role and content array structure",
              "Add input sanitization to escape potentially problematic characters",
              "Create validateInputMessage(message: InputMessage) function with detailed error messages",
              "Implement message truncation strategy for overly long inputs with user warning",
              "Add developer instruction templates stored in database for reuse"
            ],
            "middleware": [
              "Validate at least one user role message is present in input array",
              "Enforce maximum message length limits per role (developer: 10K, user: 32K)",
              "Sanitize text content to prevent injection attacks",
              "Log message role distribution for analytics (how often developer context is used)"
            ],
            "shared": [
              "Define MessageRole type: 'developer' | 'user'",
              "Create InputTextContent interface: { type: 'input_text', text: string }",
              "Define InputMessage interface: { role: MessageRole, content: InputTextContent[] }",
              "Create MAX_DEVELOPER_MESSAGE_LENGTH constant (10000)",
              "Create MAX_USER_MESSAGE_LENGTH constant (32000)",
              "Define DEFAULT_DEVELOPER_CONTEXT constant with sensible research guidelines"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchInput.supportMessageRoles",
          "related_concepts": [
            "Message roles",
            "System prompts",
            "User queries",
            "Content type formatting",
            "Prompt engineering"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.3",
          "description": "Configure reasoning summary options (auto or detailed) based on research depth preference to control the verbosity of intermediate thinking steps returned by the API",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "Request includes reasoning object with summary field set to 'auto' or 'detailed'",
            "When summary='auto', API returns concise reasoning summaries suitable for quick overview",
            "When summary='detailed', API returns comprehensive reasoning with full thought process",
            "Default reasoning summary is 'auto' when user does not specify preference",
            "For depth='thorough' requests, reasoning defaults to 'detailed' for full transparency",
            "For depth='quick' requests, reasoning defaults to 'auto' for efficiency",
            "User can override default reasoning level regardless of depth selection",
            "Invalid reasoning summary values return validation error before API call",
            "Reasoning summary preference is stored in user settings for future requests",
            "UI clearly explains the difference between auto and detailed reasoning options",
            "Token usage difference between auto and detailed is displayed to user",
            "Unit tests verify correct reasoning object formation for all option combinations"
          ],
          "implementation": {
            "frontend": [
              "Create ReasoningSummarySelector component with radio buttons for auto/detailed",
              "Add ReasoningExplanationTooltip explaining what each option provides",
              "Implement ReasoningPreview showing sample output format for each option",
              "Create TokenUsageEstimate showing approximate additional tokens for detailed mode",
              "Add user preference toggle to remember reasoning setting across sessions",
              "Display reasoning level badge in research request confirmation UI"
            ],
            "backend": [
              "Create buildReasoningConfig(summary: ReasoningSummary, depth?: ResearchDepth) function",
              "Implement getDefaultReasoningSummary(depth: ResearchDepth): ReasoningSummary logic",
              "Add reasoning configuration to DeepResearchRequest interface",
              "Store user reasoning preference in user settings table",
              "Create validateReasoningSummary(value: unknown) function with type guard",
              "Log reasoning configuration choices for product analytics"
            ],
            "middleware": [
              "Validate reasoning.summary is either 'auto' or 'detailed'",
              "Apply default reasoning based on depth when not explicitly specified",
              "Enforce consistency between depth and reasoning (warn if mismatch)"
            ],
            "shared": [
              "Define ReasoningSummary type: 'auto' | 'detailed'",
              "Create ReasoningConfig interface: { summary: ReasoningSummary }",
              "Define REASONING_DEPTH_DEFAULTS constant: { quick: 'auto', thorough: 'detailed' }",
              "Create reasoningSummarySchema Zod validator",
              "Define REASONING_TOKEN_ESTIMATES constant: { auto: 500, detailed: 2000 }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchConfig.configureReasoningSummary",
          "related_concepts": [
            "Reasoning transparency",
            "Chain-of-thought",
            "Output verbosity",
            "Token usage optimization",
            "Research provenance"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.4",
          "description": "Enable background mode (background: true) for long-running research tasks that can take tens of minutes, allowing non-blocking request submission and async result retrieval",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "Request includes background: true parameter to enable async execution mode",
            "API immediately returns job metadata including unique job ID and status URL",
            "Job ID is in UUID format and uniquely identifies the research task",
            "Status URL follows pattern /api/tools/deep-research/{jobId}/status",
            "Initial response includes estimated completion time range (e.g., 5-30 minutes)",
            "Job metadata includes created_at timestamp and model used",
            "Background mode is enabled by default for all deep research requests",
            "Non-background mode (background: false) is supported for testing but discouraged in production",
            "Job metadata is stored in database for persistence across server restarts",
            "User can view list of their pending/completed research jobs",
            "Jobs are automatically cleaned up after 24 hours to free storage",
            "Unit tests verify background mode request formation and response parsing"
          ],
          "implementation": {
            "frontend": [
              "Create ResearchJobTracker component showing active research jobs with status",
              "Implement ResearchJobCard with job ID, query preview, status, and elapsed time",
              "Add ResearchJobQueue page listing all user's pending and completed jobs",
              "Create EstimatedTimeDisplay showing remaining time based on model and query complexity",
              "Implement job cancellation UI with confirmation dialog",
              "Add browser notification support for job completion (with user permission)"
            ],
            "backend": [
              "Create /api/tools/deep-research/route.ts returning job metadata for background requests",
              "Implement ResearchJobService class managing job lifecycle (create, update, complete, fail)",
              "Create research_jobs database table with id, user_id, query, model, status, created_at, completed_at, result",
              "Implement generateJobId() function returning UUID v4",
              "Create buildBackgroundRequest() ensuring background: true is included",
              "Add job cleanup cron job removing jobs older than 24 hours",
              "Implement job status transitions: pending \u2192 processing \u2192 completed/failed"
            ],
            "middleware": [
              "Validate background mode is enabled (warn if disabled in production)",
              "Ensure job ID is unique before insertion",
              "Authenticate user for job status queries to prevent unauthorized access",
              "Rate limit job creation (max 5 concurrent jobs per user)"
            ],
            "shared": [
              "Define ResearchJobStatus type: 'pending' | 'processing' | 'completed' | 'failed'",
              "Create ResearchJob interface: { id: string, userId: string, query: string, model: DeepResearchModel, status: ResearchJobStatus, createdAt: Date, completedAt?: Date, result?: DeepResearchResult, error?: string }",
              "Define BackgroundJobResponse interface: { jobId: string, statusUrl: string, estimatedMinutes: number, createdAt: string }",
              "Create JOB_RETENTION_HOURS constant (24)",
              "Define MAX_CONCURRENT_JOBS constant (5)"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchExecution.enableBackgroundMode",
          "related_concepts": [
            "Async processing",
            "Long-running jobs",
            "Non-blocking requests",
            "Job queuing",
            "Task management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_000.5",
          "description": "Implement polling mechanism for background task completion notification, allowing clients to check job status at intervals until research completes or fails",
          "type": "sub_process",
          "parent_id": "REQ_000",
          "children": [],
          "acceptance_criteria": [
            "GET endpoint /api/tools/deep-research/{jobId}/status returns current job status",
            "Status response includes: status ('pending'|'processing'|'completed'|'failed'), progress info, result or error",
            "Polling interval starts at 5 seconds and increases to 30 seconds for long-running jobs (adaptive)",
            "Client stops polling when status is 'completed' or 'failed'",
            "Progress object includes current step description and optional percentage (0-100)",
            "Completed status includes full DeepResearchResult with text, citations, and reasoning steps",
            "Failed status includes error message and error code for debugging",
            "Status endpoint returns 404 for non-existent job IDs with clear error message",
            "Status endpoint returns 403 for job IDs belonging to other users",
            "Client handles network errors during polling with automatic retry",
            "UI displays real-time progress updates as polling retrieves new status",
            "Unit tests verify polling logic with various status transitions"
          ],
          "implementation": {
            "frontend": [
              "Create useResearchJobPolling(jobId) hook managing polling lifecycle",
              "Implement adaptive polling interval logic (5s \u2192 10s \u2192 30s based on elapsed time)",
              "Create ResearchProgressIndicator component showing current step and percentage",
              "Add polling status indicator (spinning icon with 'Checking...' text)",
              "Implement automatic polling stop on completion/failure with result display",
              "Create PollingErrorBanner for network failures with manual retry button",
              "Add 'Cancel Research' button that stops polling and optionally cancels job"
            ],
            "backend": [
              "Create /api/tools/deep-research/[jobId]/status/route.ts GET endpoint",
              "Implement getJobStatus(jobId: string, userId: string) service method",
              "Create updateJobProgress(jobId: string, step: string, percentage?: number) method",
              "Implement parseOpenAIProgress(response) to extract progress from streaming updates",
              "Add job ownership validation before returning status",
              "Create error response formatting for 404 (not found) and 403 (forbidden) cases",
              "Implement caching layer for completed job results to reduce database queries"
            ],
            "middleware": [
              "Authenticate user for all status endpoint requests",
              "Validate jobId format (UUID) before database query",
              "Log polling frequency per job for analytics",
              "Implement conditional response (ETag/If-None-Match) to reduce bandwidth for unchanged status"
            ],
            "shared": [
              "Define JobStatusResponse interface: { status: ResearchJobStatus, progress?: { step: string, percentage?: number }, result?: DeepResearchResult, error?: { code: string, message: string } }",
              "Create PollingConfig interface: { initialIntervalMs: number, maxIntervalMs: number, intervalMultiplier: number }",
              "Define DEFAULT_POLLING_CONFIG constant: { initialIntervalMs: 5000, maxIntervalMs: 30000, intervalMultiplier: 1.5 }",
              "Create JobErrorCode enum: NOT_FOUND, FORBIDDEN, INTERNAL_ERROR, RATE_LIMITED, TIMEOUT",
              "Define calculateNextInterval(currentInterval: number, config: PollingConfig) utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchExecution.implementPollingMechanism",
          "related_concepts": [
            "Polling patterns",
            "Job status tracking",
            "Client-side intervals",
            "Progress indicators",
            "Completion handling"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_001",
      "description": "The system must support Deep Research tool configuration including web_search_preview, code_interpreter, file_search, and mcp tools",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_001.1",
          "description": "Implement web_search_preview tool with configurable domains, search_context_size (low/medium/high), and user_location parameters for Deep Research API integration",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "web_search_preview tool can be enabled/disabled in Deep Research requests via UI toggle",
            "domains parameter accepts an array of allowed domains (whitelist mode) OR blocked domains (blacklist mode) with clear mode indicator",
            "search_context_size parameter supports 'low', 'medium', 'high' enum values that control context token usage and API costs",
            "user_location parameter accepts ISO 3166-1 alpha-2 country code (required) and optional city/region strings for geo-relevant results",
            "Domain patterns are validated using regex before API call (format: example.com, not https://example.com/path)",
            "Invalid domain patterns return validation errors with specific pattern that failed",
            "Empty domains array defaults to unrestricted web search with confirmation prompt",
            "Configuration persists to user preferences store across conversation sessions",
            "UI displays active search restrictions summary before research begins (e.g., '3 domains whitelisted, US location')",
            "search_context_size displays estimated token cost tooltip: low (~500 tokens), medium (~2000 tokens), high (~5000 tokens)",
            "Location autocomplete uses a country/city database for valid selections",
            "Tool configuration is serialized correctly for OpenAI Responses API format: { type: 'web_search_preview', domains: [...], search_context_size: '...', user_location: {...} }"
          ],
          "implementation": {
            "frontend": [
              "WebSearchConfigPanel component with domain whitelist/blacklist input fields using chip/tag input pattern",
              "DomainModeToggle component switching between 'Include only' (whitelist) and 'Exclude' (blacklist) modes",
              "SearchContextSizeSelector dropdown with low/medium/high options displaying token cost estimates and recommended use cases",
              "UserLocationPicker component with country dropdown (ISO codes) and optional city input with debounced autocomplete",
              "ToolConfigurationModal that aggregates all web_search_preview settings with preview of final API payload",
              "ActiveSearchFiltersChip component showing compact summary of current restrictions in Deep Research UI",
              "Form validation displaying inline errors for invalid domain patterns",
              "Reset to defaults button clearing all web search customizations"
            ],
            "backend": [
              "POST /api/tools/deep-research/configure endpoint accepting WebSearchPreviewConfig payload",
              "GET /api/tools/deep-research/config endpoint returning user's saved web_search_preview preferences",
              "WebSearchPreviewConfig Zod schema with strict validation for all fields",
              "validateDomainPattern(domain: string): boolean function checking domain format without protocol/path",
              "buildWebSearchTool(config: WebSearchPreviewConfig): WebSearchPreviewTool function constructing OpenAI-compatible tool object",
              "mergeConfigs(userPrefs: WebSearchPreviewConfig, requestOverrides: Partial<WebSearchPreviewConfig>): WebSearchPreviewConfig for request-time customization",
              "Domain pattern sanitization removing accidental protocols (https://) or paths (/page)"
            ],
            "middleware": [
              "validateDomains middleware ensuring array contains valid domain formats using DOMAIN_PATTERN_REGEX",
              "sanitizeUserLocation middleware preventing XSS/injection in city/region strings",
              "rateLimitConfigChanges middleware limiting configuration updates to 10/minute per user",
              "validateSearchContextSize middleware ensuring value is one of allowed enum values"
            ],
            "shared": [
              "WebSearchPreviewConfig interface: { enabled: boolean, domains: string[], domainMode: 'whitelist' | 'blacklist', search_context_size: SearchContextSize, user_location?: UserLocation }",
              "SearchContextSize union type: 'low' | 'medium' | 'high'",
              "UserLocation interface: { country: string (ISO 3166-1 alpha-2), city?: string, region?: string }",
              "DOMAIN_PATTERN_REGEX constant: /^(?:[a-z0-9](?:[a-z0-9-]{0,61}[a-z0-9])?\\.)+[a-z0-9][a-z0-9-]{0,61}[a-z0-9]$/i",
              "DEFAULT_WEB_SEARCH_CONFIG constant: { enabled: true, domains: [], domainMode: 'whitelist', search_context_size: 'medium', user_location: undefined }",
              "SEARCH_CONTEXT_SIZE_TOKENS map: { low: 500, medium: 2000, high: 5000 }",
              "ISO_COUNTRY_CODES constant array for validation"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.webSearchPreview",
          "related_concepts": [
            "OpenAI Responses API",
            "web search filtering",
            "domain whitelisting/blacklisting",
            "geolocation context",
            "search result ranking",
            "token consumption optimization"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.2",
          "description": "Support code_interpreter tool execution for Deep Research with session cost tracking ($0.03/session), result extraction, and file download capabilities",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "code_interpreter tool can be enabled/disabled in Deep Research requests via simple toggle switch",
            "No additional configuration parameters required - tool object is simply { type: 'code_interpreter' }",
            "Code execution results are extracted from response.output array by filtering items where type === 'code_interpreter_call'",
            "Execution stdout/stderr outputs are captured and displayed in formatted code blocks",
            "Session cost ($0.03/session) is tracked per Deep Research request and displayed to user",
            "Cumulative session costs are aggregated in user's usage dashboard",
            "Code execution errors are parsed and displayed with syntax-highlighted stack traces",
            "Generated files (charts, CSVs, images) are extracted from response and made downloadable",
            "Tool is disabled by default to prevent unexpected costs - requires explicit user opt-in",
            "Cost acknowledgment checkbox required before first-time enablement",
            "Session status indicator shows 'Code Running' during active code execution phases",
            "Execution timeout errors (if any) display clear message with execution time limit info"
          ],
          "implementation": {
            "frontend": [
              "CodeInterpreterToggle component with on/off switch and '$0.03/session' cost label",
              "FirstTimeEnablementModal with cost acknowledgment checkbox required for initial opt-in",
              "CodeExecutionResultViewer component rendering stdout in <pre> blocks with syntax highlighting",
              "CodeErrorDisplay component showing stderr with Python stack trace formatting",
              "CodeInterpreterSessionIndicator badge showing 'Code Running...' during execution",
              "GeneratedFilesList component displaying downloadable files with icons by type (CSV, PNG, etc.)",
              "DownloadFileButton component triggering file download from blob URL",
              "SessionCostDisplay showing current request's code interpreter cost",
              "CumulativeCostWidget in user profile showing total code_interpreter spend"
            ],
            "backend": [
              "buildCodeInterpreterTool(): { type: 'code_interpreter' } simple function returning tool config",
              "extractCodeInterpreterResults(output: OutputItem[]): CodeExecutionResult[] filtering and parsing code execution outputs",
              "parseCodeOutput(item: CodeInterpreterCallItem): { stdout: string, stderr: string, files: GeneratedFile[] } extracting execution details",
              "POST /api/tools/code-interpreter/files/:fileId endpoint serving generated files from temporary storage",
              "storeGeneratedFile(content: Buffer, filename: string): string uploading to Vercel Blob and returning download URL",
              "SessionCostTracker service incrementing user's code_interpreter_cost by $0.03 per session",
              "GET /api/usage/code-interpreter endpoint returning user's cumulative code interpreter costs"
            ],
            "middleware": [
              "validateCodeInterpreterAcknowledgment middleware checking user has accepted cost terms",
              "logCodeInterpreterUsage middleware recording session usage for billing: { userId, requestId, timestamp, cost: 0.03 }",
              "cleanupGeneratedFiles middleware deleting temporary files after 24-hour retention period"
            ],
            "shared": [
              "CodeInterpreterConfig interface: { enabled: boolean }",
              "CodeExecutionResult interface: { stdout: string, stderr: string, files: GeneratedFile[], executionTimeMs?: number, status: 'success' | 'error' | 'timeout' }",
              "GeneratedFile interface: { filename: string, contentType: string, downloadUrl: string, sizeBytes: number }",
              "CodeInterpreterCallItem interface matching OpenAI's output item schema for code execution",
              "CODE_INTERPRETER_SESSION_COST constant: 0.03",
              "CODE_INTERPRETER_FILE_RETENTION_HOURS constant: 24",
              "UserCodeInterpreterUsage interface: { userId: string, totalSessions: number, totalCost: number, lastUsed: Date }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.codeInterpreter",
          "related_concepts": [
            "sandboxed code execution",
            "Python runtime environment",
            "data analysis workflows",
            "computation result extraction",
            "session-based billing",
            "generated file handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.3",
          "description": "Enable file_search tool with vector_store_ids support allowing maximum of 2 vector stores for semantic document retrieval during Deep Research",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "file_search tool accepts vector_store_ids parameter with array of 1-2 vector store ID strings",
            "Attempting to configure more than 2 vector stores returns clear validation error: 'Maximum 2 vector stores allowed'",
            "Vector store IDs are validated against OpenAI's vector store API to verify existence before research begins",
            "Invalid or non-existent vector_store_id returns error: 'Vector store {id} not found or access denied'",
            "UI displays user's existing vector stores with name, document count, and creation date for selection",
            "Vector store multi-select component enforces max 2 selection with visual indicator",
            "File search results include source document references with filename, page/chunk number, and relevance score",
            "Empty vector_store_ids array (or omitted) disables file_search tool from request",
            "Vector store ID format is validated using regex before API call (format: vs_xxxxx)",
            "Selected vector stores persist as user preference for future research tasks",
            "UI allows creating new vector store via file upload flow integrated with OpenAI Vector Stores API",
            "Vector store status (ready/processing/failed) is checked and displayed before allowing selection"
          ],
          "implementation": {
            "frontend": [
              "VectorStoreSelector component with multi-select dropdown enforcing max 2 selections",
              "VectorStoreCard component displaying store name, file count, status badge, and last updated timestamp",
              "VectorStoreLimitWarning alert shown when user attempts to select 3rd store",
              "CreateVectorStoreModal with file upload dropzone supporting PDF, TXT, DOCX formats",
              "VectorStoreUploadProgress component showing indexing status for newly created stores",
              "FileSearchResultsPanel component rendering retrieved chunks with source attribution and relevance bars",
              "ChunkPreviewCard showing extracted text with highlighted search terms and 'View source' link",
              "VectorStoreStatusBadge component with colors: green (ready), yellow (processing), red (failed)",
              "RemoveVectorStoreButton for clearing selection from current research config"
            ],
            "backend": [
              "GET /api/vector-stores endpoint listing user's vector stores from OpenAI API with caching",
              "POST /api/vector-stores endpoint creating new vector store and uploading files to OpenAI",
              "GET /api/vector-stores/:id/status endpoint checking individual store processing status",
              "buildFileSearchTool(vectorStoreIds: string[]): FileSearchTool function with 2-store limit validation",
              "validateVectorStoreIds(ids: string[]): Promise<ValidationResult> verifying stores exist via OpenAI API",
              "extractFileSearchResults(output: OutputItem[]): FileSearchResult[] parsing retrieved chunks from response",
              "mapChunkToSource(chunk: RetrievedChunk): { filename: string, pageNumber?: number, chunkIndex: number } extracting source info"
            ],
            "middleware": [
              "validateVectorStoreIdsLength middleware rejecting requests with > 2 store IDs",
              "verifyVectorStoreAccess middleware checking user has access to specified stores",
              "checkVectorStoreStatus middleware ensuring selected stores have status 'ready' before request",
              "validateVectorStoreIdFormat middleware checking ID matches vs_[a-zA-Z0-9]+ pattern"
            ],
            "shared": [
              "FileSearchConfig interface: { enabled: boolean, vector_store_ids: string[] }",
              "VectorStore interface: { id: string, name: string, fileCount: number, status: 'ready' | 'processing' | 'failed', createdAt: Date, bytesUsed: number }",
              "FileSearchResult interface: { chunkText: string, sourceFile: string, pageNumber?: number, chunkIndex: number, relevanceScore: number }",
              "RetrievedChunk interface matching OpenAI's file_search response format",
              "MAX_VECTOR_STORES constant: 2",
              "VECTOR_STORE_ID_REGEX constant: /^vs_[a-zA-Z0-9]+$/",
              "VectorStoreValidationResult interface: { valid: boolean, errors: { storeId: string, error: string }[] }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.fileSearch",
          "related_concepts": [
            "vector embeddings",
            "semantic search",
            "document retrieval",
            "RAG (Retrieval Augmented Generation) patterns",
            "OpenAI vector stores",
            "chunk-based document indexing"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.4",
          "description": "Implement mcp (Model Context Protocol) tool with server_url validation (HTTPS required) and require_approval settings for secure custom data source integration",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "mcp tool accepts server_url parameter pointing to MCP-compliant server endpoint",
            "server_url is strictly validated for HTTPS protocol - HTTP URLs are rejected with error: 'MCP servers require HTTPS'",
            "server_url format is validated as valid URL with proper hostname",
            "require_approval boolean (default: true) controls whether MCP tool calls need explicit user approval",
            "When require_approval is true, UI shows approval dialog before each MCP tool invocation with tool name and input preview",
            "MCP server connection is verified via health check endpoint before research begins",
            "Failed MCP server connections (timeout, 4xx, 5xx) display clear error with server URL and status code",
            "Multiple MCP tools can be configured with different server URLs in single research request",
            "MCP tool invocation history is logged with timestamp, server_url, tool_name, input, output for audit",
            "User can configure 'trusted' MCP servers that bypass require_approval for future requests",
            "Trusted server list is stored per-user and manageable via settings UI",
            "MCP server URL is checked against configurable blocklist of known malicious endpoints",
            "Approval timeout (60 seconds default) auto-rejects pending approvals to prevent blocking"
          ],
          "implementation": {
            "frontend": [
              "McpServerConfigForm component with server_url input field and HTTPS validation indicator",
              "RequireApprovalToggle checkbox with explanation tooltip about approval workflow",
              "McpConnectionTestButton triggering server health check with success/failure feedback",
              "McpApprovalDialog modal shown during research when MCP tool is invoked (if require_approval=true)",
              "ApprovalDialogContent showing tool_name, input_preview (truncated to 500 chars), and Approve/Reject buttons",
              "ApprovalTimeoutIndicator countdown showing remaining seconds before auto-reject",
              "TrustedMcpServersList in settings page with add/remove/edit capabilities",
              "McpToolInvocationLog expandable panel showing history of MCP calls during current research session",
              "McpServerStatusIndicator badge showing connection health (green=healthy, red=unreachable, yellow=slow)"
            ],
            "backend": [
              "POST /api/tools/mcp/verify endpoint testing MCP server connectivity and schema compliance",
              "buildMcpTool(serverUrl: string, requireApproval: boolean): McpTool function constructing OpenAI-compatible tool config",
              "McpServerRegistry service storing user's configured MCP servers in database",
              "POST /api/tools/mcp/servers endpoint to add new MCP server configuration",
              "DELETE /api/tools/mcp/servers/:serverId endpoint to remove MCP server",
              "PATCH /api/tools/mcp/servers/:serverId/trust endpoint to toggle trusted status",
              "McpApprovalQueue service managing pending approval requests with WebSocket notifications",
              "POST /api/tools/mcp/approvals/:invocationId/approve endpoint for user approval submission",
              "POST /api/tools/mcp/approvals/:invocationId/reject endpoint for user rejection",
              "McpInvocationLogger service recording all MCP calls with full context for audit trail",
              "GET /api/tools/mcp/invocations endpoint returning paginated invocation history"
            ],
            "middleware": [
              "validateMcpServerUrl middleware enforcing HTTPS protocol requirement",
              "validateUrlFormat middleware checking URL is well-formed with valid hostname",
              "checkMcpBlocklist middleware rejecting URLs matching known malicious server patterns",
              "enforceTrustRequirement middleware auto-setting require_approval=true for non-trusted servers",
              "mcpApprovalTimeout middleware auto-rejecting approvals not responded within 60 seconds"
            ],
            "shared": [
              "McpToolConfig interface: { server_url: string, require_approval: boolean }",
              "McpServer interface: { id: string, url: string, name: string, description?: string, trusted: boolean, lastVerified?: Date, healthStatus: 'healthy' | 'unreachable' | 'unknown' }",
              "McpInvocation interface: { id: string, serverUrl: string, toolName: string, input: unknown, output?: unknown, timestamp: Date, approvedBy?: string, status: 'pending' | 'approved' | 'rejected' | 'completed' }",
              "McpApprovalRequest interface: { invocationId: string, toolName: string, inputPreview: string, serverUrl: string, expiresAt: Date, status: 'pending' | 'approved' | 'rejected' | 'expired' }",
              "MCP_HTTPS_REQUIRED constant: true",
              "DEFAULT_REQUIRE_APPROVAL constant: true",
              "MCP_APPROVAL_TIMEOUT_SECONDS constant: 60",
              "MCP_BLOCKLIST_PATTERNS constant array of regex patterns for blocked servers"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolConfig.mcpTool",
          "related_concepts": [
            "Model Context Protocol",
            "external tool servers",
            "approval workflows",
            "custom data sources",
            "server authentication",
            "security whitelisting"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_001.5",
          "description": "Process and display intermediate reasoning steps and web_search_call queries for transparency into the Deep Research agent's investigation process",
          "type": "sub_process",
          "parent_id": "REQ_001",
          "children": [],
          "acceptance_criteria": [
            "Intermediate reasoning steps are extracted by filtering response.output items where type === 'reasoning'",
            "Each reasoning step's summary field is extracted and displayed chronologically",
            "Both 'auto' and 'detailed' reasoning summary modes are supported with appropriate display formatting",
            "web_search_call items are filtered from response.output where type === 'web_search_call'",
            "Search query text is extracted from each web_search_call item's query field",
            "Reasoning steps and search queries are displayed in unified timeline view showing research progression",
            "Timeline entries include step type icon (\ud83d\udcad reasoning, \ud83d\udd0d search), timestamp/order, and content",
            "'Show AI's thinking' toggle allows user to expand/collapse transparency panel",
            "Transparency panel is collapsed by default to reduce visual clutter",
            "Real-time streaming updates append new reasoning steps and search queries as they occur (background mode)",
            "Step count badges show total reasoning steps and search queries performed",
            "Individual reasoning steps are expandable/collapsible for detailed vs summary view",
            "Search queries are displayed as clickable chips that could trigger manual re-search (future feature)",
            "Empty reasoning or search arrays display 'No intermediate steps recorded' message"
          ],
          "implementation": {
            "frontend": [
              "ResearchTransparencyPanel component containing collapsible reasoning and search sections",
              "ShowThinkingToggle button with icon that expands/collapses transparency panel",
              "ResearchTimeline component rendering chronological list of reasoning steps and searches",
              "TimelineEntry component with type icon, order number, and content preview",
              "ReasoningStepCard expandable component showing full reasoning summary text",
              "SearchQueryChip compact component displaying search query with \ud83d\udd0d icon",
              "StepCountBadges component showing '5 reasoning steps, 12 searches' summary",
              "StreamingTimelineUpdater component appending new entries via SSE/WebSocket during background research",
              "EmptyTransparencyState component for cases with no intermediate steps",
              "TimelineFilter allowing user to show only reasoning, only searches, or both"
            ],
            "backend": [
              "ReasoningStepProcessor.extractIntermediateSteps(output: OutputItem[]): ReasoningStep[] filtering and parsing reasoning items",
              "WebSearchCallLogger.captureSearchQueries(output: OutputItem[]): SearchQuery[] filtering and parsing search calls",
              "combineTransparencyData(reasoning: ReasoningStep[], searches: SearchQuery[]): TimelineEntry[] merging into unified timeline",
              "GET /api/tools/deep-research/:requestId/transparency endpoint returning reasoning steps and search queries",
              "SSE endpoint /api/tools/deep-research/:requestId/transparency/stream for real-time updates during background research",
              "TransparencyDataStore service caching transparency data for completed research requests",
              "calculateResearchMetrics(data: TransparencyData): { reasoningSteps: number, searchQueries: number, uniqueSearchTerms: number }"
            ],
            "middleware": [
              "SSE connection middleware managing streaming transparency updates",
              "transparencyDataCache middleware caching parsed transparency data to avoid re-processing",
              "sanitizeReasoningSummary middleware removing any potentially sensitive auto-generated content"
            ],
            "shared": [
              "ReasoningStep interface: { index: number, summary: string, timestamp?: Date, type: 'planning' | 'analyzing' | 'synthesizing' | 'unknown' }",
              "SearchQuery interface: { query: string, index: number, timestamp?: Date }",
              "TimelineEntry interface: { type: 'reasoning' | 'search', index: number, content: string, timestamp?: Date }",
              "TransparencyData interface: { reasoningSteps: ReasoningStep[], searchQueries: SearchQuery[], totalSteps: number }",
              "OutputItem union type covering 'reasoning' | 'web_search_call' | 'message' | 'code_interpreter_call' types",
              "REASONING_STEP_TYPES array: ['planning', 'analyzing', 'synthesizing'] for classification",
              "ResearchMetrics interface: { reasoningSteps: number, searchQueries: number, uniqueSearchTerms: number, estimatedDuration?: number }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchTransparency.displayIntermediateSteps",
          "related_concepts": [
            "reasoning transparency",
            "chain-of-thought visibility",
            "agentic workflow logging",
            "research methodology audit",
            "search query logging",
            "user trust building"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_002",
      "description": "The system must integrate OpenAI Image Creation API using gpt-image-1.5 model with full parameter support",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_002.1",
          "description": "Implement POST requests to https://api.openai.com/v1/images/generations with proper authentication and error handling",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "POST requests are sent to https://api.openai.com/v1/images/generations with Content-Type: application/json header",
            "Authorization header includes Bearer token with OPENAI_API_KEY from environment variables",
            "Request body includes required fields: model, prompt, and optional fields: n, size, quality, output_format, background",
            "Prompt length validation enforces 32K character limit for GPT Image models",
            "Request timeout is configurable with default of 120 seconds for image generation",
            "Rate limit errors (429) trigger exponential backoff retry logic with maximum 3 attempts using 10s base delay",
            "API errors return structured ToolError responses with code, message, retryable status, and suggestedAction",
            "Network errors are caught and wrapped in consistent ToolError format",
            "Request logging captures prompt (truncated to 100 chars), model, size, quality for debugging without exposing sensitive data",
            "Successful requests return parsed JSON response with b64_json field extracted",
            "Invalid API key returns 401 status with INVALID_API_KEY error code",
            "Missing OPENAI_API_KEY environment variable returns 500 with CONFIG_ERROR code"
          ],
          "implementation": {
            "frontend": [
              "ImageGenerationLoading component with animated spinner showing generation progress (can take 10-30 seconds)",
              "ImageGenerationError component displaying error message, error code, and retry button for retryable errors",
              "ImageGenerationProgress component showing request status (pending, generating, complete, failed) with status icon",
              "useImageGeneration hook managing loading state, error state, and retry logic",
              "Retry button with cooldown timer preventing rapid retry attempts",
              "Toast notification for successful generation with preview thumbnail"
            ],
            "backend": [
              "Create /api/tools/generate-image/route.ts following existing route.ts pattern from /api/generate",
              "Instantiate OpenAI client with API key from process.env.OPENAI_API_KEY",
              "Implement Zod schema for ImageGenerationRequest validation (model, prompt, size, quality, output_format, background, n)",
              "Create ImageGenerationError class extending Error with code, retryable, suggestedAction properties",
              "Implement generateImageWithRetry function with exponential backoff (10s base for rate limits, 2s for other errors)",
              "Create makeImageRequest function wrapping openai.images.generate() with error mapping",
              "Add request timeout wrapper using AbortController with configurable duration (default 120000ms)",
              "Implement request logging middleware capturing prompt preview, model, parameters without full prompt text"
            ],
            "middleware": [
              "API key validation middleware checking OPENAI_API_KEY exists before processing request",
              "Request body size validation ensuring prompt is under 32K characters",
              "Application-level rate limiting using in-memory counter or Redis to prevent API quota exhaustion",
              "Request logging middleware writing to console/log service with timestamp, model, size, quality, duration"
            ],
            "shared": [
              "ImageGenerationRequest interface: { model: ImageModel, prompt: string, size?: ImageSize, quality?: ImageQuality, output_format?: ImageFormat, background?: ImageBackground, n?: number }",
              "ImageGenerationResponse interface: { b64_json: string, revised_prompt?: string }",
              "ToolError type: { code: 'RATE_LIMIT' | 'INVALID_REQUEST' | 'API_ERROR' | 'TIMEOUT' | 'CONFIG_ERROR' | 'INVALID_API_KEY', message: string, retryable: boolean, suggestedAction?: string }",
              "API_ENDPOINTS constant: { IMAGE_GENERATION: 'https://api.openai.com/v1/images/generations' }",
              "DEFAULT_IMAGE_TIMEOUT constant: 120000 (milliseconds)",
              "MAX_PROMPT_LENGTH constant: 32000 (characters)",
              "MAX_RETRIES constant: 3",
              "RATE_LIMIT_BASE_DELAY_MS constant: 10000",
              "BASE_RETRY_DELAY_MS constant: 2000"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationService.createImageRequest",
          "related_concepts": [
            "OpenAI API authentication",
            "HTTP POST requests",
            "API rate limiting",
            "exponential backoff",
            "request validation",
            "error handling patterns",
            "timeout management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.2",
          "description": "Support gpt-image-1.5, gpt-image-1, and gpt-image-1-mini models with DALL-E 3/2 deprecation warnings (May 12, 2026)",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Default model is gpt-image-1.5 for production quality output when no model specified",
            "gpt-image-1-mini is selectable when cost optimization is explicitly requested (approximately 80% cheaper)",
            "gpt-image-1 is available as standard tier option between mini and 1.5",
            "Model parameter accepts only valid values: 'gpt-image-1.5', 'gpt-image-1', 'gpt-image-1-mini'",
            "Invalid model values are rejected with 400 status and descriptive error message listing valid options",
            "DALL-E 3 model requests return 400 with deprecation warning: 'dall-e-3 is deprecated as of May 12, 2026. Please use gpt-image-1.5 instead.'",
            "DALL-E 2 model requests return 400 with deprecation warning: 'dall-e-2 is deprecated as of May 12, 2026. Please use gpt-image-1 or gpt-image-1-mini instead.'",
            "Cost estimation is calculated and returned based on model and quality combination before execution",
            "Model capabilities are validated against requested parameters (all models support all sizes and background options)",
            "Model usage is logged for cost tracking with model name, quality, and timestamp"
          ],
          "implementation": {
            "frontend": [
              "ImageModelSelector component with dropdown showing three options: 'Best Quality (gpt-image-1.5)', 'Standard (gpt-image-1)', 'Cost-Optimized (gpt-image-1-mini)'",
              "CostEstimateDisplay component showing estimated price per image based on selected model and quality ($0.01-$0.19 range)",
              "DeprecationWarningBanner component displaying migration guidance when user attempts DALL-E model selection",
              "ModelInfoTooltip component explaining differences between models (quality, speed, cost tradeoffs)",
              "QualityModelMatrix component showing cost grid for all model/quality combinations",
              "Model selector integration with ImageGenerationForm passing selected model to API"
            ],
            "backend": [
              "validateImageModel function checking against SUPPORTED_IMAGE_MODELS array and returning validation result",
              "calculateImageCost(model: ImageModel, quality: ImageQuality): number function using pricing matrix",
              "checkDeprecatedModel function returning deprecation message for dall-e-3 and dall-e-2",
              "Model validation in /api/tools/generate-image route before OpenAI API call",
              "Model fallback logic: if gpt-image-1.5 fails with model_not_found, retry with gpt-image-1",
              "Model usage logger recording model, quality, timestamp, estimated cost for analytics"
            ],
            "middleware": [
              "Model validation middleware returning 400 for deprecated models with migration guidance",
              "Model usage logging middleware capturing model selection for cost tracking",
              "Feature flag support for gradual model migration or A/B testing"
            ],
            "shared": [
              "ImageModel type: 'gpt-image-1.5' | 'gpt-image-1' | 'gpt-image-1-mini'",
              "SUPPORTED_IMAGE_MODELS constant: ['gpt-image-1.5', 'gpt-image-1', 'gpt-image-1-mini']",
              "DEPRECATED_MODELS constant: { 'dall-e-3': { deprecationDate: '2026-05-12', migrateTo: 'gpt-image-1.5' }, 'dall-e-2': { deprecationDate: '2026-05-12', migrateTo: 'gpt-image-1' } }",
              "IMAGE_MODEL_PRICING object: { 'gpt-image-1.5': { low: 0.01, medium: 0.04, high: 0.17 }, 'gpt-image-1': { low: 0.02, medium: 0.07, high: 0.19 }, 'gpt-image-1-mini': { low: 0.002, medium: 0.01, high: 0.04 } }",
              "ModelCapabilities interface: { supportsTransparency: boolean, maxImages: number, maxPromptLength: number }",
              "MODEL_INFO constant array with name, description, useCases, costTier for UI display"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationService.selectModel",
          "related_concepts": [
            "model selection strategy",
            "cost optimization",
            "quality tiers",
            "DALL-E 3 deprecation",
            "DALL-E 2 deprecation",
            "fallback patterns",
            "model capability validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.3",
          "description": "Handle base64 response format (GPT Image models always return base64, never URLs)",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Response JSON is parsed and b64_json field is extracted from response.data[0].b64_json",
            "Base64 string is validated using regex pattern /^[A-Za-z0-9+/]+=*$/ before processing",
            "Invalid base64 responses trigger ImageGenerationError with INVALID_RESPONSE code and response preview for debugging",
            "Multiple images (n > 1) are handled by mapping through data array and extracting each b64_json",
            "Revised prompt is extracted from response.data[0].revised_prompt if present and returned to caller",
            "Empty, null, or undefined b64_json field triggers error with message 'No image data in response'",
            "Response parsing handles malformed JSON gracefully with try-catch and descriptive error",
            "Large base64 strings (up to 10MB for high quality images) are processed without memory issues using streaming if needed",
            "Response metadata (model used, created timestamp) is extracted and logged for monitoring",
            "Parsing duration is measured and logged for performance monitoring"
          ],
          "implementation": {
            "frontend": [
              "ImageParsingProgress component showing parsing status for multiple images with progress bar",
              "ParseErrorDisplay component showing error message with retry option and technical details expandable",
              "RevisedPromptDisplay component showing OpenAI's interpretation of user prompt in collapsible section",
              "MultiImageIndicator badge showing '1 of N' when multiple images returned",
              "ImagePreview component rendering base64 directly for immediate feedback before blob upload"
            ],
            "backend": [
              "parseImageResponse(response: OpenAIImageResponse): ParsedImageResult[] function extracting all image data",
              "validateBase64(data: string): boolean function using VALID_BASE64_REGEX pattern",
              "extractMetadata(response: OpenAIImageResponse): ResponseMetadata function for logging data",
              "Response schema validator checking data array exists and has expected structure",
              "Multi-image iterator using Array.map to process data array for n > 1",
              "Performance timing wrapper measuring parse duration for monitoring"
            ],
            "middleware": [
              "Response size monitoring logging warning if base64 exceeds 5MB threshold",
              "Response logging middleware recording size, image count, has_revised_prompt without actual content",
              "Memory usage check before processing large responses"
            ],
            "shared": [
              "OpenAIImageResponse interface: { created: number, data: ImageData[] }",
              "ImageData interface: { b64_json: string, revised_prompt?: string }",
              "ParsedImageResult interface: { base64: string, revisedPrompt?: string, index: number }",
              "ResponseMetadata interface: { created: number, imageCount: number, hasRevisedPrompt: boolean }",
              "VALID_BASE64_REGEX constant: /^[A-Za-z0-9+/]+=*$/",
              "MAX_BASE64_SIZE_BYTES constant: 10485760 (10MB)",
              "Base64ValidationError class extending ImageGenerationError with invalidChars context"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationService.parseBase64Response",
          "related_concepts": [
            "base64 encoding",
            "JSON parsing",
            "response validation",
            "data integrity",
            "image format detection",
            "memory management",
            "multi-image handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.4",
          "description": "Convert base64 responses to image buffers and persist to Vercel Blob storage",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Base64 string is decoded to Buffer using Buffer.from(base64, 'base64')",
            "Buffer is validated by checking first bytes match expected magic bytes: PNG (89 50 4E 47), JPEG (FF D8 FF), WebP (52 49 46 46)",
            "Image buffer is uploaded to Vercel Blob storage using @vercel/blob put() function with public access",
            "Generated blob URL is returned as permanent URL for image display and future access",
            "Filename is generated with pattern: 'generated-image-{timestamp}-{index}.{format}' (e.g., 'generated-image-1705432800000-0.png')",
            "Content-Type header is set correctly based on output_format: 'image/png', 'image/jpeg', or 'image/webp'",
            "Upload errors are caught and return ImageStorageError with UPLOAD_FAILED code and retry suggestion",
            "Multiple images are stored individually with indexed filenames using Promise.all for parallel uploads",
            "Image metadata (prompt hash, model, quality, generation timestamp, size in bytes) is stored in Vercel Blob metadata",
            "BLOB_READ_WRITE_TOKEN environment variable is validated before upload attempt",
            "Upload size is validated against Vercel Blob limits before attempting storage"
          ],
          "implementation": {
            "frontend": [
              "GeneratedImage component rendering blob URL in <img> tag with loading placeholder",
              "ImageDownloadButton component triggering browser download with correct filename and format",
              "CopyImageUrlButton component copying blob URL to clipboard with success toast",
              "ImageGallery component displaying grid of multiple generated images with selection state",
              "UploadingOverlay component showing upload progress spinner over image preview",
              "ImageMetadataPanel showing generation details: model, quality, size, timestamp",
              "StorageQuotaIndicator component (if applicable) showing remaining blob storage"
            ],
            "backend": [
              "convertBase64ToBuffer(base64: string): Buffer function using Buffer.from(base64, 'base64')",
              "validateImageSignature(buffer: Buffer, expectedFormat: ImageFormat): boolean checking magic bytes",
              "uploadImageToBlob(buffer: Buffer, filename: string, contentType: string, metadata: ImageMetadata): Promise<BlobResult> using @vercel/blob put()",
              "generateImageFilename(timestamp: number, index: number, format: ImageFormat): string creating unique filename",
              "batchUploadImages(images: ParsedImageResult[], format: ImageFormat, metadata: BaseMetadata): Promise<StoredImage[]> using Promise.all",
              "buildImageMetadata(prompt: string, model: ImageModel, quality: ImageQuality): ImageMetadata creating metadata object with prompt hash (not full prompt)",
              "Validate BLOB_READ_WRITE_TOKEN environment variable before upload in route handler"
            ],
            "middleware": [
              "Upload size validation middleware checking buffer size before blob storage attempt",
              "Content-Type setter middleware based on ImageFormat enum",
              "Storage quota checker middleware (if Vercel provides quota info)",
              "Upload retry middleware with 2 retries for transient storage errors"
            ],
            "shared": [
              "IMAGE_MAGIC_BYTES constant: { png: [0x89, 0x50, 0x4E, 0x47], jpeg: [0xFF, 0xD8, 0xFF], webp: [0x52, 0x49, 0x46, 0x46] }",
              "ContentTypeMap constant: { png: 'image/png', jpeg: 'image/jpeg', webp: 'image/webp' }",
              "StoredImage interface: { url: string, filename: string, contentType: string, metadata: ImageMetadata, size: number }",
              "ImageMetadata interface: { promptHash: string, model: ImageModel, quality: ImageQuality, createdAt: number, sizeBytes: number }",
              "BLOB_STORAGE_PREFIX constant: 'generated-images/' for organizing storage",
              "ImageStorageError class extending Error with code: 'UPLOAD_FAILED' | 'INVALID_SIGNATURE' | 'CONFIG_ERROR' and retryable boolean",
              "BlobUploadOptions interface matching @vercel/blob put() options: { access: 'public', token: string, contentType: string }"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageStorageService.convertAndStoreImage",
          "related_concepts": [
            "Buffer operations",
            "Vercel Blob storage",
            "image format conversion",
            "file persistence",
            "URL generation",
            "content-type headers",
            "storage cleanup"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_002.5",
          "description": "Support all gpt-image-1.5 parameters: size (1024x1024, 1536x1024, 1024x1536, auto), quality (low/medium/high), output_format (png/jpeg/webp), background (auto/transparent/opaque), n (1-10 images)",
          "type": "sub_process",
          "parent_id": "REQ_002",
          "children": [],
          "acceptance_criteria": [
            "Size parameter accepts values: '1024x1024' (square), '1536x1024' (landscape), '1024x1536' (portrait), 'auto' with default '1024x1024'",
            "Quality parameter accepts values: 'low', 'medium', 'high' with default 'high'",
            "Output format parameter accepts values: 'png', 'jpeg', 'webp' with default 'png'",
            "Background parameter accepts values: 'auto', 'transparent', 'opaque' with default 'auto'",
            "N parameter accepts integers 1-10 with default 1, values outside range return validation error",
            "Transparent background with jpeg format triggers warning and auto-switches to png (jpeg doesn't support transparency)",
            "All parameters are validated by Zod schema before API call with descriptive error messages",
            "UI provides intuitive controls for each parameter with previews and descriptions",
            "Parameter combinations are validated for compatibility (e.g., transparent + png = valid)",
            "All parameter values are correctly passed to OpenAI API request body"
          ],
          "implementation": {
            "frontend": [
              "ImageSizeSelector component with radio buttons showing aspect ratio icons (square, landscape, portrait) and 'Auto' option",
              "ImageQualitySelector component with slider or radio showing Low ($0.01), Medium ($0.04), High ($0.17) with cost labels",
              "ImageFormatSelector component with radio buttons showing PNG (best for transparency), JPEG (smaller files), WebP (modern browsers)",
              "BackgroundSelector component with three visual options: Auto (AI decides), Transparent (checkerboard preview), Opaque (solid preview)",
              "ImageCountSelector component with number input or stepper 1-10 with cost multiplier display",
              "ImageGenerationForm component composing all selectors with live cost estimate and parameter summary",
              "ParameterCompatibilityWarning component showing toast when incompatible options selected (e.g., transparent + jpeg)",
              "AspectRatioPreview component showing visual representation of selected size dimensions"
            ],
            "backend": [
              "ImageGenerationParamsSchema Zod schema validating all parameters with proper types and constraints",
              "validateParameterCompatibility(params: ImageGenerationRequest): ValidationResult checking format/background compatibility",
              "buildApiRequestBody(params: ImageGenerationRequest): OpenAIRequestBody constructing properly formatted request",
              "normalizeParams(params: Partial<ImageGenerationRequest>): ImageGenerationRequest applying defaults",
              "calculateTotalCost(model: ImageModel, quality: ImageQuality, n: number): number multiplying per-image cost by count"
            ],
            "middleware": [
              "Parameter validation middleware running Zod schema validation on request body",
              "Auto-correction middleware fixing incompatible combinations (transparent + jpeg \u2192 transparent + png) with warning header",
              "Request logging middleware capturing all parameter values for analytics"
            ],
            "shared": [
              "ImageSize type: '1024x1024' | '1536x1024' | '1024x1536' | 'auto'",
              "ImageQuality type: 'low' | 'medium' | 'high'",
              "ImageFormat type: 'png' | 'jpeg' | 'webp'",
              "ImageBackground type: 'auto' | 'transparent' | 'opaque'",
              "IMAGE_SIZE_OPTIONS constant: [{ value: '1024x1024', label: 'Square (1024\u00d71024)', aspectRatio: '1:1' }, { value: '1536x1024', label: 'Landscape (1536\u00d71024)', aspectRatio: '3:2' }, { value: '1024x1536', label: 'Portrait (1024\u00d71536)', aspectRatio: '2:3' }, { value: 'auto', label: 'Auto (AI decides)', aspectRatio: 'auto' }]",
              "QUALITY_OPTIONS constant: [{ value: 'low', label: 'Low', description: 'Fast, draft quality' }, { value: 'medium', label: 'Medium', description: 'Balanced quality' }, { value: 'high', label: 'High', description: 'Best quality' }]",
              "FORMAT_OPTIONS constant: [{ value: 'png', label: 'PNG', mimeType: 'image/png', supportsTransparency: true }, { value: 'jpeg', label: 'JPEG', mimeType: 'image/jpeg', supportsTransparency: false }, { value: 'webp', label: 'WebP', mimeType: 'image/webp', supportsTransparency: true }]",
              "BACKGROUND_OPTIONS constant: [{ value: 'auto', label: 'Auto', description: 'AI decides based on prompt' }, { value: 'transparent', label: 'Transparent', description: 'Clear background (PNG/WebP only)' }, { value: 'opaque', label: 'Opaque', description: 'Solid background' }]",
              "MAX_IMAGE_COUNT constant: 10",
              "MIN_IMAGE_COUNT constant: 1",
              "DEFAULT_IMAGE_PARAMS constant: { model: 'gpt-image-1.5', size: '1024x1024', quality: 'high', output_format: 'png', background: 'auto', n: 1 }"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageGenerationTypes.ParameterSupport",
          "related_concepts": [
            "image dimensions",
            "aspect ratio",
            "image quality tiers",
            "file formats",
            "transparency support",
            "batch generation",
            "parameter validation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_003",
      "description": "The system must implement document generation using AI-generated structured content combined with document creation libraries",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_003.1",
          "description": "Generate structured JSON content using OpenAI Structured Outputs with json_schema response format to produce document-ready content that can be transformed into PDF, DOCX, or XLSX files",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "POST request to OpenAI chat.completions.create with response_format: { type: 'json_schema', json_schema: {...} }",
            "System prompt instructs AI to generate document content matching the specified schema structure",
            "User prompt contains the document description, purpose, and any context needed for content generation",
            "Response content is parsed as valid JSON matching the defined schema",
            "Handles OpenAI API errors (rate limits, invalid requests, server errors) with appropriate retry logic",
            "Validates returned JSON against schema before proceeding to document generation",
            "Supports different document types by selecting appropriate schema (report, spreadsheet, letter, etc.)",
            "Logs generation time, token usage, and content length for cost tracking",
            "Rejects malformed responses with clear error message for user retry",
            "Timeout after 60 seconds for long content generation requests"
          ],
          "implementation": {
            "frontend": [
              "DocumentGenerationForm component with textarea for content description input",
              "DocumentTypeSelector dropdown with options: PDF, Word Document, Excel Spreadsheet",
              "GenerationProgressIndicator showing AI content generation status",
              "ContentPreview panel displaying generated JSON structure before document creation",
              "RegenerateButton to request new content with same or modified prompt",
              "TemplateSelector to choose from predefined document templates (invoice, report, letter)"
            ],
            "backend": [
              "POST /api/tools/generate-document/content endpoint to generate structured content",
              "DocumentContentGenerator service class with generateStructuredContent(prompt, documentType, schema) method",
              "buildStructuredOutputsRequest(prompt, schema) function to construct OpenAI API request",
              "Schema registry mapping document types to their JSON schemas",
              "Token usage tracking and logging for cost management",
              "Response validation using zod or similar schema validator"
            ],
            "middleware": [
              "Request validation for content description (min 10 chars, max 5000 chars)",
              "Rate limiting for document generation (10 requests per minute per user)",
              "API key validation for OpenAI requests",
              "Content moderation check on user prompt before generation"
            ],
            "shared": [
              "DocumentContentRequest interface: { prompt: string, documentType: 'pdf' | 'docx' | 'xlsx', templateId?: string }",
              "StructuredOutputsConfig interface matching OpenAI API parameters",
              "GeneratedContent interface: { content: object, schema: object, generatedAt: Date, tokenUsage: number }",
              "DOCUMENT_GENERATION_TIMEOUT constant (60000ms)",
              "MAX_CONTENT_PROMPT_LENGTH constant (5000)"
            ]
          },
          "testable_properties": [],
          "function_id": "DocumentContentGenerator.generateStructuredContent",
          "related_concepts": [
            "OpenAI Structured Outputs",
            "json_schema response_format",
            "gpt-4o model",
            "content transformation",
            "document templating",
            "schema validation"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.2",
          "description": "Define and manage JSON schemas for document content structures with title, sections, tables, and metadata to ensure AI-generated content is consistently structured for document creation",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Base document schema includes: title (string), author (optional string), createdAt (ISO date string), sections (array)",
            "Section schema includes: heading (string), content (string), subsections (optional nested sections array)",
            "Table schema includes: headers (string array), rows (2D string array), caption (optional string)",
            "List schema includes: items (string array), ordered (boolean), nested (optional recursive list)",
            "Spreadsheet schema includes: sheetName (string), columns (column definition array), rows (data array)",
            "Column definition schema includes: header (string), type ('string' | 'number' | 'date' | 'currency'), width (optional number)",
            "All schemas pass JSON Schema Draft 2020-12 validation",
            "Schemas are versioned to support backward compatibility",
            "Schema registry provides getSchema(documentType, version?) method",
            "Invalid schema requests return clear error with available schema types",
            "Schemas include examples for AI prompt engineering"
          ],
          "implementation": {
            "frontend": [
              "SchemaVisualizer component showing expected document structure as tree/outline",
              "FieldRequirementIndicator showing required vs optional fields",
              "SchemaVersionSelector for choosing schema version (if multiple exist)",
              "SampleOutputPreview showing example JSON matching the schema"
            ],
            "backend": [
              "DocumentSchemaRegistry singleton class managing all document schemas",
              "registerSchema(type, version, schema) method to add new schemas",
              "getSchema(type, version?) method returning schema for specified type",
              "validateContent(content, schemaType) method to validate generated content",
              "Schema definition files: reportSchema.ts, spreadsheetSchema.ts, letterSchema.ts, invoiceSchema.ts",
              "Schema migration utilities for version upgrades"
            ],
            "middleware": [
              "Schema validation middleware for incoming document content",
              "Schema version header parsing (X-Schema-Version)"
            ],
            "shared": [
              "DocumentSchema type: JSON Schema object with strict typing",
              "BaseDocumentContent interface: { title: string, author?: string, createdAt: string }",
              "DocumentSection interface: { heading: string, content: string, subsections?: DocumentSection[] }",
              "TableContent interface: { headers: string[], rows: string[][], caption?: string }",
              "SpreadsheetColumn interface: { header: string, type: DataType, width?: number }",
              "SpreadsheetContent interface: { sheetName: string, columns: SpreadsheetColumn[], rows: (string | number | Date)[][] }",
              "SCHEMA_VERSION constant (current version)",
              "SUPPORTED_DOCUMENT_TYPES constant array"
            ]
          },
          "testable_properties": [],
          "function_id": "DocumentSchemaRegistry.defineDocumentSchemas",
          "related_concepts": [
            "JSON Schema specification",
            "document structure modeling",
            "schema versioning",
            "content validation",
            "nested object schemas",
            "array schemas"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.3",
          "description": "Integrate PDFKit library to transform AI-generated structured JSON content into professionally formatted PDF documents with headers, sections, tables, and styling",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Install and configure pdfkit npm package as project dependency",
            "Create PDF document with A4 page size (default) or Letter size option",
            "Render document title as centered header with configurable font size (default 24pt)",
            "Render sections with heading (18pt bold) and content (12pt regular) with proper spacing",
            "Support nested subsections with indentation (each level indented 20pt)",
            "Render tables with alternating row colors, borders, and header row styling",
            "Support bullet lists (unordered) and numbered lists (ordered)",
            "Add page numbers at bottom center of each page",
            "Implement automatic page breaks when content exceeds page height",
            "Add document metadata (title, author, creation date) to PDF properties",
            "Output PDF as Buffer for upload to Vercel Blob or direct download",
            "Handle font loading errors gracefully with fallback to Helvetica",
            "Support custom header/footer content per document template",
            "Generate PDF in under 5 seconds for documents up to 50 pages"
          ],
          "implementation": {
            "frontend": [
              "PdfPreviewPanel component showing thumbnail of generated PDF",
              "PdfDownloadButton with filename input field",
              "PdfOptionsPanel for page size, orientation, margins configuration",
              "PdfStyleSelector for choosing color scheme and font family",
              "PdfGenerationProgress showing page-by-page creation progress"
            ],
            "backend": [
              "npm install pdfkit @types/pdfkit in project dependencies",
              "PdfGenerator service class with createPDF(content: DocumentContent, options: PdfOptions): Promise<Buffer>",
              "renderTitle(doc, title) method for title rendering",
              "renderSection(doc, section, depth) recursive method for section/subsection rendering",
              "renderTable(doc, table) method for table rendering with autofit columns",
              "renderList(doc, list) method for ordered/unordered list rendering",
              "addPageNumbers(doc) method to add page numbers to all pages",
              "POST /api/tools/generate-document/pdf endpoint accepting structured content and returning PDF buffer",
              "Font manager for loading and caching custom fonts"
            ],
            "middleware": [
              "Content size validation (reject documents expected to exceed 100 pages)",
              "Memory limit monitoring during PDF generation",
              "Request timeout enforcement (30 second limit for PDF generation)"
            ],
            "shared": [
              "PdfOptions interface: { pageSize: 'A4' | 'Letter', orientation: 'portrait' | 'landscape', margins: Margins }",
              "Margins interface: { top: number, bottom: number, left: number, right: number }",
              "PdfStyles interface: { titleFont: string, titleSize: number, bodyFont: string, bodySize: number, accentColor: string }",
              "DEFAULT_PDF_OPTIONS constant with sensible defaults",
              "SUPPORTED_PAGE_SIZES constant array",
              "MAX_PDF_PAGES constant (100)"
            ]
          },
          "testable_properties": [],
          "function_id": "PdfGenerator.createPDF",
          "related_concepts": [
            "PDFKit library",
            "PDF document structure",
            "font embedding",
            "page layout",
            "table rendering",
            "vector graphics"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.4",
          "description": "Integrate docx library to transform AI-generated structured JSON content into professionally formatted Microsoft Word documents with styles, sections, tables, and proper document structure",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Install and configure docx npm package as project dependency",
            "Create new Document with proper document properties (title, author, description)",
            "Define paragraph styles for Title, Heading1, Heading2, Normal text",
            "Render document title using Title style at document start",
            "Render sections with Heading1 style for headings and Normal style for content",
            "Support nested subsections using Heading2, Heading3 styles with proper hierarchy",
            "Render tables with header row formatting (bold, shaded background)",
            "Support bullet lists and numbered lists with proper indentation",
            "Add page numbers in document footer",
            "Implement section breaks for multi-section documents",
            "Output DOCX as Buffer using Packer.toBuffer() for storage/download",
            "Support custom document templates with predefined styles",
            "Add table of contents generation for documents with sections",
            "Handle special characters and Unicode text properly",
            "Generate DOCX in under 3 seconds for documents up to 100 pages"
          ],
          "implementation": {
            "frontend": [
              "DocxPreviewPanel component with document structure outline view",
              "DocxDownloadButton with filename input and .docx extension",
              "DocxStyleSelector for choosing document template/style set",
              "DocxOptionsPanel for header/footer, margins, page numbering options",
              "DocxTableOfContentsToggle checkbox to include/exclude TOC"
            ],
            "backend": [
              "npm install docx @types/docx in project dependencies",
              "DocxGenerator service class with createDOCX(content: DocumentContent, options: DocxOptions): Promise<Buffer>",
              "createDocumentStyles() method returning style definitions for title, headings, body",
              "renderTitle(doc, title) method creating title paragraph",
              "renderSection(doc, section, level) recursive method for sections with appropriate heading level",
              "renderTable(doc, table) method creating Table with TableRow and TableCell elements",
              "renderList(doc, list, ordered) method for bulleted or numbered lists",
              "createTableOfContents(doc, sections) method for TOC generation",
              "POST /api/tools/generate-document/docx endpoint accepting structured content and returning DOCX buffer"
            ],
            "middleware": [
              "Content validation for DOCX-specific requirements",
              "File size estimation before generation",
              "Memory monitoring during document packing"
            ],
            "shared": [
              "DocxOptions interface: { template: string, includeTableOfContents: boolean, pageNumbers: boolean, margins: Margins }",
              "DocxStyles interface: { titleStyle: IParagraphStyleOptions, headingStyles: IParagraphStyleOptions[], bodyStyle: IParagraphStyleOptions }",
              "HeadingLevel enum: { Title, Heading1, Heading2, Heading3, Normal }",
              "DEFAULT_DOCX_OPTIONS constant",
              "DOCX_STYLE_TEMPLATES map of predefined style sets",
              "MAX_DOCX_SECTIONS constant (50)"
            ]
          },
          "testable_properties": [],
          "function_id": "DocxGenerator.createDOCX",
          "related_concepts": [
            "docx npm library",
            "Office Open XML format",
            "document styles",
            "paragraph formatting",
            "table styling",
            "Word document structure"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_003.5",
          "description": "Integrate ExcelJS library to transform AI-generated structured JSON content into Excel spreadsheets with worksheets, formatted data, formulas, and professional styling",
          "type": "sub_process",
          "parent_id": "REQ_003",
          "children": [],
          "acceptance_criteria": [
            "Install and configure exceljs npm package as project dependency",
            "Create new Workbook with document properties (title, author, created date)",
            "Add worksheet with specified name from content schema",
            "Set column headers in first row with bold formatting and background color",
            "Auto-fit column widths based on content length (min 10, max 50 characters)",
            "Apply data type formatting: numbers with thousand separators, dates in locale format, currency with symbol",
            "Support multiple worksheets in single workbook for complex documents",
            "Add cell borders for data range (thin border all cells)",
            "Freeze first row (header row) for scrolling",
            "Support formula cells with proper validation (e.g., =SUM(A2:A10))",
            "Add data validation dropdowns for specified columns",
            "Apply alternating row colors for readability",
            "Output XLSX as Buffer using workbook.xlsx.writeBuffer()",
            "Handle large datasets efficiently (up to 10,000 rows without performance issues)",
            "Generate XLSX in under 5 seconds for spreadsheets up to 10,000 rows"
          ],
          "implementation": {
            "frontend": [
              "ExcelPreviewPanel showing spreadsheet grid preview with first 10 rows",
              "ExcelDownloadButton with filename input and .xlsx extension",
              "WorksheetTabSelector for multi-sheet documents",
              "ColumnTypeConfigPanel for specifying column data types",
              "ExcelStyleSelector for color scheme and formatting options",
              "ExcelFormulaHelper suggesting common formulas for numeric columns"
            ],
            "backend": [
              "npm install exceljs @types/exceljs in project dependencies",
              "ExcelGenerator service class with createXLSX(content: SpreadsheetContent[], options: ExcelOptions): Promise<Buffer>",
              "addWorksheet(workbook, sheetData) method creating and populating worksheet",
              "setColumnHeaders(worksheet, columns) method styling header row",
              "populateData(worksheet, rows, columns) method adding data with type formatting",
              "applyColumnFormatting(worksheet, columns) method for number/date/currency formats",
              "autoFitColumns(worksheet) method calculating optimal column widths",
              "addFormulas(worksheet, formulaDefinitions) method for calculated cells",
              "POST /api/tools/generate-document/xlsx endpoint accepting structured content and returning XLSX buffer"
            ],
            "middleware": [
              "Row count validation (max 10,000 rows per sheet)",
              "Column count validation (max 100 columns)",
              "Memory monitoring for large spreadsheet generation",
              "Formula validation to prevent circular references"
            ],
            "shared": [
              "ExcelOptions interface: { sheetNames?: string[], headerStyle: CellStyle, alternateRowColors: boolean, freezeHeader: boolean }",
              "CellStyle interface: { font: FontOptions, fill: FillOptions, border: BorderOptions }",
              "ColumnDataType enum: 'string' | 'number' | 'date' | 'currency' | 'percentage' | 'formula'",
              "FormulaDefinition interface: { cell: string, formula: string }",
              "DEFAULT_EXCEL_OPTIONS constant",
              "MAX_ROWS_PER_SHEET constant (10000)",
              "MAX_COLUMNS constant (100)",
              "CURRENCY_SYMBOLS map by locale"
            ]
          },
          "testable_properties": [],
          "function_id": "ExcelGenerator.createXLSX",
          "related_concepts": [
            "ExcelJS npm library",
            "XLSX format",
            "worksheet management",
            "cell formatting",
            "column auto-width",
            "data validation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_004",
      "description": "The system must maintain and enhance existing voice input implementation using MediaRecorder, Vercel Blob storage, and OpenAI Whisper API",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_004.1",
          "description": "Capture audio using browser MediaRecorder API with automatic MIME type detection (webm for Chrome/Firefox, mp4 for Safari)",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "MediaRecorder successfully initializes when user clicks Record button",
            "Microphone permission request is displayed and handled (allow/deny scenarios)",
            "MIME type detection correctly identifies 'audio/webm' for Chrome/Firefox browsers",
            "MIME type detection correctly identifies 'audio/mp4' for Safari browsers",
            "Falls back to 'audio/webm' when no supported MIME type is detected",
            "Audio data chunks are collected via ondataavailable event handler",
            "Recording timer displays elapsed time in MM:SS format, updating every second",
            "Recording automatically stops at MAX_RECORDING_TIME_MS (5 minutes / 300 seconds)",
            "Recording indicator (red pulsing dot) is visible during active recording",
            "Stop button properly triggers MediaRecorder.stop() and stream track cleanup",
            "Audio blob is created from collected chunks with correct MIME type on stop",
            "MediaStream tracks are stopped to release microphone when recording ends",
            "Error state is set and user-friendly message displayed when microphone access denied",
            "Component properly cleans up all resources (timers, streams, URLs) on unmount",
            "Reset functionality clears all state and releases resources for re-recording"
          ],
          "implementation": {
            "frontend": [
              "AudioRecorder React component with forwardRef for parent control access",
              "RecorderState type union: 'idle' | 'recording' | 'stopped' | 'error'",
              "getMimeType() function using MediaRecorder.isTypeSupported() for detection",
              "startRecording() async function requesting getUserMedia({ audio: true })",
              "stopRecording() callback with timer, MediaRecorder, and stream cleanup",
              "Recording indicator UI (data-testid='recording-indicator') with animate-pulse",
              "Timer display component showing formatTime(recordingTime) as MM:SS",
              "Record button (Mic icon) for idle state, Stop button (Square icon) for recording",
              "Playback controls (Play/Pause, Re-record) for stopped state",
              "Error message display area for permission denied or recording errors",
              "useImperativeHandle for exposing reset() method to parent component"
            ],
            "backend": [],
            "middleware": [],
            "shared": [
              "AudioRecorderProps interface with onRecordingComplete(audioBlob: Blob) callback",
              "AudioRecorderHandle interface exposing reset() method",
              "MAX_RECORDING_TIME_MS constant (5 * 60 * 1000 = 300000)",
              "MAX_RECORDING_TIME_SECONDS constant (300)"
            ]
          },
          "testable_properties": [],
          "function_id": "AudioRecorder.captureAudio",
          "related_concepts": [
            "MediaRecorder API",
            "getUserMedia API",
            "browser audio codecs",
            "MIME type detection",
            "cross-browser compatibility",
            "audio streaming",
            "blob handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.2",
          "description": "Upload recorded audio blob to Vercel Blob storage to bypass 4.5MB serverless function payload limit (up to 25MB supported)",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Client-side validation rejects files exceeding MAX_FILE_SIZE_BYTES (25MB)",
            "TranscriptionError with code 'FILE_TOO_LARGE' thrown for oversized files",
            "FormData is correctly constructed with file blob and timestamped filename",
            "File extension is correctly determined from MIME type (mp4 vs webm)",
            "Filename format follows pattern: recording-{timestamp}.{extension}",
            "POST request to /api/upload endpoint includes FormData body",
            "Server validates BLOB_READ_WRITE_TOKEN environment variable exists",
            "Server validates file size does not exceed 25MB limit",
            "Vercel Blob put() is called with 'public' access and token",
            "Successful upload returns { url: blob.url } response",
            "Upload errors return { error, code } with appropriate HTTP status",
            "TranscriptionError with code 'UPLOAD_ERROR' thrown on failure",
            "retryable flag is set to true for network errors, false for validation errors",
            "Blob URL is returned for subsequent transcription API call"
          ],
          "implementation": {
            "frontend": [
              "transcribeAudio() function in transcription.ts orchestrating upload + transcribe",
              "Client-side file size validation before upload attempt",
              "FormData construction with audioBlob and generated filename",
              "fetch() POST to /api/upload with FormData body",
              "Error parsing from upload response JSON",
              "TranscriptionError instantiation with appropriate error codes"
            ],
            "backend": [
              "POST /api/upload route handler in Next.js App Router",
              "formData.get('file') extraction as File type",
              "File size validation against MAX_FILE_SIZE_BYTES",
              "BLOB_READ_WRITE_TOKEN environment variable validation",
              "Vercel Blob put(filename, file, { access: 'public', token }) call",
              "Success response: NextResponse.json({ url: blob.url })",
              "Error responses with { error, code } and appropriate HTTP status codes"
            ],
            "middleware": [
              "Environment variable validation for BLOB_READ_WRITE_TOKEN"
            ],
            "shared": [
              "MAX_FILE_SIZE_MB constant (25)",
              "MAX_FILE_SIZE_BYTES constant (25 * 1024 * 1024 = 26214400)",
              "TranscriptionError class with message, code, and retryable properties",
              "TranscriptionErrorCode type: 'UPLOAD_ERROR' | 'FILE_TOO_LARGE' | 'CONFIG_ERROR'"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionClient.uploadToVercelBlob",
          "related_concepts": [
            "Vercel Blob storage",
            "serverless function limits",
            "FormData API",
            "file upload",
            "blob URL handling",
            "client-side validation",
            "error handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.3",
          "description": "Call /api/transcribe endpoint with Vercel Blob URL for Whisper transcription",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "POST request sent to /api/transcribe with Content-Type: application/json",
            "Request body includes blobUrl (required) and optional language parameter",
            "TranscriptionOptions.language is included in payload when provided",
            "Network errors throw TranscriptionError with code 'NETWORK' and retryable: true",
            "Response JSON is parsed and checked for success/failure",
            "Failed responses throw TranscriptionError with server-provided code and message",
            "Successful responses return data.text as the transcribed string",
            "retryable flag from server response is preserved in thrown errors",
            "API call properly handles missing or malformed response bodies"
          ],
          "implementation": {
            "frontend": [
              "fetch() POST to /api/transcribe with JSON Content-Type header",
              "JSON.stringify() for blobUrl and optional language parameter",
              "Try-catch wrapper for network error handling",
              "Response status check (response.ok) for success/failure determination",
              "Error response parsing: data.error, data.code, data.retryable",
              "TranscriptionError construction with server-provided details",
              "Return data.text on successful transcription"
            ],
            "backend": [],
            "middleware": [],
            "shared": [
              "TranscriptionOptions interface with optional language: string (ISO 639-1)",
              "TranscribeRequestPayload interface: { blobUrl: string, language?: string }",
              "TranscribeResponsePayload interface: { text: string } | { error: string, code: string, retryable: boolean }"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionClient.callTranscribeEndpoint",
          "related_concepts": [
            "REST API design",
            "JSON payload",
            "async request handling",
            "error propagation",
            "transcription options",
            "language parameter"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.4",
          "description": "Process audio through OpenAI Whisper API using whisper-1 model with exponential backoff retry logic",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "OPENAI_API_KEY environment variable is validated before processing",
            "Blob URL is fetched and converted to ArrayBuffer",
            "Content-Type header from blob response is captured for file type validation",
            "File size is validated against 25MB limit server-side",
            "File type is validated against SUPPORTED_AUDIO_TYPES map",
            "OpenAI client is instantiated with validated API key",
            "Audio buffer is converted to File using openai toFile() utility",
            "Filename includes proper extension based on content type mapping",
            "openai.audio.transcriptions.create() called with whisper-1 model",
            "Optional language parameter is passed to Whisper API when provided",
            "Rate limit errors (429) trigger retry with RATE_LIMIT_BASE_DELAY_MS (10s) base",
            "Network/server errors (500-504) trigger retry with BASE_RETRY_DELAY_MS (2s) base",
            "Exponential backoff doubles delay on each retry: delay = base * 2^retries",
            "Maximum of MAX_RETRIES (3) retry attempts before final failure",
            "401 errors throw TranscriptionError with 'INVALID_API_KEY' code, non-retryable",
            "Retry logging includes attempt number, delay, and error code",
            "Final transcription.text is extracted and returned on success"
          ],
          "implementation": {
            "frontend": [],
            "backend": [
              "POST /api/transcribe route handler parsing { blobUrl, language } from body",
              "OPENAI_API_KEY environment variable validation",
              "fetch(blobUrl) to retrieve audio from Vercel Blob storage",
              "Content-Type extraction from blob response headers",
              "File size validation: fileBuffer.byteLength > MAX_FILE_SIZE_BYTES",
              "SUPPORTED_AUDIO_TYPES map for content type to extension mapping",
              "OpenAI client instantiation: new OpenAI({ apiKey })",
              "toFile() conversion: toFile(new Uint8Array(fileBuffer), filename, { type })",
              "transcribeWithRetry(openai, file, language, retryCount) recursive function",
              "makeOpenAIRequest(openai, file, language) for actual API call",
              "openai.audio.transcriptions.create({ file, model: 'whisper-1', language? })",
              "TranscriptionError class with code and retryable properties",
              "Error mapping: 401\u2192INVALID_API_KEY, 429\u2192RATE_LIMIT, 5xx\u2192API_ERROR"
            ],
            "middleware": [
              "API key presence validation",
              "File type validation against supported audio types",
              "Rate limit detection and appropriate delay calculation"
            ],
            "shared": [
              "SUPPORTED_AUDIO_TYPES constant map: { 'audio/webm': 'webm', 'audio/mp4': 'm4a', ... }",
              "MAX_RETRIES constant (3)",
              "BASE_RETRY_DELAY_MS constant (2000)",
              "RATE_LIMIT_BASE_DELAY_MS constant (10000)",
              "TranscriptionErrorCode union: 'RATE_LIMIT' | 'INVALID_API_KEY' | 'API_ERROR' | 'NETWORK'"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionAPI.processWithWhisper",
          "related_concepts": [
            "OpenAI Audio API",
            "Whisper model",
            "exponential backoff",
            "retry logic",
            "rate limiting",
            "error classification",
            "file conversion"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_004.5",
          "description": "Delete temporary blob from Vercel Blob storage after successful transcription and return text with isVoiceTranscription flag",
          "type": "sub_process",
          "parent_id": "REQ_004",
          "children": [],
          "acceptance_criteria": [
            "Blob deletion is attempted after successful Whisper transcription",
            "BLOB_READ_WRITE_TOKEN is validated before deletion attempt",
            "Vercel Blob del(blobUrl, { token }) is called to remove temporary file",
            "Blob deletion failure is logged but does not fail the transcription request",
            "Transcription text is returned in response: { text: string }",
            "Blob deletion is also attempted on transcription failure (cleanup on error)",
            "Blob deletion errors are logged with console.warn for monitoring",
            "Frontend receives text and creates message with isVoiceTranscription: true flag",
            "Message is added to conversation store via addMessage() action",
            "isVoiceTranscription flag is preserved in Message interface",
            "Auto-send behavior triggers when isVoiceTranscription is true (if configured)",
            "UI can distinguish voice-originated messages from typed messages"
          ],
          "implementation": {
            "frontend": [
              "onRecordingComplete callback handling transcription result",
              "Message creation with isVoiceTranscription: true flag",
              "addMessage() store action call with voice message data",
              "Auto-send logic triggering API call for voice transcriptions",
              "UI indicator or styling for voice-originated messages (optional)"
            ],
            "backend": [
              "try-finally block ensuring blob cleanup on success",
              "BLOB_READ_WRITE_TOKEN validation before del() call",
              "del(blobUrl, { token: blobToken }) from @vercel/blob",
              "console.warn() logging for cleanup failures",
              "Separate try-catch in error handler for cleanup on transcription failure",
              "Final NextResponse.json({ text }) return statement"
            ],
            "middleware": [],
            "shared": [
              "Message interface with isVoiceTranscription?: boolean field",
              "AddMessagePayload interface including voice flag",
              "BLOB_READ_WRITE_TOKEN environment variable requirement"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionAPI.cleanupAndReturn",
          "related_concepts": [
            "Vercel Blob deletion",
            "resource cleanup",
            "error resilience",
            "message flagging",
            "state management",
            "graceful degradation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_005",
      "description": "The system must enforce voice recording constraints with retry logic and exponential backoff",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_005.1",
          "description": "Enforce 5 minute maximum recording duration with visual countdown timer and automatic stop",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MAX_RECORDING_TIME_MS constant is set to 300000 (5 minutes)",
            "Visual countdown timer displays remaining time in MM:SS format during recording",
            "Timer color changes to warning (yellow/orange) at 1 minute remaining",
            "Timer color changes to critical (red) at 30 seconds remaining",
            "Audio and visual warning triggers at 30 seconds remaining",
            "Recording automatically stops when timer reaches 0",
            "stopRecording() is called via ref to avoid stale closure when auto-stopping",
            "Timer interval is cleared on both manual stop and auto-stop",
            "MediaStream tracks are stopped to release microphone access",
            "User sees toast notification when recording is auto-stopped due to time limit",
            "Recording state transitions to 'stopped' after auto-stop",
            "Audio blob is still created and passed to onRecordingComplete callback on auto-stop"
          ],
          "implementation": {
            "frontend": [
              "AudioRecorder.tsx: Add countdown display showing remaining time (MAX_RECORDING_TIME_SECONDS - recordingTime)",
              "AudioRecorder.tsx: Add CSS classes for warning states (.timer-warning at 60s, .timer-critical at 30s)",
              "AudioRecorder.tsx: Add visual pulse animation when entering critical time zone",
              "AudioRecorder.tsx: Use existing stopRecordingRef pattern to call stopRecording() from timer callback",
              "AudioRecorder.tsx: Add aria-live region for screen reader accessibility of countdown",
              "Add toast notification component or use existing notification system for auto-stop message"
            ],
            "backend": [],
            "middleware": [],
            "shared": [
              "constants.ts: Export MAX_RECORDING_TIME_MS = 300000 and MAX_RECORDING_TIME_SECONDS = 300 for reuse",
              "types.ts: Add RecorderWarningLevel type = 'normal' | 'warning' | 'critical'"
            ]
          },
          "testable_properties": [],
          "function_id": "AudioRecorder.enforceMaxDuration",
          "related_concepts": [
            "MediaRecorder",
            "timer management",
            "visual feedback",
            "auto-stop",
            "cleanup"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.2",
          "description": "Validate 25 MB maximum file size before upload with user-friendly error messages",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MAX_FILE_SIZE_MB constant is set to 25",
            "MAX_FILE_SIZE_BYTES is calculated as MAX_FILE_SIZE_MB * 1024 * 1024 (26214400)",
            "Client-side validation in transcription.ts checks audioBlob.size before upload attempt",
            "Server-side validation in transcribe/route.ts checks fileBuffer.byteLength after blob fetch",
            "TranscriptionError with code 'FILE_TOO_LARGE' is thrown when validation fails",
            "Error message includes the actual file size and the limit (e.g., 'File size 28.5MB exceeds 25MB limit')",
            "retryable flag is set to false for file size errors (not transient)",
            "UI displays human-readable error message with suggestion to record shorter audio",
            "Error is displayed before any network request is made (fast feedback)",
            "Validation occurs in both client (transcription.ts line 22) and server (route.ts line 63)"
          ],
          "implementation": {
            "frontend": [
              "transcription.ts: Validate audioBlob.size <= MAX_FILE_SIZE_BYTES before upload",
              "transcription.ts: Include actual file size in error message for user clarity",
              "MessageInput.tsx or AudioRecorder.tsx: Display file size error with actionable suggestion",
              "Add formatFileSize utility function to convert bytes to human-readable format (KB, MB)"
            ],
            "backend": [
              "transcribe/route.ts: Validate fileBuffer.byteLength after fetching from Vercel Blob",
              "transcribe/route.ts: Return 400 status with FILE_TOO_LARGE code and retryable: false"
            ],
            "middleware": [],
            "shared": [
              "constants.ts: Export MAX_FILE_SIZE_MB = 25 and MAX_FILE_SIZE_BYTES = 26214400",
              "types.ts: Ensure TranscriptionErrorCode includes 'FILE_TOO_LARGE'",
              "utils.ts: Add formatFileSize(bytes: number): string function"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.validateFileSize",
          "related_concepts": [
            "Blob size",
            "client-side validation",
            "server-side validation",
            "error UX",
            "TranscriptionError"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.3",
          "description": "Implement 3 retry attempts for transcription failures with attempt tracking and logging",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "MAX_RETRIES constant is set to 3",
            "transcribeWithRetry function accepts retries parameter starting at 0",
            "Only errors with retryable: true trigger retry attempts",
            "Each retry attempt is logged with format 'Retry {n}/{MAX_RETRIES} after {delay}ms ({error_code})'",
            "Retry counter increments on each recursive call (retries + 1)",
            "Function throws after MAX_RETRIES attempts are exhausted",
            "Final error includes context that all retries were exhausted",
            "Non-retryable errors (INVALID_API_KEY, FILE_TOO_LARGE) throw immediately without retry",
            "Successful transcription on retry returns text without additional error",
            "Retry attempts are tracked in structured log format for debugging/monitoring"
          ],
          "implementation": {
            "frontend": [
              "Optional: Display retry status in UI ('Retrying transcription... attempt 2 of 3')"
            ],
            "backend": [
              "transcribe/route.ts: transcribeWithRetry(openai, file, language, retries) recursive function",
              "transcribe/route.ts: Check error.retryable && retries < MAX_RETRIES before retry",
              "transcribe/route.ts: Log each retry with console.warn including attempt number and error code",
              "transcribe/route.ts: Pass retries + 1 on recursive call",
              "transcribe/route.ts: Throw error when retries >= MAX_RETRIES"
            ],
            "middleware": [],
            "shared": [
              "constants.ts: Export MAX_RETRIES = 3",
              "types.ts: TranscriptionError.retryable boolean property determines retry eligibility"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.retryWithAttemptTracking",
          "related_concepts": [
            "retry logic",
            "attempt counter",
            "structured logging",
            "error recovery",
            "recursion"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.4",
          "description": "Apply 10 second base delay with exponential backoff for rate limit errors (HTTP 429) up to 60 second max",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "RATE_LIMIT_BASE_DELAY_MS constant is set to 10000 (10 seconds)",
            "Maximum delay is capped at 60000ms (60 seconds)",
            "Delay calculation uses exponential backoff: baseDelay * 2^retries",
            "Retry 1: 10s delay, Retry 2: 20s delay, Retry 3: 40s delay (all under 60s cap)",
            "Delay is capped at MAX_RATE_LIMIT_DELAY_MS if calculation exceeds it",
            "Rate limit errors are identified by HTTP status 429 or error.code === 'RATE_LIMIT'",
            "TranscriptionError with code 'RATE_LIMIT' has retryable: true",
            "Delay is applied using await new Promise(resolve => setTimeout(resolve, delay))",
            "Log message includes calculated delay: 'Rate limit hit, waiting {delay}ms before retry'",
            "Error message to user indicates rate limit and suggests waiting"
          ],
          "implementation": {
            "frontend": [
              "Display rate limit status in UI with countdown ('Rate limited, retrying in 10s...')",
              "Show progress indicator during rate limit wait period"
            ],
            "backend": [
              "transcribe/route.ts: RATE_LIMIT_BASE_DELAY_MS = 10000 constant",
              "transcribe/route.ts: MAX_RATE_LIMIT_DELAY_MS = 60000 constant",
              "transcribe/route.ts: Calculate delay = Math.min(RATE_LIMIT_BASE_DELAY_MS * Math.pow(2, retries), MAX_RATE_LIMIT_DELAY_MS)",
              "transcribe/route.ts: Check error.code === 'RATE_LIMIT' to use rate limit delay",
              "transcribe/route.ts: makeOpenAIRequest catches 429 status and throws TranscriptionError('RATE_LIMIT', true)",
              "transcribe/route.ts: Log rate limit occurrence with delay calculation"
            ],
            "middleware": [],
            "shared": [
              "constants.ts: Export RATE_LIMIT_BASE_DELAY_MS = 10000",
              "constants.ts: Export MAX_RATE_LIMIT_DELAY_MS = 60000"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.rateLimitBackoff",
          "related_concepts": [
            "HTTP 429",
            "rate limiting",
            "exponential backoff",
            "OpenAI API limits",
            "async delay"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_005.5",
          "description": "Apply 2 second base delay with exponential backoff for network errors up to 30 second max with jitter",
          "type": "sub_process",
          "parent_id": "REQ_005",
          "children": [],
          "acceptance_criteria": [
            "BASE_RETRY_DELAY_MS constant is set to 2000 (2 seconds)",
            "Maximum network retry delay is capped at 30000ms (30 seconds)",
            "Delay calculation uses exponential backoff: baseDelay * 2^retries",
            "Retry 1: ~2s, Retry 2: ~4s, Retry 3: ~8s (with jitter)",
            "Jitter is applied using formula: delay * (0.5 + Math.random()) for \u00b150% variance",
            "Network errors are identified by catch block in fetch or status 502/503/504",
            "TranscriptionError with code 'NETWORK' has retryable: true",
            "Connection timeout errors are treated as network errors",
            "ECONNREFUSED, ETIMEDOUT, ENOTFOUND errors are caught and wrapped",
            "Log message includes jittered delay: 'Network error, retrying in {delay}ms (with jitter)'",
            "Final error after retries includes suggestion to check network connection"
          ],
          "implementation": {
            "frontend": [
              "transcription.ts: Catch network errors in fetch call and throw TranscriptionError('NETWORK', true)",
              "Display network error status with retry countdown",
              "Show 'Check your internet connection' suggestion after final retry failure"
            ],
            "backend": [
              "transcribe/route.ts: BASE_RETRY_DELAY_MS = 2000 constant",
              "transcribe/route.ts: MAX_NETWORK_DELAY_MS = 30000 constant",
              "transcribe/route.ts: Calculate base delay = Math.min(BASE_RETRY_DELAY_MS * Math.pow(2, retries), MAX_NETWORK_DELAY_MS)",
              "transcribe/route.ts: Apply jitter = delay * (0.5 + Math.random())",
              "transcribe/route.ts: Check error.code === 'NETWORK' to use network delay with jitter",
              "transcribe/route.ts: makeOpenAIRequest catches 502/503/504 and throws TranscriptionError('NETWORK', true)",
              "transcribe/route.ts: Catch fetch errors and identify network-related errors"
            ],
            "middleware": [],
            "shared": [
              "constants.ts: Export BASE_RETRY_DELAY_MS = 2000",
              "constants.ts: Export MAX_NETWORK_DELAY_MS = 30000",
              "utils.ts: Add applyJitter(delay: number, factor: number = 0.5): number function"
            ]
          },
          "testable_properties": [],
          "function_id": "TranscriptionService.networkErrorBackoff",
          "related_concepts": [
            "network resilience",
            "connection errors",
            "jitter",
            "exponential backoff",
            "timeout handling"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_006",
      "description": "The system must implement an Intent Classification layer to route plain language requests to appropriate tools",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_006.1",
          "description": "Create classifyIntent function using gpt-4o-mini with json_object response format for tool routing",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "Function accepts a userMessage string parameter and optional conversationHistory array for context-aware classification",
            "Function makes a POST request to OpenAI API using gpt-4o-mini model with temperature 0 for deterministic output",
            "Request includes response_format: { type: 'json_object' } configuration to enforce JSON response",
            "Function returns a Promise<ClassifiedIntent> with parsed and validated JSON response",
            "Function handles empty or whitespace-only messages by returning error with code 'INVALID_INPUT'",
            "Function validates the returned JSON matches ClassifiedIntent structure before returning",
            "Function logs classification attempts including userMessage hash, detected tool, confidence, and latency for analytics",
            "Function completes classification within 2 seconds for typical messages (95th percentile)",
            "Function reuses existing retry logic pattern from generate/route.ts (MAX_RETRIES=3, exponential backoff)",
            "Function handles rate limit errors (429) with appropriate backoff and retry",
            "Function handles network timeouts with configurable timeout (default 10 seconds)",
            "Unit tests verify JSON parsing of all four tool type responses with varying confidence levels",
            "Unit tests verify error handling for malformed JSON, API errors, and timeout scenarios",
            "Function includes conversation history in prompt when provided for context-aware multi-turn classification"
          ],
          "implementation": {
            "frontend": [
              "Create ClassificationLoadingIndicator component showing subtle 'Analyzing request...' spinner",
              "Integrate loading indicator with message submission flow in MessageInput component",
              "Add visual feedback when classification completes showing detected intent icon briefly",
              "Handle classification errors gracefully with user-friendly error toast notifications"
            ],
            "backend": [
              "Create POST /api/tools/classify/route.ts API endpoint following existing route patterns",
              "Import OpenAI SDK and initialize client with OPENAI_API_KEY from environment",
              "Implement request body validation using zod schema: { message: string, conversationHistory?: Message[] }",
              "Call openai.chat.completions.create with model 'gpt-4o-mini', temperature 0, response_format: { type: 'json_object' }",
              "Parse JSON response using JSON.parse with try-catch for malformed response handling",
              "Validate parsed response against ClassifiedIntent interface using type guard function",
              "Return 400 for invalid request body, 422 for unparseable response, 500 for API errors",
              "Apply existing retry logic pattern (MAX_RETRIES=3, INITIAL_DELAY_MS=1000, exponential backoff factor 2)",
              "Log classification metrics to analytics service: latency, token usage, detected tool, confidence"
            ],
            "middleware": [
              "Rate limit classify endpoint to 60 requests/minute per user to prevent abuse",
              "Add request timeout middleware with 10 second limit",
              "No authentication middleware required - uses same session as chat endpoints"
            ],
            "shared": [
              "Create frontend/src/lib/intentClassifier.ts as main module for intent classification",
              "Export async function classifyIntent(userMessage: string, conversationHistory?: Message[]): Promise<ClassifiedIntent>",
              "Create ClassificationError custom error class extending Error with code, retryable, and originalError fields",
              "Define CLASSIFY_TIMEOUT_MS constant (10000)",
              "Define CLASSIFY_MAX_RETRIES constant (3)",
              "Create buildClassificationMessages(userMessage: string, history?: Message[]) helper function",
              "Export classifyIntent for use by message processing pipeline in useChat hook or similar"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.classifyIntent",
          "related_concepts": [
            "OpenAI Chat Completions API",
            "JSON Schema Validation",
            "Structured Outputs",
            "gpt-4o-mini model",
            "Promise-based async patterns",
            "Error handling and retries"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.2",
          "description": "Define four core intent types: deep_research, image_generation, document_generation, chat_completion",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "ToolIntent type is defined as union literal: 'deep_research' | 'image_generation' | 'document_generation' | 'chat_completion'",
            "deep_research intent includes DeepResearchParams interface with query (required), depth ('quick' | 'thorough', optional), topics (string[], optional)",
            "image_generation intent includes ImageGenerationParams interface with prompt (required), size ('1024x1024' | '1536x1024' | '1024x1536' | 'auto', optional), quality ('low' | 'medium' | 'high', optional), style (string, optional)",
            "document_generation intent includes DocumentGenerationParams interface with type ('pdf' | 'docx' | 'xlsx', required), contentDescription (required), template (string, optional), title (string, optional)",
            "chat_completion intent includes ChatCompletionParams interface with message (required) - serves as default fallback",
            "Each intent type params interface is exported individually for type-safe parameter handling in tool handlers",
            "ExtractedParams discriminated union type allows narrowing based on tool type using 'kind' discriminant field",
            "Type guard functions exist: isDeepResearchParams(), isImageGenerationParams(), isDocumentGenerationParams(), isChatCompletionParams()",
            "isValidToolIntent(value: string): value is ToolIntent type guard validates unknown strings",
            "TOOL_INTENT_DISPLAY_NAMES constant object provides human-readable labels: { deep_research: 'Deep Research', image_generation: 'Image Generation', document_generation: 'Document Generation', chat_completion: 'Chat' }",
            "TOOL_INTENT_ICONS constant object maps intent to icon names for UI display",
            "Unit tests verify each intent type can be correctly serialized/deserialized through JSON.parse/stringify roundtrip",
            "Documentation JSDoc comments explain when each intent type should be used with example phrases"
          ],
          "implementation": {
            "frontend": [
              "Create IntentTypeBadge component that displays intent icon and label based on ToolIntent value",
              "Create ToolIntentIcon component mapping intent types to appropriate icons (Search, Image, FileText, MessageSquare)",
              "Add toast notification when non-chat intent is detected: 'Using [Tool Name] for your request'",
              "Display intent badge in message bubble header when tool is invoked"
            ],
            "backend": [
              "Create validateToolIntent(value: unknown): ToolIntent function that throws on invalid input or returns valid intent",
              "Implement fallback logic: if classifier returns unknown tool type, log warning and return 'chat_completion'",
              "Add analytics event logging for each intent type selection to track usage patterns",
              "Create getDefaultParamsForIntent(intent: ToolIntent) factory function returning empty params with required defaults"
            ],
            "middleware": [
              "No additional middleware required - type validation happens at API boundary in route handlers"
            ],
            "shared": [
              "Define ToolIntent union type in frontend/src/lib/intentClassifier.ts",
              "Define DeepResearchParams interface: { kind: 'deep_research'; query: string; depth?: 'quick' | 'thorough'; topics?: string[] }",
              "Define ImageGenerationParams interface: { kind: 'image_generation'; prompt: string; size?: ImageSize; quality?: ImageQuality; style?: string }",
              "Define DocumentGenerationParams interface: { kind: 'document_generation'; type: DocumentType; contentDescription: string; template?: string; title?: string }",
              "Define ChatCompletionParams interface: { kind: 'chat_completion'; message: string }",
              "Define ExtractedParams = DeepResearchParams | ImageGenerationParams | DocumentGenerationParams | ChatCompletionParams",
              "Define ImageSize = '1024x1024' | '1536x1024' | '1024x1536' | 'auto'",
              "Define ImageQuality = 'low' | 'medium' | 'high'",
              "Define DocumentType = 'pdf' | 'docx' | 'xlsx'",
              "Export TOOL_INTENT_DISPLAY_NAMES: Record<ToolIntent, string>",
              "Export TOOL_INTENT_ICONS: Record<ToolIntent, string>",
              "Export all type guard functions for runtime validation",
              "Add JSDoc documentation to each type explaining usage scenarios"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.ToolIntentTypes",
          "related_concepts": [
            "TypeScript Union Types",
            "Discriminated Unions",
            "Intent Detection",
            "Tool Routing",
            "Exhaustive Pattern Matching",
            "Type Guards"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.3",
          "description": "Implement ClassifiedIntent response structure with tool name, confidence score (0.0-1.0), and extracted parameters",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "ClassifiedIntent interface contains 'tool' field of type ToolIntent (required)",
            "ClassifiedIntent interface contains 'confidence' field as number between 0.0 and 1.0 (required)",
            "ClassifiedIntent interface contains 'extractedParams' field of type ExtractedParams (required)",
            "ClassifiedIntent interface contains optional 'alternativeIntents' field as AlternativeIntent[] for ambiguous cases",
            "ClassifiedIntent interface contains optional 'rawMessage' field preserving original user input",
            "ClassifiedIntent interface contains optional 'classifiedAt' timestamp field for debugging",
            "Confidence score semantics: >0.8 = high certainty (proceed), 0.5-0.8 = medium (proceed with note), <0.5 = low (request clarification)",
            "CONFIDENCE_THRESHOLDS constant defines: { HIGH: 0.8, MEDIUM: 0.5, LOW: 0.3, MINIMUM: 0.1 }",
            "extractedParams structure varies by tool type but always includes 'kind' discriminant field",
            "AlternativeIntent interface: { tool: ToolIntent; confidence: number } for secondary classifications",
            "alternativeIntents included when second-best confidence > 0.4 AND within 0.3 of primary confidence",
            "validateClassifiedIntent(response: unknown): ClassifiedIntent throws ClassificationError on invalid structure",
            "clampConfidence(n: number): number clamps to 0.0-1.0 range with Math.max(0, Math.min(1, n))",
            "shouldRequestClarification(intent: ClassifiedIntent): boolean returns true if confidence < CONFIDENCE_THRESHOLDS.MEDIUM",
            "getConfidenceLevel(confidence: number): 'high' | 'medium' | 'low' returns semantic confidence level",
            "Unit tests verify confidence normalization handles edge cases: negative numbers, >1 values, NaN, undefined",
            "Unit tests verify param extraction for sample messages of each tool type"
          ],
          "implementation": {
            "frontend": [
              "Create ConfidenceIndicator component showing colored bar/badge based on confidence level (green=high, yellow=medium, red=low)",
              "Create ClarificationPrompt component shown when confidence < 0.5 asking user to confirm intent",
              "ClarificationPrompt displays: 'I think you want to [detected intent]. Is that correct?' with Yes/No/Other buttons",
              "Create AlternativeIntentsSuggestion component showing alternative tools user can select when ambiguous",
              "Display confidence percentage on hover/tap of intent badge for power users"
            ],
            "backend": [
              "Implement parseClassificationResponse(rawJson: string): ClassifiedIntent with validation and normalization",
              "Parse confidence from classifier response string and apply clampConfidence normalization",
              "Extract and validate tool-specific parameters from extractedParams, filling defaults for missing optional fields",
              "Include alternativeIntents in response when classifier provides them and they meet threshold criteria",
              "Log confidence distribution histogram to analytics for monitoring classifier accuracy over time",
              "Store classification results in database for training data collection (anonymized)"
            ],
            "middleware": [
              "No additional middleware required - validation happens in shared parsing function"
            ],
            "shared": [
              "Define ClassifiedIntent interface in frontend/src/lib/intentClassifier.ts",
              "Define AlternativeIntent interface: { tool: ToolIntent; confidence: number }",
              "Define CONFIDENCE_THRESHOLDS constant: { HIGH: 0.8, MEDIUM: 0.5, LOW: 0.3, MINIMUM: 0.1 }",
              "Export validateClassifiedIntent(response: unknown): ClassifiedIntent function with detailed error messages",
              "Export clampConfidence(n: number): number utility function",
              "Export shouldRequestClarification(intent: ClassifiedIntent): boolean function",
              "Export getConfidenceLevel(confidence: number): 'high' | 'medium' | 'low' helper function",
              "Export isClassifiedIntent(value: unknown): value is ClassifiedIntent type guard",
              "Create ClassificationValidationError extending ClassificationError with field-specific error details"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.ClassifiedIntentResponse",
          "related_concepts": [
            "Confidence Scoring",
            "Parameter Extraction",
            "NLP Classification",
            "Structured Response",
            "JSON Schema",
            "Validation Patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.4",
          "description": "Create and maintain system prompt with classification criteria and few-shot examples for each tool type",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "INTENT_CLASSIFIER_SYSTEM_PROMPT constant contains complete system prompt for gpt-4o-mini classifier",
            "System prompt clearly defines all four tool types with detailed descriptions and use cases",
            "deep_research criteria keywords: 'research', 'investigate', 'find out', 'analyze', 'what is the latest', 'study', 'explore', 'look into', 'deep dive'",
            "image_generation criteria keywords: 'create image', 'draw', 'generate picture', 'visualize', 'design', 'illustrate', 'make artwork', 'paint', 'render'",
            "document_generation criteria keywords: 'create PDF', 'generate report', 'make spreadsheet', 'Word document', 'Excel file', 'export as', 'create document', 'build a report'",
            "chat_completion criteria: general conversation, writing assistance, explanations, creative writing, questions - explicit DEFAULT when no other tool matches",
            "System prompt includes 2-3 few-shot examples per tool type demonstrating classification with parameters",
            "System prompt specifies exact JSON output format with field descriptions and types",
            "System prompt includes parameter extraction instructions for each tool type with examples",
            "System prompt includes confidence scoring guidance: 'assign 0.9+ for clear matches, 0.6-0.8 for likely matches, 0.3-0.5 for uncertain'",
            "System prompt instructs model to include alternativeIntents when confidence < 0.8",
            "System prompt includes negative examples showing what NOT to classify as each tool type",
            "Prompt stored as template literal constant with version comment header (e.g., // v1.0.0 - 2026-01-16)",
            "INTENT_CLASSIFIER_PROMPT_VERSION constant tracks prompt version for A/B testing and rollback",
            "Unit tests verify classifier produces expected tool for each trigger phrase category",
            "Integration tests verify classifier correctly handles ambiguous multi-intent requests"
          ],
          "implementation": {
            "frontend": [
              "No direct frontend implementation - prompt is backend-only configuration"
            ],
            "backend": [
              "Create INTENT_CLASSIFIER_SYSTEM_PROMPT constant string in frontend/src/lib/intentClassifier.ts",
              "Structure prompt with sections: ## Role, ## Available Tools, ## Classification Criteria, ## Output Format, ## Examples, ## Confidence Guidelines",
              "Include complete JSON schema in prompt for response format validation guidance",
              "Provide 3 few-shot examples per tool showing input message \u2192 JSON output mapping",
              "Include parameter extraction examples showing how to parse size, quality, document type from messages",
              "Add confidence scoring rubric explaining when to assign high/medium/low confidence",
              "Include edge case handling instructions for ambiguous or multi-tool requests",
              "Add version comment header and changelog in code comments for tracking prompt evolution",
              "Create buildClassificationPrompt(conversationHistory?: Message[]) function that incorporates history context"
            ],
            "middleware": [
              "No additional middleware required"
            ],
            "shared": [
              "Export INTENT_CLASSIFIER_SYSTEM_PROMPT constant string",
              "Export INTENT_CLASSIFIER_PROMPT_VERSION constant (e.g., '1.0.0')",
              "Create separate constants for trigger phrase arrays: DEEP_RESEARCH_TRIGGERS, IMAGE_GENERATION_TRIGGERS, DOCUMENT_GENERATION_TRIGGERS, CHAT_COMPLETION_TRIGGERS",
              "Define CLASSIFICATION_JSON_SCHEMA constant documenting expected response structure for reference",
              "Create buildClassificationMessages(userMessage: string, history?: Message[]): ChatCompletionMessage[] helper",
              "Create getPromptForVersion(version: string): string function to support A/B testing different prompts",
              "Document prompt engineering decisions and iteration history in code comments or separate markdown file"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.SystemPrompt",
          "related_concepts": [
            "Prompt Engineering",
            "Few-Shot Learning",
            "System Instructions",
            "Classification Criteria",
            "Tool Descriptions",
            "Prompt Versioning"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_006.5",
          "description": "Handle low confidence classifications (<0.5) with user clarification prompts",
          "type": "sub_process",
          "parent_id": "REQ_006",
          "children": [],
          "acceptance_criteria": [
            "When confidence < 0.5, system pauses automatic tool execution and prompts user for clarification",
            "Clarification prompt displays detected intent with confidence level and asks user to confirm or select alternative",
            "User presented with options: (1) Confirm detected intent, (2) Select from alternatives, (3) Type clarification, (4) Cancel/use chat",
            "If user confirms detected intent, proceed with tool execution regardless of low confidence (user override)",
            "If user selects alternative intent, re-classify with selected tool as hint and extract appropriate parameters",
            "If user types clarification, append to original message and re-run classification with enhanced context",
            "If user cancels, fall back to chat_completion with original message",
            "Clarification dialog shows friendly language: 'I want to make sure I understand...' not technical jargon",
            "Clarification state persists if user navigates away and returns within session",
            "Analytics track: clarification frequency, user choices, whether clarification improved accuracy",
            "CLARIFICATION_TIMEOUT_MS constant (300000 = 5 minutes) after which pending clarification is auto-cancelled",
            "Unit tests verify clarification flow triggers at exactly confidence = 0.49 but not at 0.50",
            "Unit tests verify each user choice path (confirm, alternative, clarify, cancel) produces correct outcome",
            "Accessibility: clarification dialog is keyboard navigable and screen reader friendly"
          ],
          "implementation": {
            "frontend": [
              "Create ClarificationDialog modal component with: detected intent display, confidence indicator, action buttons",
              "Create IntentConfirmButton component: 'Yes, [detected intent]' with tool icon",
              "Create AlternativeIntentSelector component: radio buttons or cards for alternative tools with descriptions",
              "Create ClarificationInput text field: 'Help me understand better: [input]' with submit button",
              "Create CancelClarificationButton: 'Just chat normally' falling back to chat_completion",
              "Add useClarificationState hook managing: isPending, pendingIntent, userChoice, clarificationText states",
              "Integrate ClarificationDialog into message submission flow in Chat component",
              "Show subtle loading state during re-classification after clarification input",
              "Display toast confirmation after clarification resolved: 'Got it! Using [selected tool]'",
              "Ensure dialog is mobile-responsive with appropriate touch targets"
            ],
            "backend": [
              "Create POST /api/tools/classify/confirm endpoint to handle user confirmation of low-confidence intent",
              "Create POST /api/tools/classify/clarify endpoint accepting clarificationText and re-running classification",
              "Implement reclassifyWithHint(originalMessage: string, hintedTool: ToolIntent): ClassifiedIntent function",
              "Implement reclassifyWithClarification(originalMessage: string, clarification: string): ClassifiedIntent function",
              "Store pending clarification state in session/database for persistence across page reloads",
              "Log clarification events: { originalIntent, originalConfidence, userAction, finalIntent, finalConfidence }",
              "Implement clarification timeout cleanup job removing stale pending clarifications"
            ],
            "middleware": [
              "No additional middleware required - uses existing session handling"
            ],
            "shared": [
              "Define ClarificationState interface: { isPending: boolean; pendingIntent: ClassifiedIntent | null; timestamp: number }",
              "Define ClarificationAction type: 'confirm' | 'select_alternative' | 'provide_clarification' | 'cancel'",
              "Define ClarificationResult interface: { action: ClarificationAction; finalIntent: ClassifiedIntent; userInput?: string }",
              "Export shouldRequestClarification(intent: ClassifiedIntent): boolean function (confidence < 0.5)",
              "Export CLARIFICATION_TIMEOUT_MS constant (300000)",
              "Export CLARIFICATION_CONFIDENCE_THRESHOLD constant (0.5)",
              "Create buildClarificationPrompt(originalMessage: string, detectedIntent: ClassifiedIntent): string helper",
              "Create formatIntentForDisplay(intent: ToolIntent): string helper for user-friendly tool names"
            ]
          },
          "testable_properties": [],
          "function_id": "IntentClassifier.LowConfidenceHandler",
          "related_concepts": [
            "User Clarification",
            "Confidence Thresholds",
            "Interactive Classification",
            "Fallback Handling",
            "User Experience",
            "Disambiguation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_007",
      "description": "The system must implement a Tool Registry with tool definitions including name, description, trigger phrases, handler functions, and response types",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_007.1",
          "description": "Create central toolRegistry Map data structure with ToolDefinition entries for all supported tools (deep_research, image_generation, document_generation, chat_completion)",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "ToolDefinition interface is defined with required fields: name (string), description (string), triggerPhrases (string[]), handler (function reference), responseType (enum)",
            "ResponseType type union includes 'text', 'image', and 'file' values",
            "toolRegistry is implemented as Map<string, ToolDefinition> for O(1) lookup performance",
            "Registry contains entries for: 'deep_research', 'image_generation', 'document_generation', 'chat_completion'",
            "Each tool entry includes unique identifier key matching its handler purpose",
            "Registry is exported as a singleton module for consistent access across application",
            "TypeScript strict mode compilation passes with no type errors",
            "Unit tests verify all four tool entries exist and have valid ToolDefinition shape",
            "Registry is immutable after initialization (use Object.freeze or readonly Map pattern)",
            "getToolByName(name: string) function returns ToolDefinition | undefined",
            "getAllTools() function returns array of all registered ToolDefinitions",
            "hasToolByName(name: string) function returns boolean for tool existence check"
          ],
          "implementation": {
            "frontend": [
              "Create ToolRegistryProvider context component for React access to registry",
              "Create useToolRegistry() hook for components to access registered tools",
              "Create ToolSelector component displaying available tools with icons and descriptions",
              "Display tool availability status in UI based on registry contents"
            ],
            "backend": [
              "Create /api/tools/registry/route.ts endpoint returning list of available tools with metadata",
              "Tool registry serves as source of truth for API route handler delegation",
              "Implement validateToolExists middleware checking tool name against registry"
            ],
            "middleware": [
              "Request validation ensuring tool parameter exists in registry before routing",
              "Tool availability check middleware for conditional tool access control"
            ],
            "shared": [
              "Create frontend/src/lib/tools/types.ts with ToolDefinition interface and ResponseType type",
              "Create frontend/src/lib/tools/toolRegistry.ts exporting toolRegistry Map<string, ToolDefinition>",
              "Define ToolParams interface for handler function parameter typing",
              "Define ToolResult interface with success/error union type for handler return values",
              "Add ToolError interface extending existing error patterns from types.ts (code, message, retryable)",
              "Create index.ts barrel export for tools module",
              "Define ToolHandler type as async function signature: (params: ToolParams) => Promise<ToolResult>",
              "Create TOOL_NAMES constant object with type-safe tool name strings"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.createToolRegistryMap",
          "related_concepts": [
            "Map data structure",
            "ToolDefinition interface",
            "type-safe registry pattern",
            "singleton module pattern",
            "handler function references",
            "tool configuration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.2",
          "description": "Define comprehensive trigger phrase arrays for natural language intent classification enabling voice-to-tool routing",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "deep_research tool has trigger phrases: ['research', 'investigate', 'find out about', 'analyze', 'look up', 'dig into', 'explore']",
            "image_generation tool has trigger phrases: ['create image', 'draw', 'generate picture', 'visualize', 'make art', 'design', 'illustrate', 'render']",
            "document_generation tool has trigger phrases: ['create document', 'generate report', 'make spreadsheet', 'create PDF', 'write a doc', 'make a file', 'export to']",
            "chat_completion tool has trigger phrases: ['write', 'help me', 'tell me', 'explain', 'suggest', 'answer', 'chat']",
            "Trigger phrases are case-insensitive during matching",
            "Trigger phrases support partial word matching (e.g., 'researching' matches 'research')",
            "Phrases are ordered by specificity (longer/more specific phrases first to avoid false positives)",
            "Unit tests verify each tool's trigger phrases correctly identify sample user inputs",
            "Edge cases handled: empty input, input with only stopwords, punctuation-heavy input",
            "isPhraseMatch helper returns confidence score (0-1) based on match quality"
          ],
          "implementation": {
            "frontend": [
              "Create TriggerPhraseDebugPanel component (dev mode only) showing matched phrases for debugging",
              "Display detected intent indicator when user types known trigger phrases",
              "Auto-suggest tool selection based on partial phrase match during typing"
            ],
            "backend": [
              "Create phrase matching service with stemming support for word root matching",
              "Implement priority scoring when multiple tools have overlapping trigger phrases",
              "Log trigger phrase matches for analytics and phrase effectiveness optimization"
            ],
            "middleware": [],
            "shared": [
              "Add triggerPhrases: readonly string[] field to ToolDefinition interface",
              "Populate deep_research entry with research-related phrases from common voice patterns",
              "Populate image_generation entry with creative/visual phrases",
              "Populate document_generation entry with file/document creation phrases",
              "Populate chat_completion entry with general writing/conversation phrases",
              "Create helper function isPhraseMatch(input: string, phrases: string[]): { matched: boolean, confidence: number, phrase: string | null }",
              "Create normalizeInput(input: string): string function for consistent matching (lowercase, trim, remove extra whitespace)",
              "Define PHRASE_MATCH_THRESHOLD constant (e.g., 0.7) for minimum confidence requirement"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.defineTriggerPhrases",
          "related_concepts": [
            "natural language processing",
            "intent detection",
            "phrase matching",
            "voice transcription patterns",
            "fuzzy matching",
            "keyword extraction"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.3",
          "description": "Specify responseType enum values (text/image/file) for UI rendering determination enabling type-safe response handling",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "ResponseType is defined as union type: 'text' | 'image' | 'file'",
            "deep_research tool has responseType: 'text'",
            "image_generation tool has responseType: 'image'",
            "document_generation tool has responseType: 'file'",
            "chat_completion tool has responseType: 'text'",
            "Type guard functions correctly narrow ToolResult to specific response types",
            "UI components use responseType to determine appropriate rendering strategy",
            "ResponseType is used at compile-time for exhaustive switch statement checking",
            "Unit tests verify each registered tool has valid responseType value",
            "Adding a new ResponseType value requires updating all switch statements (enforced by TypeScript)"
          ],
          "implementation": {
            "frontend": [
              "Create ToolResponseRenderer component with switch on responseType for polymorphic rendering",
              "Create TextResponseView component for 'text' responseType (markdown rendering, copy button)",
              "Create ImageResponseView component for 'image' responseType (image preview, download, zoom)",
              "Create FileResponseView component for 'file' responseType (file icon, filename, download button, preview if possible)",
              "Implement ResponseTypeIcon component mapping responseType to visual indicator"
            ],
            "backend": [
              "Validate API response matches expected responseType for the invoked tool",
              "Set appropriate Content-Type headers based on responseType",
              "Implement response serialization strategy per responseType (JSON for text, base64 for image, blob URL for file)"
            ],
            "middleware": [
              "Response type validation middleware ensuring handler returns expected type",
              "Content-Type header injection based on tool's responseType"
            ],
            "shared": [
              "Define ResponseType = 'text' | 'image' | 'file' in tools/types.ts",
              "Add responseType field to ToolDefinition interface",
              "Create type guard functions isTextResult, isImageResult, isFileResult in tools/typeGuards.ts",
              "Define ToolTextResult interface: { type: 'text', content: string, citations?: Citation[] }",
              "Define ToolImageResult interface: { type: 'image', imageUrl: string, revisedPrompt?: string, width: number, height: number }",
              "Define ToolFileResult interface: { type: 'file', fileUrl: string, filename: string, mimeType: string, size: number }",
              "Define ToolResult as discriminated union: ToolTextResult | ToolImageResult | ToolFileResult"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.specifyResponseTypes",
          "related_concepts": [
            "discriminated unions",
            "response type guards",
            "UI rendering strategies",
            "polymorphic response handling",
            "type narrowing",
            "content type detection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.4",
          "description": "Register handler function references (handleDeepResearch, handleImageGeneration, handleDocumentGeneration, handleChatCompletion) with proper async function signatures and error handling",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "handleDeepResearch handler is registered for 'deep_research' tool",
            "handleImageGeneration handler is registered for 'image_generation' tool",
            "handleDocumentGeneration handler is registered for 'document_generation' tool",
            "handleChatCompletion handler is registered for 'chat_completion' tool (existing /api/generate logic refactored)",
            "All handlers conform to ToolHandler type signature: (params: ToolParams) => Promise<ToolResult>",
            "Handlers catch and wrap errors in ToolError format with code, message, retryable fields",
            "Handlers support cancellation via AbortController signal in params",
            "Unit tests verify each handler is correctly registered and callable",
            "Integration tests verify handlers make correct API calls to external services",
            "Handlers emit events for progress tracking (onStart, onProgress, onComplete, onError)"
          ],
          "implementation": {
            "frontend": [
              "Create useToolExecution() hook wrapping handler invocation with loading/error states",
              "Implement tool execution progress indicator using handler events",
              "Create AbortButton component allowing user to cancel long-running tool operations",
              "Display tool-specific error messages with retry option for retryable errors"
            ],
            "backend": [
              "Create frontend/src/lib/tools/handlers/deepResearch.ts with handleDeepResearch function",
              "Create frontend/src/lib/tools/handlers/imageGeneration.ts with handleImageGeneration function",
              "Create frontend/src/lib/tools/handlers/documentGeneration.ts with handleDocumentGeneration function",
              "Create frontend/src/lib/tools/handlers/chatCompletion.ts with handleChatCompletion function (refactor from existing /api/generate)",
              "Each handler makes appropriate API calls to corresponding /api/tools/* endpoints",
              "Implement timeout handling with configurable duration per tool type",
              "Create handlers/index.ts barrel export for all handler functions"
            ],
            "middleware": [
              "Request logging middleware capturing tool invocation metrics",
              "Error transformation middleware wrapping API errors in ToolError format"
            ],
            "shared": [
              "Define ToolHandler type: (params: ToolParams & { signal?: AbortSignal }) => Promise<ToolResult>",
              "Define ToolParams base interface with userMessage, conversationHistory, options fields",
              "Define DeepResearchParams extending ToolParams with depth, tools configuration",
              "Define ImageGenerationParams extending ToolParams with size, quality, format, background",
              "Define DocumentGenerationParams extending ToolParams with documentType, template",
              "Define ChatCompletionParams extending ToolParams with model, temperature, maxTokens",
              "Create ToolExecutionContext interface with signal, onProgress, onComplete callbacks",
              "Define ToolProgressEvent interface for streaming progress updates"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.registerHandlerFunctions",
          "related_concepts": [
            "async function handlers",
            "error boundaries",
            "dependency injection",
            "handler registration",
            "function composition",
            "retry patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_007.5",
          "description": "Implement invokeToolHandler utility for executing tools through registry lookup with unified error handling and response normalization",
          "type": "sub_process",
          "parent_id": "REQ_007",
          "children": [],
          "acceptance_criteria": [
            "invokeToolHandler accepts toolName string and params object",
            "Function looks up tool in registry and throws ToolError if not found",
            "Function invokes registered handler with provided params",
            "Returns Promise<ToolResult> with normalized response format",
            "Errors from handlers are caught and re-thrown as ToolError with appropriate code",
            "Execution timing is measured and logged for performance monitoring",
            "Function supports optional execution context with signal, callbacks",
            "Unit tests verify successful invocation for all registered tools",
            "Unit tests verify error handling for unknown tool names",
            "Unit tests verify timeout handling for long-running tools",
            "Function validates params against tool's expected parameter schema before invocation"
          ],
          "implementation": {
            "frontend": [
              "Create ToolExecutor component orchestrating tool invocation from UI",
              "Integrate invokeToolHandler into message submission flow after intent classification",
              "Display tool execution status (pending, executing, completed, failed) in message thread",
              "Show estimated execution time based on tool type before invocation"
            ],
            "backend": [
              "Create frontend/src/lib/tools/invokeToolHandler.ts with main execution function",
              "Implement parameter validation against tool-specific schemas before handler call",
              "Create execution metrics collector logging tool name, duration, success/failure",
              "Implement concurrent execution limiter to prevent overwhelming external APIs",
              "Create /api/tools/execute/route.ts unified endpoint that uses invokeToolHandler internally"
            ],
            "middleware": [
              "Tool execution rate limiting per user/session",
              "Execution logging middleware for audit trail",
              "Cost estimation middleware returning estimated cost before execution"
            ],
            "shared": [
              "Create invokeToolHandler function signature: (toolName: string, params: ToolParams, context?: ToolExecutionContext) => Promise<ToolResult>",
              "Define InvocationOptions interface with timeout, retries, validateParams fields",
              "Create validateToolParams function for pre-invocation parameter validation",
              "Define ToolInvocationError extending ToolError with toolName field",
              "Create ToolExecutionMetrics interface: { toolName, startTime, endTime, duration, success, errorCode? }",
              "Define ToolNotFoundError for registry lookup failures",
              "Create TOOL_TIMEOUTS constant object with per-tool timeout configurations (deep_research: 600000ms, image_generation: 120000ms, etc.)"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolRegistry.invokeToolHandler",
          "related_concepts": [
            "strategy pattern",
            "command pattern",
            "registry lookup",
            "response normalization",
            "unified error handling",
            "execution context"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_008",
      "description": "The system must implement Deep Research Handler that executes research queries with configurable depth and returns text with citations",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_008.1",
          "description": "Implement depth parameter selection that routes to o4-mini-deep-research (~$3/query) for 'quick' depth and o3-deep-research (~$30/query) for 'thorough' depth, with cost estimation displayed before execution",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "TypeScript type defines depth options: 'quick' | 'thorough' with corresponding model mappings",
            "Default depth is 'quick' to avoid unexpected high costs unless explicitly requested",
            "Depth 'quick' maps to model 'o4-mini-deep-research-2025-06-26' (~$3 average per query)",
            "Depth 'thorough' maps to model 'o3-deep-research-2025-06-26' (~$30 average per query)",
            "Cost estimation is displayed before research execution with depth selection",
            "User must acknowledge cost estimate for 'thorough' depth (>$10 estimated)",
            "Depth selection persists in user preferences for future research tasks",
            "Model name is validated against allowed models before API call",
            "Invalid depth values return 400 status with descriptive error message",
            "Depth selection is logged for cost tracking and analytics purposes"
          ],
          "implementation": {
            "frontend": [
              "Create ResearchDepthSelector component with 'Quick (~$3)' and 'Thorough (~$30)' radio buttons",
              "Display cost estimate badge next to each depth option with tooltip explaining trade-offs",
              "Implement CostAcknowledgmentModal for thorough depth requiring user confirmation",
              "Add depth selection to DeepResearchForm component with clear labeling",
              "Show visual indicator (clock icon for quick, magnifying glass for thorough) next to options",
              "Store depth preference in localStorage for persistence across sessions"
            ],
            "backend": [
              "Create getModelForDepth(depth: ResearchDepth): string function returning appropriate model name",
              "Implement estimateResearchCost(depth: ResearchDepth): { min: number, avg: number, max: number } utility",
              "Add depth-to-model mapping constant: { quick: 'o4-mini-deep-research-2025-06-26', thorough: 'o3-deep-research-2025-06-26' }",
              "Validate depth parameter in API route before model selection",
              "Log depth selection with request ID for cost attribution"
            ],
            "middleware": [
              "Add Zod schema validation for depth parameter with enum constraint",
              "Implement cost acknowledgment check for thorough depth requests",
              "Add rate limiting per depth level (more restrictive for thorough)"
            ],
            "shared": [
              "Create ResearchDepth type: 'quick' | 'thorough'",
              "Create DEPTH_MODEL_MAP constant: Record<ResearchDepth, string>",
              "Create DEPTH_COST_ESTIMATES constant with min/avg/max costs per depth",
              "Add depth field to DeepResearchRequest interface",
              "Create CostEstimate interface: { depth: ResearchDepth, estimatedMin: number, estimatedAvg: number, estimatedMax: number, currency: 'USD' }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchConfig.selectDepthLevel",
          "related_concepts": [
            "model selection",
            "cost optimization",
            "user preference",
            "OpenAI Responses API",
            "research depth configuration"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.2",
          "description": "Create /api/tools/deep-research POST endpoint with Zod validation for request body, rate limiting to prevent API abuse, and structured error responses following existing patterns from generate and transcribe routes",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "POST /api/tools/deep-research endpoint accepts JSON body with query, depth, tools, and systemPrompt fields",
            "Request body is validated using Zod schema with required 'query' field (string, 1-10000 chars)",
            "Optional 'depth' field validates against 'quick' | 'thorough' enum, defaults to 'quick'",
            "Optional 'tools' array validates tool objects (web_search_preview, code_interpreter)",
            "Optional 'systemPrompt' field for developer-role instructions (max 5000 chars)",
            "Rate limiting enforces max 5 requests per minute per API key for thorough depth",
            "Rate limiting enforces max 20 requests per minute per API key for quick depth",
            "429 status returned when rate limit exceeded with Retry-After header",
            "API key validation returns 401 if OPENAI_API_KEY environment variable missing",
            "Request logging captures query (truncated to 200 chars), depth, tool count, timestamp",
            "Response follows existing pattern: { result: DeepResearchResult } on success",
            "Error responses follow pattern: { error: string, code: string, retryable: boolean }",
            "DeepResearchError class extends Error with code and retryable properties matching existing ChatGenerationError pattern"
          ],
          "implementation": {
            "frontend": [
              "Create useDeepResearch hook wrapping fetch call to /api/tools/deep-research",
              "Implement request state management (idle, loading, polling, complete, error)",
              "Add retry button for retryable errors with exponential backoff",
              "Display rate limit error with countdown timer showing when user can retry",
              "Create DeepResearchForm component collecting query, depth, and tool options"
            ],
            "backend": [
              "Create /api/tools/deep-research/route.ts with POST handler following existing route patterns",
              "Define Zod schema: z.object({ query: z.string().min(1).max(10000), depth: z.enum(['quick', 'thorough']).optional().default('quick'), tools: z.array(toolSchema).optional(), systemPrompt: z.string().max(5000).optional() })",
              "Implement in-memory rate limiter using Map<apiKeyHash, { count, windowStart }>",
              "Create DeepResearchError class extending Error with code, retryable properties",
              "Initialize OpenAI client with apiKey from process.env.OPENAI_API_KEY",
              "Call openai.responses.create() with validated parameters",
              "Parse and return DeepResearchResult on success"
            ],
            "middleware": [
              "Create rateLimiter middleware checking request count against depth-specific limits",
              "Implement API key validation ensuring OPENAI_API_KEY exists before processing",
              "Add request logging middleware capturing anonymized metrics",
              "Set appropriate CORS headers for API route"
            ],
            "shared": [
              "Create DeepResearchRequest interface with query, depth, tools, systemPrompt fields",
              "Create DeepResearchResult interface with text, citations, reasoningSteps, searchQueries fields",
              "Define tool schemas: WebSearchToolSchema, CodeInterpreterToolSchema",
              "Create DeepResearchErrorCode type: 'RATE_LIMIT' | 'INVALID_REQUEST' | 'API_ERROR' | 'TIMEOUT' | 'CONFIG_ERROR'",
              "Add RATE_LIMITS constant: { quick: { requests: 20, windowMs: 60000 }, thorough: { requests: 5, windowMs: 60000 } }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchAPI.createEndpoint",
          "related_concepts": [
            "Next.js API routes",
            "Zod validation",
            "rate limiting",
            "OpenAI Responses API",
            "error handling patterns"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.3",
          "description": "Configure web_search_preview tool (enabled by default) and optional code_interpreter tool with background mode enabled for long-running research tasks, following OpenAI Responses API tool specification",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "web_search_preview tool is included by default in all Deep Research requests",
            "web_search_preview tool object format: { type: 'web_search_preview' }",
            "Optional web_search_preview config supports domains, search_context_size, user_location",
            "code_interpreter tool is opt-in and requires explicit user selection",
            "code_interpreter tool object format: { type: 'code_interpreter' }",
            "code_interpreter cost warning ($0.03/session) displayed when enabled",
            "background: true is always set for Deep Research requests",
            "Background mode returns response ID for polling instead of blocking",
            "Tool array is validated to contain only supported tool types",
            "Empty tools array defaults to [{ type: 'web_search_preview' }]",
            "Duplicate tools are deduplicated before API call",
            "Tool configuration is serialized correctly in API request body"
          ],
          "implementation": {
            "frontend": [
              "Create ToolConfigurationPanel with checkboxes for available tools",
              "Web search toggle enabled by default with 'Recommended' badge",
              "Code interpreter toggle with cost warning tooltip ($0.03 per session)",
              "Advanced settings accordion for web_search_preview (domains, context size, location)",
              "Create WebSearchDomainInput component for whitelist/blacklist domains",
              "Create SearchContextSizeSelect with low/medium/high options",
              "Display tool summary chips showing enabled tools before request"
            ],
            "backend": [
              "Create buildToolsArray(config: ToolConfiguration): Tool[] function",
              "Implement web_search_preview tool builder with optional config merging",
              "Implement code_interpreter tool builder returning { type: 'code_interpreter' }",
              "Add deduplication logic removing duplicate tool types from array",
              "Validate tool types against SUPPORTED_TOOLS constant",
              "Always append background: true to request body",
              "Create buildDeepResearchRequest(params: DeepResearchParams): OpenAIResponsesRequest function"
            ],
            "middleware": [
              "Validate tool array contains only supported tool types",
              "Log which tools are enabled for each request for analytics",
              "Enforce code_interpreter cost acknowledgment if tool is enabled"
            ],
            "shared": [
              "Create Tool union type: WebSearchPreviewTool | CodeInterpreterTool",
              "Create WebSearchPreviewTool interface: { type: 'web_search_preview', domains?: string[], search_context_size?: 'low' | 'medium' | 'high', user_location?: { country: string, city?: string } }",
              "Create CodeInterpreterTool interface: { type: 'code_interpreter' }",
              "Create ToolConfiguration interface: { enableWebSearch: boolean, enableCodeInterpreter: boolean, webSearchConfig?: WebSearchConfig }",
              "Create SUPPORTED_TOOLS constant array: ['web_search_preview', 'code_interpreter']",
              "Create DEFAULT_TOOLS constant: [{ type: 'web_search_preview' }]"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchToolBuilder.configureTools",
          "related_concepts": [
            "OpenAI Responses API tools",
            "web_search_preview",
            "code_interpreter",
            "background mode",
            "async task handling"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.4",
          "description": "Implement polling mechanism with exponential backoff starting at 5 seconds and capping at 60 seconds between polls, with maximum 30 minute total duration before timeout, for background mode Deep Research tasks",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "Initial poll occurs 5 seconds after background task creation",
            "Poll interval doubles with each attempt: 5s \u2192 10s \u2192 20s \u2192 40s \u2192 60s (capped)",
            "Maximum poll interval is 60 seconds between attempts",
            "Total polling duration caps at 30 minutes (1800 seconds) before timeout",
            "Timeout error includes elapsed time and suggests thorough depth may need longer",
            "Poll request uses GET /v1/responses/{response_id} to check status",
            "Status 'completed' triggers result extraction and callback",
            "Status 'in_progress' continues polling with next backoff interval",
            "Status 'failed' triggers error handling with API error details",
            "Progress updates are emitted during polling (step count, elapsed time)",
            "Polling can be cancelled by user via AbortController",
            "Network errors during polling trigger retry with same interval (max 3 retries per poll)",
            "Final result is cached to prevent redundant API calls"
          ],
          "implementation": {
            "frontend": [
              "Create ResearchProgressIndicator component showing elapsed time and status",
              "Implement cancel button triggering AbortController.abort()",
              "Display estimated remaining time based on depth (quick: ~2-5 min, thorough: ~10-30 min)",
              "Show intermediate updates (reasoning steps, search queries) as they arrive",
              "Create PollingStatusBadge showing 'Researching...', 'Almost done...', etc.",
              "Implement progress bar with indeterminate mode (no exact percentage known)",
              "Toast notification when research completes if user navigated away"
            ],
            "backend": [
              "Create pollForCompletion(responseId: string, options: PollOptions): Promise<DeepResearchResult> function",
              "Implement exponential backoff calculator: Math.min(5000 * 2^attempt, 60000)",
              "Create checkResponseStatus(responseId: string): Promise<ResponseStatus> function",
              "Handle API response statuses: 'completed', 'in_progress', 'failed', 'cancelled'",
              "Implement timeout check: if (elapsedMs > 30 * 60 * 1000) throw TimeoutError",
              "Add retry logic for network errors during status check (max 3 retries)",
              "Cache completed results in memory with responseId key",
              "Emit progress events via callback or event emitter pattern"
            ],
            "middleware": [
              "Log poll attempts with responseId, attempt number, interval, elapsed time",
              "Track polling metrics (avg completion time, timeout rate) for monitoring",
              "Implement circuit breaker if repeated polling failures detected"
            ],
            "shared": [
              "Create PollOptions interface: { maxDurationMs: number, initialIntervalMs: number, maxIntervalMs: number, onProgress?: (progress: PollProgress) => void, signal?: AbortSignal }",
              "Create PollProgress interface: { elapsedMs: number, attemptCount: number, lastStatus: ResponseStatus, intermediateResults?: Partial<DeepResearchResult> }",
              "Create ResponseStatus type: 'completed' | 'in_progress' | 'failed' | 'cancelled'",
              "Create DEFAULT_POLL_OPTIONS constant: { maxDurationMs: 1800000, initialIntervalMs: 5000, maxIntervalMs: 60000 }",
              "Create TimeoutError class extending DeepResearchError with elapsed time details"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchPoller.pollForCompletion",
          "related_concepts": [
            "exponential backoff",
            "polling strategy",
            "async task completion",
            "timeout handling",
            "background job monitoring"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_008.5",
          "description": "Extract final report text from response.output[-1].content[0].text and citations from response.output[-1].content[0].annotations array, with null safety and structured parsing following OpenAI Responses API format",
          "type": "sub_process",
          "parent_id": "REQ_008",
          "children": [],
          "acceptance_criteria": [
            "Final report text extracted from response.output[-1].content[0].text path",
            "Returns empty string with warning log if output array is empty or undefined",
            "Returns empty string with warning log if content array is empty or undefined",
            "Returns empty string with warning log if text field is null, undefined, or empty",
            "Citations extracted from response.output[-1].content[0].annotations array",
            "Each citation includes: title (string), url (string), start_index (number), end_index (number)",
            "Invalid citation URLs are filtered out with warning logged",
            "Duplicate citations (same URL) are deduplicated preserving first occurrence",
            "Citations array returns empty array [] (not null) when no annotations present",
            "Markdown formatting in report text is preserved exactly as returned",
            "Parser handles both o3 and o4-mini response formats (same structure)",
            "Parser validates response shape before extraction with descriptive errors",
            "Unit tests cover: empty response, null content, malformed annotations, valid response"
          ],
          "implementation": {
            "frontend": [
              "Create ResearchReportDisplay component rendering extracted markdown text",
              "Create CitationList component showing citations as numbered references",
              "Implement inline citation markers [1], [2] linked to citation list",
              "Create CitationPopover showing title and URL on hover over marker",
              "Add 'Copy Report' button copying text without citation markers",
              "Add 'Copy with Citations' button copying text with bibliography",
              "Display word count and reading time estimate for report"
            ],
            "backend": [
              "Create extractReportText(response: DeepResearchResponse): string function with null checks",
              "Create extractCitations(response: DeepResearchResponse): Citation[] function",
              "Implement citation deduplication using URL as unique key with Set",
              "Add URL validation using URL constructor try/catch for malformed URLs",
              "Create validateResponseShape(response: unknown): response is DeepResearchResponse type guard",
              "Implement safe array access: response.output?.at(-1)?.content?.at(0)",
              "Log extraction warnings for missing/empty fields without throwing",
              "Create parseDeepResearchResponse(response: unknown): DeepResearchResult aggregator function"
            ],
            "middleware": [
              "Add response validation middleware checking expected shape",
              "Log extraction success rate and common failure patterns",
              "Cache parsed results to avoid re-parsing identical responses"
            ],
            "shared": [
              "Create DeepResearchResponse interface matching OpenAI Responses API structure",
              "Create OutputItem interface with type, content fields",
              "Create ContentBlock interface with type, text, annotations fields",
              "Create Citation interface: { title: string, url: string, startIndex: number, endIndex: number, domain: string }",
              "Create AnnotationItem interface matching OpenAI annotation schema: { type: 'url_citation', text: string, start_index: number, end_index: number, url: string, title: string }",
              "Create DeepResearchResult interface: { reportText: string, citations: Citation[], reasoningSteps: ReasoningStep[], searchQueries: string[], metadata: ResultMetadata }",
              "Create ResultMetadata interface: { model: string, completedAt: Date, totalTokens?: number, elapsedMs: number }"
            ]
          },
          "testable_properties": [],
          "function_id": "DeepResearchResponseParser.extractResults",
          "related_concepts": [
            "response parsing",
            "citation extraction",
            "OpenAI Responses API output format",
            "null safety",
            "data validation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_009",
      "description": "The system must implement consistent error handling patterns across all tool handlers with structured error responses",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_009.1",
          "description": "Define ToolError interface with code, message, retryable flag, and suggestedAction fields as the foundation for consistent error handling across all tool handlers",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "ToolError interface includes code field as union type: 'RATE_LIMIT' | 'INVALID_REQUEST' | 'API_ERROR' | 'TIMEOUT' | 'NETWORK' | 'CONFIG_ERROR' | 'VALIDATION_ERROR'",
            "ToolError interface includes message field as string for human-readable error description",
            "ToolError interface includes retryable boolean field indicating whether the operation can be retried",
            "ToolError interface includes optional suggestedAction field as string with user-actionable guidance",
            "ToolErrorCode type alias is exported for type-safe error code usage",
            "Interface extends Error class to support stack traces and instanceof checks",
            "ToolError class constructor accepts (message, code, retryable, suggestedAction?) parameters",
            "Interface is compatible with existing TranscriptionError and ChatGenerationError patterns",
            "toJSON() method returns serializable object for API responses",
            "Static factory methods exist: ToolError.rateLimit(), ToolError.timeout(), ToolError.apiError(), ToolError.network()",
            "Unit tests verify interface shape matches OpenAPI/JSON schema requirements",
            "Interface documentation includes JSDoc comments explaining each field's purpose"
          ],
          "implementation": {
            "frontend": [
              "Create ErrorDisplay component that renders ToolError with appropriate styling based on code",
              "Add RetryButton component that appears when error.retryable is true",
              "Display suggestedAction text as actionable hint below error message",
              "Implement error toast notifications using existing notification system"
            ],
            "backend": [
              "Create ToolError class in frontend/src/lib/errors/ToolError.ts extending Error",
              "Implement static factory methods for common error scenarios",
              "Add toJSON() method for consistent serialization in API responses",
              "Export ToolErrorCode type and ERROR_CODES constant object",
              "Create isToolError() type guard function for runtime type checking"
            ],
            "middleware": [
              "Add error transformation middleware to convert unknown errors to ToolError format",
              "Ensure all API routes return consistent { error, code, retryable, suggestedAction? } shape",
              "Validate error responses match ToolError schema before sending to client"
            ],
            "shared": [
              "Define ToolErrorCode union type: 'RATE_LIMIT' | 'INVALID_REQUEST' | 'API_ERROR' | 'TIMEOUT' | 'NETWORK' | 'CONFIG_ERROR' | 'VALIDATION_ERROR'",
              "Define ToolError interface with code, message, retryable, suggestedAction fields",
              "Create ERROR_MESSAGES constant mapping error codes to default messages",
              "Create SUGGESTED_ACTIONS constant mapping error codes to default suggestions",
              "Define ToolErrorResponse interface for API response shape"
            ]
          },
          "testable_properties": [],
          "function_id": "ToolError.defineInterface",
          "related_concepts": [
            "error standardization",
            "TypeScript interfaces",
            "error serialization",
            "existing TranscriptionError pattern",
            "ChatGenerationError pattern"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.2",
          "description": "Implement rate limit error handling with Retry-After header parsing for intelligent backoff timing across all tool handlers",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "Detects HTTP 429 status code from OpenAI API responses",
            "Parses Retry-After header when present (supports both seconds format and HTTP-date format)",
            "Falls back to configurable default delay (10 seconds) when Retry-After header is missing",
            "Converts Retry-After HTTP-date format to milliseconds remaining",
            "Validates parsed delay is within acceptable bounds (min 1s, max 300s)",
            "Returns ToolError with code='RATE_LIMIT', retryable=true, and suggestedAction including wait time",
            "Emits rate limit event for monitoring/alerting systems",
            "Integrates with existing exponential backoff logic (10s base, 2x multiplier)",
            "Unit tests cover: numeric Retry-After, date Retry-After, missing header, invalid values",
            "Handles rate limits from different OpenAI endpoints (responses, images, chat)",
            "Logs rate limit occurrences with endpoint, delay, and attempt number"
          ],
          "implementation": {
            "frontend": [
              "Create RateLimitCountdown component showing 'Retrying in X seconds...'",
              "Add cancel button to abort waiting during rate limit backoff",
              "Display informative message explaining rate limiting to user",
              "Show progress indicator during rate limit wait period",
              "Update UI state to indicate 'rate limited' vs 'error' condition"
            ],
            "backend": [
              "Create parseRetryAfterHeader(header: string | null): number function in frontend/src/lib/errors/rateLimitUtils.ts",
              "Implement isRateLimitError(error: unknown): boolean type guard",
              "Create calculateBackoffDelay(attempt: number, retryAfter?: number): number utility",
              "Add extractRateLimitInfo(response: Response): RateLimitInfo function",
              "Implement rate limit detection for OpenAI SDK errors (check status and headers)",
              "Create wrapper function: withRateLimitRetry<T>(fn: () => Promise<T>, maxRetries: number): Promise<T>"
            ],
            "middleware": [
              "Add rate limit tracking middleware that records 429 responses per endpoint",
              "Implement circuit breaker pattern for repeated rate limits (pause requests after 3 consecutive 429s)",
              "Forward Retry-After header in proxied error responses to client",
              "Log rate limit events with structured data: { endpoint, retryAfter, timestamp, userId }"
            ],
            "shared": [
              "Define RateLimitInfo interface: { retryAfterMs: number, resetAt: Date, remainingRequests?: number }",
              "Define RATE_LIMIT_DEFAULTS constant: { baseDelayMs: 10000, maxDelayMs: 300000, maxRetries: 3 }",
              "Create isHttpDate(value: string): boolean utility function",
              "Define parseHttpDate(dateString: string): Date utility function",
              "Export RATE_LIMIT_ERROR_CODE constant ('RATE_LIMIT')"
            ]
          },
          "testable_properties": [],
          "function_id": "RateLimitHandler.parseRetryAfter",
          "related_concepts": [
            "HTTP 429 response",
            "Retry-After header",
            "exponential backoff",
            "rate limit recovery",
            "existing 10s base delay pattern"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.3",
          "description": "Handle API errors with safe error messages ensuring no API key exposure or sensitive data leakage in error responses",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "API keys are never included in error messages sent to clients",
            "Error messages are sanitized to remove file paths, stack traces, and internal details",
            "Original full error is logged server-side for debugging with appropriate log level",
            "OpenAI API error messages are mapped to user-friendly equivalents",
            "Sensitive headers (Authorization, x-api-key) are stripped from logged requests",
            "Request/response bodies containing credentials are redacted before logging",
            "Error codes are preserved for programmatic handling while messages are sanitized",
            "Creates ToolError with generic message but logs original error with full context",
            "Handles nested error objects and error chains safely",
            "Unit tests verify no sensitive data appears in sanitized output",
            "Integration tests confirm API keys are not exposed in any error scenario",
            "Maintains error traceability via correlation IDs without exposing internals"
          ],
          "implementation": {
            "frontend": [
              "Display sanitized error.message to user, never raw API error",
              "Show generic 'An error occurred' for unexpected error shapes",
              "Implement error boundary that catches and sanitizes rendering errors",
              "Never log full error objects to browser console in production"
            ],
            "backend": [
              "Create sanitizeErrorMessage(error: unknown): string function stripping sensitive data",
              "Implement createSafeToolError(error: unknown, context: string): ToolError factory",
              "Add SENSITIVE_PATTERNS regex array for detecting API keys, tokens, passwords",
              "Create redactSensitiveData(obj: object): object utility for logging",
              "Implement mapOpenAIError(error: OpenAIError): ToolError with user-friendly messages",
              "Add error classification: isAuthError(), isValidationError(), isServerError()",
              "Create structured logger that auto-redacts sensitive fields"
            ],
            "middleware": [
              "Add error sanitization middleware as final error handler in API routes",
              "Strip Authorization and x-api-key headers from request logs",
              "Implement request body sanitization for logging (redact password, token, key fields)",
              "Add correlation ID to all error logs for traceability"
            ],
            "shared": [
              "Define SENSITIVE_FIELD_NAMES constant: ['apiKey', 'api_key', 'authorization', 'password', 'token', 'secret']",
              "Define SENSITIVE_PATTERNS regex: /sk-[a-zA-Z0-9]{20,}|Bearer\\s+[a-zA-Z0-9._-]+/gi",
              "Create SafeError interface: { message: string, code: string, correlationId: string }",
              "Define USER_FRIENDLY_MESSAGES map: { 'invalid_api_key': 'Authentication failed. Please check configuration.' }",
              "Export sanitizeForLogging(data: unknown): unknown utility function"
            ]
          },
          "testable_properties": [],
          "function_id": "SafeErrorHandler.sanitizeApiErrors",
          "related_concepts": [
            "security",
            "error sanitization",
            "PII protection",
            "API key safety",
            "error logging vs user display"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.4",
          "description": "Implement timeout handling with configurable durations per tool type, supporting both short synchronous operations and long-running background tasks",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "Each tool type has configurable timeout: image_generation (120s), deep_research (300s initial, 1800s polling), document_generation (60s), chat_completion (30s)",
            "Timeout is implemented using AbortController for cancellable requests",
            "Timeout error returns ToolError with code='TIMEOUT', retryable=true, suggestedAction='Try again or use a simpler query'",
            "Background tasks (deep_research) have separate initial request timeout vs total polling timeout",
            "User can cancel long-running operations via UI abort button",
            "Partial results are preserved when timeout occurs during streaming responses",
            "Timeout configuration is overridable per-request for special cases",
            "Cleanup functions run on timeout (e.g., abort pending fetch, release resources)",
            "Unit tests verify timeout triggers at correct duration for each tool",
            "Timeout events are logged with tool type, configured duration, and elapsed time",
            "Handles both fetch timeouts and SDK operation timeouts consistently"
          ],
          "implementation": {
            "frontend": [
              "Show elapsed time indicator for long-running operations",
              "Display 'Operation taking longer than expected' warning at 50% of timeout",
              "Add 'Cancel' button that triggers AbortController.abort()",
              "Implement timeout countdown for operations approaching limit",
              "Show appropriate error message distinguishing timeout from other errors"
            ],
            "backend": [
              "Create fetchWithTimeout<T>(url: string, options: RequestInit, timeoutMs: number): Promise<T> utility",
              "Implement createTimeoutController(timeoutMs: number): { controller: AbortController, cleanup: () => void }",
              "Add TOOL_TIMEOUTS constant: { deep_research: 300000, image_generation: 120000, document_generation: 60000, chat_completion: 30000 }",
              "Create withTimeout<T>(promise: Promise<T>, timeoutMs: number, context: string): Promise<T> wrapper",
              "Implement TimeoutError class extending ToolError with elapsed time",
              "Add timeout handling to polling loops with separate poll interval vs total timeout"
            ],
            "middleware": [
              "Add request timeout middleware using AbortController",
              "Implement cleanup registry for timed-out requests (close connections, abort streams)",
              "Log timeout events with: { tool, configuredMs, elapsedMs, operation }"
            ],
            "shared": [
              "Define ToolTimeoutConfig interface: { [toolName: string]: { initialMs: number, totalMs?: number, pollIntervalMs?: number } }",
              "Define DEFAULT_TOOL_TIMEOUTS constant with per-tool configurations",
              "Create TimeoutOptions interface: { timeoutMs: number, onTimeout?: () => void, signal?: AbortSignal }",
              "Define REQUEST_TIMEOUT_MS constant (30000) for default HTTP requests",
              "Export isTimeoutError(error: unknown): boolean type guard"
            ]
          },
          "testable_properties": [],
          "function_id": "TimeoutHandler.configurePerTool",
          "related_concepts": [
            "AbortController",
            "request timeouts",
            "background tasks",
            "polling timeouts",
            "tool-specific limits"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_009.5",
          "description": "Log errors with structured logging format for monitoring, debugging, and alerting integration across all tool handlers",
          "type": "sub_process",
          "parent_id": "REQ_009",
          "children": [],
          "acceptance_criteria": [
            "All tool errors are logged with consistent structured format: { timestamp, level, tool, code, message, correlationId, metadata }",
            "Log levels are appropriate: ERROR for failures, WARN for retryable errors, INFO for recoveries",
            "Correlation ID links related log entries across request lifecycle",
            "Sensitive data is automatically redacted from logs (API keys, user data)",
            "Error stack traces are captured in development but summarized in production",
            "Logs include contextual metadata: userId, sessionId, requestId, tool, operation",
            "Integration with existing console.error pattern for backward compatibility",
            "Structured logs are JSON-formatted for log aggregation tools (e.g., DataDog, CloudWatch)",
            "Error frequency and patterns can be queried from logs",
            "Log retention considers compliance requirements (PII minimization)",
            "Unit tests verify log output format and content for various error scenarios",
            "Performance: logging adds <1ms overhead to error handling path"
          ],
          "implementation": {
            "frontend": [
              "Send client-side errors to /api/log endpoint for server-side aggregation",
              "Include browser/device context in error reports",
              "Batch error logs to reduce network requests",
              "Implement error sampling for high-frequency errors to avoid log flooding"
            ],
            "backend": [
              "Create StructuredLogger class in frontend/src/lib/logging/StructuredLogger.ts",
              "Implement logError(error: ToolError, context: LogContext): void method",
              "Add formatLogEntry(level: LogLevel, data: LogData): string for JSON output",
              "Create withLogging<T>(fn: () => Promise<T>, context: LogContext): Promise<T> wrapper",
              "Implement log level filtering based on environment (verbose in dev, errors only in prod)",
              "Add generateCorrelationId(): string for request tracing",
              "Create extractErrorMetadata(error: unknown): ErrorMetadata utility"
            ],
            "middleware": [
              "Add request logging middleware that assigns correlationId to each request",
              "Implement error logging middleware that captures and logs all unhandled errors",
              "Forward logs to external monitoring service (if configured)",
              "Add request/response timing to log context"
            ],
            "shared": [
              "Define LogLevel enum: 'DEBUG' | 'INFO' | 'WARN' | 'ERROR'",
              "Define LogContext interface: { correlationId, tool, operation, userId?, sessionId?, metadata? }",
              "Define StructuredLogEntry interface: { timestamp: string, level: LogLevel, message: string, context: LogContext, error?: ErrorMetadata }",
              "Define ErrorMetadata interface: { code: string, retryable: boolean, stack?: string, originalMessage?: string }",
              "Create LOG_REDACT_FIELDS constant listing fields to auto-redact",
              "Export createLogContext(tool: string, operation: string): LogContext factory"
            ]
          },
          "testable_properties": [],
          "function_id": "StructuredLogger.logToolErrors",
          "related_concepts": [
            "structured logging",
            "observability",
            "error monitoring",
            "debugging",
            "audit trail",
            "log aggregation"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_010",
      "description": "The system must track and display cost estimates for expensive operations before execution",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_010.1",
          "description": "Display estimated costs for Deep Research queries (~$3 quick, ~$30 thorough) before execution to allow user to make informed decisions about research depth selection",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "Cost estimate displays immediately when user selects research depth (quick vs thorough)",
            "Quick depth shows estimated cost range of $2-$5 with average ~$3 for o4-mini-deep-research",
            "Thorough depth shows estimated cost range of $20-$40 with average ~$30 for o3-deep-research",
            "Cost breakdown shows base model cost plus additional tool costs (web search, code interpreter)",
            "Estimated token usage is displayed alongside cost estimate",
            "Cost estimate updates dynamically as user toggles additional tools (code_interpreter, file_search)",
            "Warning indicator appears when estimated cost exceeds user-defined threshold (configurable)",
            "Cost estimate tooltip explains pricing factors: input tokens, output tokens, tool calls",
            "Historical cost comparison shows average actual costs from user's previous similar queries",
            "Unit tests verify correct cost calculations for all model and tool combinations"
          ],
          "implementation": {
            "frontend": [
              "DeepResearchCostEstimate component showing min/max/average cost range with visual indicator",
              "ResearchDepthSelector with integrated cost display next to each option",
              "CostBreakdownTooltip showing itemized costs: model tokens + web_search ($0.01/call) + code_interpreter ($0.03/session)",
              "CostWarningBanner component that appears when estimated cost exceeds configurable threshold",
              "CostComparisonWidget showing 'Your average cost for similar queries: $X'",
              "Animated cost counter that updates as user changes options",
              "Cost threshold settings in user preferences panel"
            ],
            "backend": [
              "GET /api/costs/estimate/deep-research endpoint accepting depth, tools[], and query length parameters",
              "DeepResearchCostCalculator service with calculateEstimate(depth, tools, queryLength): CostEstimate",
              "Cost model configuration with current OpenAI pricing: o3 ($10/$40 per 1M input/output), o4-mini ($2/$8 per 1M)",
              "Tool cost adder: addToolCosts(baseCost, enabledTools[]): CostEstimate with web_search and code_interpreter",
              "Historical cost aggregation query to calculate user's average costs by query type",
              "Cost estimation cache to avoid recalculating for same parameters within session"
            ],
            "middleware": [
              "Rate limit cost estimation endpoint to prevent abuse",
              "Validate depth parameter is 'quick' | 'thorough'",
              "Log cost estimation requests for analytics"
            ],
            "shared": [
              "DeepResearchCostEstimate interface: { min: number, max: number, average: number, breakdown: CostBreakdown }",
              "CostBreakdown interface: { modelInputCost: number, modelOutputCost: number, webSearchCost: number, codeInterpreterCost: number }",
              "DEEP_RESEARCH_PRICING constant object with model-specific rates",
              "ResearchDepth type: 'quick' | 'thorough'",
              "CostThreshold interface for user-configurable warning thresholds"
            ]
          },
          "testable_properties": [],
          "function_id": "CostEstimator.displayDeepResearchCosts",
          "related_concepts": [
            "OpenAI Responses API",
            "o3-deep-research pricing",
            "o4-mini-deep-research pricing",
            "user cost awareness",
            "research depth selection"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.2",
          "description": "Show image generation costs based on model and quality selection (~$0.01-$0.19) with real-time updates as user adjusts parameters",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "Cost display updates in real-time as user changes model, quality, size, and count parameters",
            "Low quality shows ~$0.01-$0.02 per image depending on model",
            "Medium quality shows ~$0.04-$0.07 per image depending on model",
            "High quality shows ~$0.17-$0.19 per image depending on model",
            "Total cost calculated as (per-image cost \u00d7 image count) and displayed prominently",
            "Model comparison widget shows cost difference between gpt-image-1.5, gpt-image-1, and gpt-image-1-mini",
            "Size parameter impact on cost is shown (if applicable for selected model)",
            "Cost estimate includes note about potential prompt revision tokens",
            "Batch generation (n > 1) shows per-image cost AND total cost clearly",
            "Cost comparison with previous generation requests shown for context"
          ],
          "implementation": {
            "frontend": [
              "ImageCostCalculator component with live-updating cost display",
              "QualityPricingGrid showing all quality levels with prices in a comparison table",
              "ModelCostComparison component showing gpt-image-1.5 vs gpt-image-1 vs gpt-image-1-mini pricing",
              "BatchCostSummary showing 'X images \u00d7 $Y = $Z total'",
              "CostPerImageBadge component displayed next to each quality option radio button",
              "ImageGenerationCostPreview in form footer showing total before submission",
              "Animated cost ticker that updates smoothly as parameters change"
            ],
            "backend": [
              "GET /api/costs/estimate/image-generation endpoint accepting model, quality, size, count parameters",
              "ImageCostCalculator service with calculateImageCost(model, quality, size, count): ImageCostEstimate",
              "Pricing matrix storage with current rates for all model/quality/size combinations",
              "Cost validation to ensure estimated cost matches actual API pricing",
              "Bulk discount calculation if OpenAI offers volume pricing"
            ],
            "middleware": [
              "Validate quality is 'low' | 'medium' | 'high'",
              "Validate count is between 1 and 10",
              "Validate model is supported: 'gpt-image-1.5' | 'gpt-image-1' | 'gpt-image-1-mini'"
            ],
            "shared": [
              "ImageCostEstimate interface: { perImage: number, total: number, model: string, quality: string, count: number }",
              "IMAGE_PRICING_MATRIX constant with all model/quality price combinations",
              "ImageQuality type: 'low' | 'medium' | 'high'",
              "ImageModel type: 'gpt-image-1.5' | 'gpt-image-1' | 'gpt-image-1-mini'",
              "calculateTotalImageCost utility function: (perImageCost: number, count: number) => number"
            ]
          },
          "testable_properties": [],
          "function_id": "CostEstimator.displayImageGenerationCosts",
          "related_concepts": [
            "gpt-image-1.5 pricing",
            "image quality tiers",
            "batch image generation",
            "cost transparency",
            "parameter-based pricing"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.3",
          "description": "Track code_interpreter session costs ($0.03/session) and web search costs ($0.01/call) during Deep Research execution for accurate billing",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "Each code_interpreter session activation is tracked and logged with $0.03 cost",
            "Each web_search_preview call is tracked and logged with $0.01 cost",
            "Running cost total is updated in real-time during Deep Research execution",
            "Cost tracker distinguishes between different types of tool calls in the same request",
            "Session costs are aggregated correctly when multiple code_interpreter sessions occur",
            "Web search call count is extracted from response.output items where type === 'web_search_call'",
            "Tool costs are added to base model costs for complete cost picture",
            "Cost tracking persists across polling intervals for background mode requests",
            "Failed tool calls that still incur charges are tracked appropriately",
            "Cost tracker provides breakdown: { modelCost, webSearchCost, codeInterpreterCost, totalCost }"
          ],
          "implementation": {
            "frontend": [
              "RunningCostIndicator component showing live cost accumulation during research",
              "ToolCostBreakdownPanel showing web_search calls count \u00d7 $0.01 and code_interpreter sessions \u00d7 $0.03",
              "CostAccumulator animation showing costs incrementing as tools are invoked",
              "ToolInvocationLog displaying each tool call with associated cost",
              "FinalCostSummary shown when research completes with full breakdown"
            ],
            "backend": [
              "ToolCostTracker service class with trackWebSearch(count) and trackCodeInterpreter(sessions) methods",
              "extractToolCostsFromResponse(response) function parsing output items for tool invocations",
              "Cost accumulator storing running totals keyed by request ID",
              "Webhook/polling handler that updates cost tracker with each response chunk",
              "getCostBreakdown(requestId): ToolCostBreakdown method for final cost retrieval",
              "Background job cost aggregation for long-running requests"
            ],
            "middleware": [
              "Cost tracking middleware that intercepts all Deep Research responses",
              "Request ID injection for cost tracking correlation",
              "Cost persistence for background mode requests across server restarts"
            ],
            "shared": [
              "ToolCostBreakdown interface: { webSearchCalls: number, webSearchCost: number, codeInterpreterSessions: number, codeInterpreterCost: number, totalToolCost: number }",
              "WEB_SEARCH_COST_PER_CALL constant: 0.01",
              "CODE_INTERPRETER_COST_PER_SESSION constant: 0.03",
              "ToolInvocation interface: { type: 'web_search' | 'code_interpreter', timestamp: Date, cost: number }",
              "CostTrackingState interface for request-scoped cost accumulation"
            ]
          },
          "testable_properties": [],
          "function_id": "CostTracker.trackToolSessionCosts",
          "related_concepts": [
            "session-based pricing",
            "per-call pricing",
            "usage metering",
            "cost accumulation",
            "real-time cost tracking"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.4",
          "description": "Implement cost confirmation dialog before executing high-cost operations to prevent accidental expensive API calls",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "Confirmation dialog appears when estimated cost exceeds user-configurable threshold (default: $5)",
            "Dialog clearly displays estimated cost, cost breakdown, and operation details",
            "User must explicitly click 'Confirm' to proceed with high-cost operation",
            "Option to 'Remember my choice' for this session to avoid repeated confirmations",
            "Alternative lower-cost options are suggested in the dialog (e.g., 'Use quick mode instead: ~$3')",
            "Dialog shows user's remaining budget if spending limits are configured",
            "Cancel button returns user to parameter selection without API call",
            "Keyboard shortcuts: Enter to confirm, Escape to cancel",
            "Confirmation is logged for audit trail with timestamp and user acknowledgment",
            "Threshold can be adjusted per-tool type (different threshold for research vs images)",
            "Dialog is accessible (ARIA labels, focus trap, screen reader support)"
          ],
          "implementation": {
            "frontend": [
              "CostConfirmationDialog modal component with cost display, breakdown, and confirm/cancel buttons",
              "CostThresholdSettings component in user preferences for configuring warning thresholds",
              "AlternativeOptionsPanel within dialog suggesting cheaper alternatives",
              "RememberChoiceCheckbox for session-based confirmation bypass",
              "BudgetRemainingIndicator showing spending limit status if configured",
              "useCostConfirmation hook that checks threshold and shows dialog if exceeded",
              "Keyboard event handlers for Enter/Escape shortcuts",
              "ARIA-compliant dialog with proper focus management"
            ],
            "backend": [
              "GET /api/user/preferences/cost-thresholds endpoint to retrieve user's threshold settings",
              "PUT /api/user/preferences/cost-thresholds endpoint to update threshold configuration",
              "POST /api/costs/confirm endpoint to log user's cost acknowledgment with timestamp",
              "CostThresholdChecker service comparing estimated cost against user thresholds",
              "Spending limit tracker if user has configured monthly/daily limits",
              "suggestAlternatives(operation, currentCost): AlternativeOption[] service method"
            ],
            "middleware": [
              "Cost confirmation verification middleware that checks for valid confirmation token before high-cost requests",
              "Session-based confirmation cache for 'remember my choice' functionality",
              "Audit logging middleware for cost confirmations"
            ],
            "shared": [
              "CostThresholdConfig interface: { globalThreshold: number, deepResearchThreshold?: number, imageGenerationThreshold?: number, documentGenerationThreshold?: number }",
              "CostConfirmation interface: { operationType: string, estimatedCost: number, confirmedAt: Date, userId: string, sessionBypassed: boolean }",
              "AlternativeOption interface: { description: string, estimatedCost: number, parameters: Record<string, unknown> }",
              "DEFAULT_COST_THRESHOLD constant: 5.00",
              "CostConfirmationResult type: 'confirmed' | 'cancelled' | 'session_bypassed'"
            ]
          },
          "testable_properties": [],
          "function_id": "CostConfirmation.showHighCostDialog",
          "related_concepts": [
            "user consent",
            "cost protection",
            "confirmation UX",
            "threshold configuration",
            "spending limits"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_010.5",
          "description": "Log actual costs for billing tracking and analytics to enable cost monitoring, budget management, and usage reporting",
          "type": "sub_process",
          "parent_id": "REQ_010",
          "children": [],
          "acceptance_criteria": [
            "Actual costs from API responses are logged with request ID, timestamp, and user ID",
            "Token usage is captured from response.usage object (input_tokens, output_tokens, cached_tokens)",
            "Actual cost is calculated from token usage using current pricing rates",
            "Discrepancy detection alerts when actual cost differs significantly from estimate (>20%)",
            "Cost logs are queryable by date range, user, operation type, and tool",
            "Daily/weekly/monthly cost aggregations are calculated and stored",
            "Cost analytics dashboard shows spending trends, top operations, and averages",
            "Export functionality for cost reports (CSV, JSON)",
            "Retention policy respects data privacy requirements (configurable retention period)",
            "Real-time cost webhook notifications when spending exceeds thresholds",
            "Cost logs include operation metadata: model, tools used, input length, output length"
          ],
          "implementation": {
            "frontend": [
              "CostAnalyticsDashboard page showing spending overview, trends, and breakdowns",
              "CostHistoryTable with filtering by date, operation type, and cost range",
              "SpendingTrendChart (daily/weekly/monthly) using chart library",
              "TopOperationsWidget showing most expensive operations",
              "CostExportButton component for CSV/JSON report download",
              "RealTimeCostAlertBanner for threshold notifications",
              "EstimateVsActualComparisonWidget showing accuracy of estimates"
            ],
            "backend": [
              "POST /api/costs/log endpoint to record actual cost from completed operation",
              "CostLogger service with logActualCost(requestId, usage, metadata): void method",
              "Cost calculation from usage: calculateActualCost(usage: TokenUsage, model: string): number",
              "CostAggregator service calculating daily/weekly/monthly totals",
              "GET /api/costs/analytics endpoint returning aggregated cost data with filters",
              "GET /api/costs/export endpoint generating CSV/JSON reports",
              "DiscrepancyDetector comparing estimated vs actual and flagging significant differences",
              "Cost threshold webhook sender for real-time notifications",
              "Database schema: cost_logs table with request_id, user_id, operation_type, estimated_cost, actual_cost, token_usage, metadata, created_at"
            ],
            "middleware": [
              "Response interceptor that extracts usage data and triggers cost logging",
              "Cost log sanitization removing sensitive data before storage",
              "Rate limiting on analytics endpoints to prevent abuse"
            ],
            "shared": [
              "CostLog interface: { requestId: string, userId: string, operationType: string, estimatedCost: number, actualCost: number, tokenUsage: TokenUsage, metadata: Record<string, unknown>, createdAt: Date }",
              "TokenUsage interface: { inputTokens: number, outputTokens: number, cachedTokens?: number, totalTokens: number }",
              "CostAnalytics interface: { totalCost: number, operationBreakdown: Record<string, number>, dailyTotals: DailyCost[], averageCostPerOperation: Record<string, number> }",
              "DailyCost interface: { date: string, total: number, operationCounts: Record<string, number> }",
              "CostDiscrepancy interface: { requestId: string, estimated: number, actual: number, percentageDiff: number }",
              "COST_LOG_RETENTION_DAYS constant (configurable, default: 90)"
            ]
          },
          "testable_properties": [],
          "function_id": "CostLogger.logActualCosts",
          "related_concepts": [
            "billing integration",
            "usage analytics",
            "cost reporting",
            "budget tracking",
            "spending history"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    },
    {
      "id": "REQ_011",
      "description": "The system must support streaming and real-time progress updates for long-running operations",
      "type": "parent",
      "parent_id": null,
      "children": [
        {
          "id": "REQ_011.1",
          "description": "Implement Server-Sent Events (SSE) support for streaming reasoning updates during Deep Research operations, enabling real-time visibility into the research agent's thought process as it progresses through planning, searching, and synthesizing phases",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "SSE endpoint accepts GET requests with response_id parameter to identify the Deep Research job",
            "Server sends 'reasoning' events containing step summary, step index, and step type (planning/searching/synthesizing)",
            "Server sends 'web_search_call' events when research agent performs web searches with query text",
            "Server sends 'progress' events with percentage estimate based on typical research duration",
            "Server sends 'done' event when research completes with final report available flag",
            "Server sends 'error' event if research fails with error code and message",
            "Connection automatically reconnects using EventSource retry mechanism (3s default)",
            "Server sends keep-alive comments every 15 seconds to prevent proxy timeouts",
            "Client receives updates within 500ms of server receiving them from OpenAI",
            "SSE connection properly closes when response_id job completes or fails",
            "Multiple clients can subscribe to same response_id simultaneously",
            "Events are JSON-encoded with consistent schema across all event types",
            "Server handles OpenAI background mode polling internally and converts to SSE stream"
          ],
          "implementation": {
            "frontend": [
              "ReasoningStreamProvider context component managing EventSource connection lifecycle",
              "useReasoningStream(responseId) hook returning { steps, currentStep, progress, error, isConnected }",
              "ReasoningEventHandler utility to parse SSE events into typed ReasoningStep objects",
              "ConnectionStatusIndicator showing SSE connection state (connecting/connected/reconnecting/error)",
              "ReasoningStepsList component rendering streaming steps with animated entry transitions",
              "Auto-scroll behavior to keep latest reasoning step visible as new steps arrive",
              "Fallback polling mechanism for browsers without EventSource support (IE11)",
              "EventSource cleanup on component unmount to prevent memory leaks"
            ],
            "backend": [
              "GET /api/tools/deep-research/stream/[responseId] SSE endpoint",
              "SSEConnectionManager class managing active connections per response_id",
              "OpenAIPollingService that polls background job and converts status to SSE events",
              "ReasoningEventFormatter converting OpenAI output items to SSE event format",
              "Keep-alive scheduler sending SSE comments every 15 seconds",
              "Connection cleanup when response completes or client disconnects",
              "Redis pub/sub integration for multi-instance SSE broadcasting (production scaling)",
              "Graceful shutdown handling ensuring clients receive final events"
            ],
            "middleware": [
              "Validate response_id belongs to authenticated user's requests",
              "Set correct SSE headers: Content-Type: text/event-stream, Cache-Control: no-cache, Connection: keep-alive",
              "Handle CORS for cross-origin SSE connections",
              "Rate limit SSE connection attempts per user (max 5 concurrent)"
            ],
            "shared": [
              "SSEEvent discriminated union type: ReasoningEvent | WebSearchEvent | ProgressEvent | DoneEvent | ErrorEvent",
              "ReasoningEvent interface: { type: 'reasoning', data: { index: number, summary: string, stepType: string, timestamp: string } }",
              "WebSearchEvent interface: { type: 'web_search_call', data: { query: string, timestamp: string } }",
              "ProgressEvent interface: { type: 'progress', data: { percentage: number, estimatedTimeRemaining?: number } }",
              "SSE_KEEP_ALIVE_INTERVAL constant (15000ms)",
              "SSE_RECONNECT_DELAY constant (3000ms)",
              "MAX_CONCURRENT_SSE_CONNECTIONS constant (5)"
            ]
          },
          "testable_properties": [],
          "function_id": "SSEService.streamReasoningUpdates",
          "related_concepts": [
            "Server-Sent Events",
            "EventSource API",
            "Deep Research Responses API",
            "real-time streaming",
            "reasoning transparency",
            "long-polling fallback",
            "connection management"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.2",
          "description": "Support image generation streaming with partial_images parameter (0-3 preview images) allowing users to see progressive image generation in real-time before the final high-quality image is complete",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "API request includes stream: true and partial_images parameter (0-3)",
            "partial_images: 0 streams only the final image without previews",
            "partial_images: 1-3 streams that many low-resolution preview images before final",
            "Each partial image event includes base64 data and preview index (1, 2, 3)",
            "Final image event is clearly distinguished from partial events",
            "Partial images have visibly lower resolution/quality than final image",
            "UI displays partial images as they arrive, replacing previous partial with next",
            "Final image smoothly replaces the last partial image when generation completes",
            "Streaming can be enabled/disabled via user preference toggle",
            "Error during streaming still attempts to return any completed partials",
            "Partial images are not persisted to storage (only final image is saved)",
            "Total streaming time displayed to user from first partial to final image",
            "Memory efficient handling prevents base64 strings from causing memory pressure"
          ],
          "implementation": {
            "frontend": [
              "ImageStreamPreview component displaying current partial/final image",
              "useImageStream(jobId) hook returning { partialImages, finalImage, isStreaming, progress }",
              "PartialImageTransition component with CSS fade animation between partials",
              "StreamingQualityIndicator badge showing 'Preview 1/3', 'Preview 2/3', 'Final'",
              "ImageGenerationProgress bar showing estimated completion during streaming",
              "StreamingToggle in image generation settings (default: enabled with 2 partials)",
              "PartialImagePlaceholder skeleton shown before first partial arrives",
              "Memory-efficient image display using URL.createObjectURL for base64 blobs"
            ],
            "backend": [
              "POST /api/tools/generate-image/stream endpoint with SSE response",
              "OpenAI streaming response handler parsing image generation chunks",
              "PartialImageExtractor service converting stream chunks to base64 events",
              "StreamProgressEstimator calculating percentage based on partial count received",
              "Final image persistence to Vercel Blob only after stream completes",
              "Stream error recovery attempting to return last successful partial",
              "Request validation ensuring partial_images is 0-3 range",
              "Timeout handling for streams exceeding 120 seconds"
            ],
            "middleware": [
              "Validate partial_images parameter is integer 0-3",
              "Set streaming response headers for chunked transfer",
              "Track streaming duration for analytics and billing",
              "Handle client disconnect during stream to clean up resources"
            ],
            "shared": [
              "ImageStreamEvent discriminated union: PartialImageEvent | FinalImageEvent | ErrorEvent",
              "PartialImageEvent interface: { type: 'partial', index: number, total: number, b64_json: string, timestamp: string }",
              "FinalImageEvent interface: { type: 'final', b64_json: string, revised_prompt?: string, duration_ms: number }",
              "ImageStreamConfig interface: { stream: boolean, partial_images: 0 | 1 | 2 | 3 }",
              "DEFAULT_PARTIAL_IMAGES constant (2)",
              "IMAGE_STREAM_TIMEOUT_MS constant (120000)"
            ]
          },
          "testable_properties": [],
          "function_id": "ImageStreamingService.streamWithPartialImages",
          "related_concepts": [
            "gpt-image-1.5 streaming API",
            "partial image previews",
            "progressive image loading",
            "base64 image streaming",
            "image quality tiers",
            "generation preview"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.3",
          "description": "Display real-time progress indicators showing current operation step for all long-running tools, providing users with clear visibility into what the system is doing and how far along the operation has progressed",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Progress indicator displays distinct phases for Deep Research: 'Analyzing query', 'Searching web', 'Reading sources', 'Synthesizing report'",
            "Progress indicator displays phases for Image Generation: 'Processing prompt', 'Generating image', 'Finalizing'",
            "Progress indicator displays phases for Document Generation: 'Generating content', 'Creating document', 'Uploading file'",
            "Current step is visually highlighted with animation (pulsing or spinner)",
            "Completed steps show checkmark icon and muted styling",
            "Pending steps show empty circle or grayed styling",
            "Progress percentage updates at least every 5 seconds during active operations",
            "Elapsed time counter shows duration since operation started (MM:SS format)",
            "Estimated time remaining shown when calculable (based on historical averages)",
            "Progress bar fills proportionally to step completion (not just step count)",
            "Tool-specific icon displayed alongside progress (magnifying glass for research, palette for image, document for files)",
            "Progress indicator persists if user navigates away and returns",
            "Accessibility: progress announced to screen readers on step changes"
          ],
          "implementation": {
            "frontend": [
              "OperationProgressPanel component with step list, progress bar, and timer",
              "ProgressStep component showing icon, label, and completion state",
              "useOperationProgress(operationId) hook connecting to progress events",
              "ElapsedTimeCounter component with real-time MM:SS display",
              "EstimatedTimeRemaining component based on average operation durations",
              "ProgressBar component with smooth CSS transition on percentage updates",
              "ToolIcon component mapping operation type to appropriate icon",
              "ProgressPersistence using sessionStorage to restore on navigation return",
              "ARIA live region for screen reader progress announcements"
            ],
            "backend": [
              "ProgressTracker service class maintaining operation state",
              "Operation step definitions per tool type with estimated durations",
              "Progress event emission integrated into each tool handler",
              "Historical duration storage for ETA calculations",
              "GET /api/operations/[id]/progress endpoint for initial state fetch",
              "Progress state persistence to database for recovery after server restart",
              "Step transition logging for analytics on actual vs estimated durations"
            ],
            "middleware": [
              "Validate operation ID belongs to authenticated user",
              "Cache recent progress states for quick retrieval",
              "Rate limit progress polling to prevent abuse (max 1 req/second)"
            ],
            "shared": [
              "OperationProgress interface: { operationId: string, operationType: ToolType, currentStep: number, totalSteps: number, stepName: string, percentage: number, elapsedMs: number, estimatedRemainingMs?: number }",
              "OperationStep interface: { index: number, name: string, status: 'pending' | 'in_progress' | 'completed', estimatedDurationMs: number }",
              "DEEP_RESEARCH_STEPS constant array defining research phases",
              "IMAGE_GENERATION_STEPS constant array defining image phases",
              "DOCUMENT_GENERATION_STEPS constant array defining document phases",
              "ToolType union type: 'deep_research' | 'image_generation' | 'document_generation'"
            ]
          },
          "testable_properties": [],
          "function_id": "ProgressIndicatorService.displayOperationProgress",
          "related_concepts": [
            "progress tracking",
            "operation status",
            "step indicators",
            "loading states",
            "user feedback",
            "estimated time remaining",
            "operation phases"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.4",
          "description": "Update UI with intermediate results as they become available during long-running operations, allowing users to see partial outputs and early insights before the complete result is ready",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Deep Research displays web search queries as they are executed (clickable links to see what was searched)",
            "Deep Research displays reasoning summaries as they complete (expandable accordion sections)",
            "Deep Research displays partial report sections as they are synthesized (growing document)",
            "Image Generation displays partial preview images as they stream (with 'Preview' badge)",
            "Document Generation displays content outline before full document is ready",
            "Each intermediate result is timestamped showing when it was received",
            "Intermediate results animate into view with subtle fade-in transition",
            "User can expand/collapse intermediate result sections to manage screen space",
            "Final result is clearly distinguished from intermediate results (visual separator)",
            "Intermediate results remain visible after final result for reference (collapsible)",
            "Loading skeletons shown for next expected intermediate result type",
            "Error in intermediate result doesn't block subsequent results from displaying",
            "Intermediate results are scrollable if they exceed viewport height"
          ],
          "implementation": {
            "frontend": [
              "IntermediateResultsFeed component rendering time-ordered result items",
              "SearchQueryResult component showing executed query with timestamp",
              "ReasoningStepResult component with expandable summary accordion",
              "PartialReportSection component showing synthesized content chunk",
              "ImagePreviewResult component displaying streaming partial with badge",
              "ContentOutlineResult component showing document structure before content",
              "ResultTimestamp component formatting relative time (e.g., '2s ago')",
              "AnimatedResultEntry wrapper with CSS fade-in and slide-up",
              "ResultsCollapseToggle for hiding/showing intermediate results section",
              "NextResultSkeleton component predicting next result type"
            ],
            "backend": [
              "IntermediateResultEmitter service broadcasting results as they arrive",
              "ResultTypeClassifier determining result type from OpenAI response items",
              "PartialReportAssembler combining report sections into coherent preview",
              "ResultOrderingService ensuring results display in correct sequence",
              "IntermediateResultCache storing results for client reconnection",
              "WebSocket or SSE integration for pushing results to connected clients",
              "Result persistence for audit trail and debugging"
            ],
            "middleware": [
              "Validate result ownership before broadcasting to client",
              "Transform OpenAI response format to frontend-friendly intermediate result format",
              "Filter sensitive information from intermediate results if applicable"
            ],
            "shared": [
              "IntermediateResult discriminated union: SearchQueryResult | ReasoningResult | PartialReportResult | ImagePreviewResult | OutlineResult",
              "SearchQueryResult interface: { type: 'search_query', query: string, timestamp: string }",
              "ReasoningResult interface: { type: 'reasoning', stepIndex: number, summary: string, timestamp: string }",
              "PartialReportResult interface: { type: 'partial_report', sectionIndex: number, content: string, timestamp: string }",
              "ImagePreviewResult interface: { type: 'image_preview', previewIndex: number, b64_json: string, timestamp: string }",
              "INTERMEDIATE_RESULT_TYPES constant array for type checking"
            ]
          },
          "testable_properties": [],
          "function_id": "IntermediateResultsService.updateUIWithResults",
          "related_concepts": [
            "incremental rendering",
            "partial results",
            "streaming UI updates",
            "optimistic updates",
            "result previews",
            "real-time feedback"
          ],
          "category": "functional"
        },
        {
          "id": "REQ_011.5",
          "description": "Allow cancellation of in-progress operations with proper cleanup of resources, API connections, and partial data, ensuring users can stop long-running tasks without leaving orphaned resources or inconsistent state",
          "type": "sub_process",
          "parent_id": "REQ_011",
          "children": [],
          "acceptance_criteria": [
            "Cancel button is visible and enabled during all long-running operations",
            "Cancel button is disabled/hidden when no operation is in progress",
            "Clicking cancel immediately shows 'Cancelling...' state with spinner",
            "Cancellation stops OpenAI API streaming/polling within 2 seconds",
            "Partial results generated before cancellation are preserved and viewable",
            "Cancelled Deep Research cleans up any background job in OpenAI",
            "Cancelled Image Generation cleans up any partial blobs from Vercel storage",
            "Cancelled Document Generation cleans up any temporary files created",
            "User receives confirmation message when cancellation completes",
            "Cost incurred before cancellation is still tracked and displayed",
            "Cancellation reason is logged for analytics (user initiated vs timeout vs error)",
            "User can start new operation immediately after cancellation completes",
            "Double-clicking cancel doesn't cause duplicate cancellation attempts",
            "Cancellation during SSE stream properly closes EventSource connection",
            "AbortController signal propagates to all nested async operations"
          ],
          "implementation": {
            "frontend": [
              "CancelOperationButton component with loading state during cancellation",
              "useCancellableOperation hook returning { cancel, isCancelling, wasCancelled }",
              "AbortController integration in all fetch calls to API endpoints",
              "CancellationConfirmationToast showing 'Operation cancelled' message",
              "PartialResultsPreservation logic keeping intermediate results visible",
              "CancelButtonVisibility logic based on current operation state",
              "Double-click prevention with disabled state after first click",
              "EventSource abort handling when cancel is triggered during stream"
            ],
            "backend": [
              "POST /api/operations/[id]/cancel endpoint triggering cancellation",
              "CancellationService class coordinating cleanup across services",
              "OpenAICancellation module stopping background jobs via API",
              "BlobCleanupService deleting partial uploads on cancellation",
              "TempFileCleanupService removing generated document fragments",
              "CancellationLogger recording cancellation events with reason codes",
              "AbortSignal propagation through all nested service calls",
              "Graceful handling when cancellation races with completion"
            ],
            "middleware": [
              "Validate cancellation request is from operation owner",
              "Idempotent cancellation handling (multiple requests don't error)",
              "Timeout on cancellation cleanup (max 10 seconds before force complete)",
              "Track cancellation costs for billing accuracy"
            ],
            "shared": [
              "CancellationRequest interface: { operationId: string, reason: CancellationReason }",
              "CancellationReason union type: 'user_initiated' | 'timeout' | 'error' | 'system'",
              "CancellationResult interface: { success: boolean, partialResultsPreserved: boolean, cleanedUpResources: string[], costIncurred: number }",
              "OperationState union type: 'idle' | 'running' | 'cancelling' | 'cancelled' | 'completed' | 'failed'",
              "CANCELLATION_TIMEOUT_MS constant (10000)",
              "Cancellable interface for services supporting cancellation"
            ]
          },
          "testable_properties": [],
          "function_id": "OperationCancellationService.cancelWithCleanup",
          "related_concepts": [
            "operation cancellation",
            "AbortController",
            "resource cleanup",
            "graceful shutdown",
            "partial data handling",
            "cost management",
            "user control"
          ],
          "category": "functional"
        }
      ],
      "acceptance_criteria": [],
      "implementation": null,
      "testable_properties": [],
      "function_id": null,
      "related_concepts": [],
      "category": "functional"
    }
  ],
  "metadata": {
    "source": "agent_sdk_decomposition",
    "research_length": 29384,
    "decomposition_stats": {
      "requirements_found": 12,
      "subprocesses_expanded": 60,
      "total_nodes": 72,
      "extraction_time_ms": 62730,
      "expansion_time_ms": 1159327
    }
  }
}